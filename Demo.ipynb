{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89279586-dc5c-4688-b3bc-e44c0a80c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arxiv_client import fetch_arxiv_papers\n",
    "import os\n",
    "import glob\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from datetime import datetime\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee3ba4e9-f44e-4dc5-b5d4-287722a46a11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with\\n  the MME-CoF Benchmark',\n",
       "  'summary': 'Recent video generation models can produce high-fidelity, temporally coherent\\nvideos, indicating that they may encode substantial world knowledge. Beyond\\nrealistic synthesis, they also exhibit emerging behaviors indicative of visual\\nperception, modeling, and manipulation. Yet, an important question still\\nremains: Are video models ready to serve as zero-shot reasoners in challenging\\nvisual reasoning scenarios? In this work, we conduct an empirical study to\\ncomprehensively investigate this question, focusing on the leading and popular\\nVeo-3. We evaluate its reasoning behavior across 12 dimensions, including\\nspatial, geometric, physical, temporal, and embodied logic, systematically\\ncharacterizing both its strengths and failure modes. To standardize this study,\\nwe curate the evaluation data into MME-CoF, a compact benchmark that enables\\nin-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our\\nfindings reveal that while current video models demonstrate promising reasoning\\npatterns on short-horizon spatial coherence, fine-grained grounding, and\\nlocally consistent dynamics, they remain limited in long-horizon causal\\nreasoning, strict geometric constraints, and abstract logic. Overall, they are\\nnot yet reliable as standalone zero-shot reasoners, but exhibit encouraging\\nsigns as complementary visual engines alongside dedicated reasoning models.\\nProject page: https://video-cof.github.io',\n",
       "  'link': 'http://arxiv.org/abs/2510.26802v1',\n",
       "  'published': datetime.datetime(2025, 10, 30, 17, 59, 55, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in\\n  Zero-Shot Real-World Scenarios',\n",
       "  'summary': 'In real-world environments, AI systems often face unfamiliar scenarios\\nwithout labeled data, creating a major challenge for conventional scene\\nunderstanding models. The inability to generalize across unseen contexts limits\\nthe deployment of vision-based applications in dynamic, unstructured settings.\\nThis work introduces a Dynamic Context-Aware Scene Reasoning framework that\\nleverages Vision-Language Alignment to address zero-shot real-world scenarios.\\nThe goal is to enable intelligent systems to infer and adapt to new\\nenvironments without prior task-specific training. The proposed approach\\nintegrates pre-trained vision transformers and large language models to align\\nvisual semantics with natural language descriptions, enhancing contextual\\ncomprehension. A dynamic reasoning module refines predictions by combining\\nglobal scene cues and object-level interactions guided by linguistic priors.\\nExtensive experiments on zero-shot benchmarks such as COCO, Visual Genome, and\\nOpen Images demonstrate up to 18% improvement in scene understanding accuracy\\nover baseline models in complex and unseen environments. Results also show\\nrobust performance in ambiguous or cluttered scenes due to the synergistic\\nfusion of vision and language. This framework offers a scalable and\\ninterpretable approach for context-aware reasoning, advancing zero-shot\\ngeneralization in dynamic real-world settings.',\n",
       "  'link': 'http://arxiv.org/abs/2510.26580v1',\n",
       "  'published': datetime.datetime(2025, 10, 30, 15, 7, 55, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'AdSum: Two-stream Audio-visual Summarization for Automated Video\\n  Advertisement Clipping',\n",
       "  'summary': 'Advertisers commonly need multiple versions of the same advertisement (ad) at\\nvarying durations for a single campaign. The traditional approach involves\\nmanually selecting and re-editing shots from longer video ads to create shorter\\nversions, which is labor-intensive and time-consuming. In this paper, we\\nintroduce a framework for automated video ad clipping using video summarization\\ntechniques. We are the first to frame video clipping as a shot selection\\nproblem, tailored specifically for advertising. Unlike existing general video\\nsummarization methods that primarily focus on visual content, our approach\\nemphasizes the critical role of audio in advertising. To achieve this, we\\ndevelop a two-stream audio-visual fusion model that predicts the importance of\\nvideo frames, where importance is defined as the likelihood of a frame being\\nselected in the firm-produced short ad. To address the lack of ad-specific\\ndatasets, we present AdSum204, a novel dataset comprising 102 pairs of\\n30-second and 15-second ads from real advertising campaigns. Extensive\\nexperiments demonstrate that our model outperforms state-of-the-art methods\\nacross various metrics, including Average Precision, Area Under Curve,\\nSpearman, and Kendall.',\n",
       "  'link': 'http://arxiv.org/abs/2510.26569v1',\n",
       "  'published': datetime.datetime(2025, 10, 30, 14, 59, 37, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'LLMs as In-Context Meta-Learners for Model and Hyperparameter Selection',\n",
       "  'summary': 'Model and hyperparameter selection are critical but challenging in machine\\nlearning, typically requiring expert intuition or expensive automated search.\\nWe investigate whether large language models (LLMs) can act as in-context\\nmeta-learners for this task. By converting each dataset into interpretable\\nmetadata, we prompt an LLM to recommend both model families and\\nhyperparameters. We study two prompting strategies: (1) a zero-shot mode\\nrelying solely on pretrained knowledge, and (2) a meta-informed mode augmented\\nwith examples of models and their performance on past tasks. Across synthetic\\nand real-world benchmarks, we show that LLMs can exploit dataset metadata to\\nrecommend competitive models and hyperparameters without search, and that\\nimprovements from meta-informed prompting demonstrate their capacity for\\nin-context meta-learning. These results highlight a promising new role for LLMs\\nas lightweight, general-purpose assistants for model selection and\\nhyperparameter optimization.',\n",
       "  'link': 'http://arxiv.org/abs/2510.26510v1',\n",
       "  'published': datetime.datetime(2025, 10, 30, 14, 4, 25, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Multi-agent Large Language Model Framework to Automatically Assess\\n  Performance of a Clinical AI Triage Tool',\n",
       "  'summary': 'Purpose: The purpose of this study was to determine if an ensemble of\\nmultiple LLM agents could be used collectively to provide a more reliable\\nassessment of a pixel-based AI triage tool than a single LLM.\\n  Methods: 29,766 non-contrast CT head exams from fourteen hospitals were\\nprocessed by a commercial intracranial hemorrhage (ICH) AI detection tool.\\nRadiology reports were analyzed by an ensemble of eight open-source LLM models\\nand a HIPAA compliant internal version of GPT-4o using a single multi-shot\\nprompt that assessed for presence of ICH. 1,726 examples were manually\\nreviewed. Performance characteristics of the eight open-source models and\\nconsensus were compared to GPT-4o. Three ideal consensus LLM ensembles were\\ntested for rating the performance of the triage tool.\\n  Results: The cohort consisted of 29,766 head CTs exam-report pairs. The\\nhighest AUC performance was achieved with llama3.3:70b and GPT-4o (AUC= 0.78).\\nThe average precision was highest for Llama3.3:70b and GPT-4o (AP=0.75 & 0.76).\\nLlama3.3:70b had the highest F1 score (0.81) and recall (0.85), greater\\nprecision (0.78), specificity (0.72), and MCC (0.57). Using MCC (95% CI) the\\nideal combination of LLMs were: Full-9 Ensemble 0.571 (0.552-0.591), Top-3\\nEnsemble 0.558 (0.537-0.579), Consensus 0.556 (0.539-0.574), and GPT4o 0.522\\n(0.500-0.543). No statistically significant differences were observed between\\nTop-3, Full-9, and Consensus (p > 0.05).\\n  Conclusion: An ensemble of medium to large sized open-source LLMs provides a\\nmore consistent and reliable method to derive a ground truth retrospective\\nevaluation of a clinical AI triage tool over a single LLM alone.',\n",
       "  'link': 'http://arxiv.org/abs/2510.26498v1',\n",
       "  'published': datetime.datetime(2025, 10, 30, 13, 50, 19, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Representation-Level Counterfactual Calibration for Debiased Zero-Shot\\n  Recognition',\n",
       "  'summary': \"Object-context shortcuts remain a persistent challenge in vision-language\\nmodels, undermining zero-shot reliability when test-time scenes differ from\\nfamiliar training co-occurrences. We recast this issue as a causal inference\\nproblem and ask: Would the prediction remain if the object appeared in a\\ndifferent environment? To answer this at inference time, we estimate object and\\nbackground expectations within CLIP's representation space, and synthesize\\ncounterfactual embeddings by recombining object features with diverse\\nalternative contexts sampled from external datasets, batch neighbors, or\\ntext-derived descriptions. By estimating the Total Direct Effect and simulating\\nintervention, we further subtract background-only activation, preserving\\nbeneficial object-context interactions while mitigating hallucinated scores.\\nWithout retraining or prompt design, our method substantially improves both\\nworst-group and average accuracy on context-sensitive benchmarks, establishing\\na new zero-shot state of the art. Beyond performance, our framework provides a\\nlightweight representation-level counterfactual approach, offering a practical\\ncausal avenue for debiased and reliable multimodal reasoning.\",\n",
       "  'link': 'http://arxiv.org/abs/2510.26466v1',\n",
       "  'published': datetime.datetime(2025, 10, 30, 13, 11, 23, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Towards Fine-Grained Vision-Language Alignment for Few-Shot Anomaly\\n  Detection',\n",
       "  'summary': 'Few-shot anomaly detection (FSAD) methods identify anomalous regions with few\\nknown normal samples. Most existing methods rely on the generalization ability\\nof pre-trained vision-language models (VLMs) to recognize potentially anomalous\\nregions through feature similarity between text descriptions and images.\\nHowever, due to the lack of detailed textual descriptions, these methods can\\nonly pre-define image-level descriptions to match each visual patch token to\\nidentify potential anomalous regions, which leads to the semantic misalignment\\nbetween image descriptions and patch-level visual anomalies, achieving\\nsub-optimal localization performance. To address the above issues, we propose\\nthe Multi-Level Fine-Grained Semantic Caption (MFSC) to provide multi-level and\\nfine-grained textual descriptions for existing anomaly detection datasets with\\nautomatic construction pipeline. Based on the MFSC, we propose a novel\\nframework named FineGrainedAD to improve anomaly localization performance,\\nwhich consists of two components: Multi-Level Learnable Prompt (MLLP) and\\nMulti-Level Semantic Alignment (MLSA). MLLP introduces fine-grained semantics\\ninto multi-level learnable prompts through automatic replacement and\\nconcatenation mechanism, while MLSA designs region aggregation strategy and\\nmulti-level alignment training to facilitate learnable prompts better align\\nwith corresponding visual regions. Experiments demonstrate that the proposed\\nFineGrainedAD achieves superior overall performance in few-shot settings on\\nMVTec-AD and VisA datasets.',\n",
       "  'link': 'http://arxiv.org/abs/2510.26464v1',\n",
       "  'published': datetime.datetime(2025, 10, 30, 13, 9, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt\\n  Tuning of Vision-Language Models',\n",
       "  'summary': \"Test-time prompt tuning (TPT) has emerged as a promising technique for\\nadapting large vision-language models (VLMs) to unseen tasks without relying on\\nlabeled data. However, the lack of dispersion between textual features can hurt\\ncalibration performance, which raises concerns about VLMs' reliability,\\ntrustworthiness, and safety. Current TPT approaches primarily focus on\\nimproving prompt calibration by either maximizing average textual feature\\ndispersion or enforcing orthogonality constraints to encourage angular\\nseparation. However, these methods may not always have optimal angular\\nseparation between class-wise textual features, which implies overlooking the\\ncritical role of angular diversity. To address this, we propose A-TPT, a novel\\nTPT framework that introduces angular diversity to encourage uniformity in the\\ndistribution of normalized textual features induced by corresponding learnable\\nprompts. This uniformity is achieved by maximizing the minimum pairwise angular\\ndistance between features on the unit hypersphere. We show that our approach\\nconsistently surpasses state-of-the-art TPT methods in reducing the aggregate\\naverage calibration error while maintaining comparable accuracy through\\nextensive experiments with various backbones on different datasets. Notably,\\nour approach exhibits superior zero-shot calibration performance on natural\\ndistribution shifts and generalizes well to medical datasets. We provide\\nextensive analyses, including theoretical aspects, to establish the grounding\\nof A-TPT. These results highlight the potency of promoting angular diversity to\\nachieve well-dispersed textual features, significantly improving VLM\\ncalibration during test-time adaptation. Our code will be made publicly\\navailable.\",\n",
       "  'link': 'http://arxiv.org/abs/2510.26441v1',\n",
       "  'published': datetime.datetime(2025, 10, 30, 12, 45, 24, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'MisSynth: Improving MISSCI Logical Fallacies Classification with\\n  Synthetic Data',\n",
       "  'summary': 'Health-related misinformation is very prevalent and potentially harmful. It\\nis difficult to identify, especially when claims distort or misinterpret\\nscientific findings. We investigate the impact of synthetic data generation and\\nlightweight fine-tuning techniques on the ability of large language models\\n(LLMs) to recognize fallacious arguments using the MISSCI dataset and\\nframework. In this work, we propose MisSynth, a pipeline that applies\\nretrieval-augmented generation (RAG) to produce synthetic fallacy samples,\\nwhich are then used to fine-tune an LLM model. Our results show substantial\\naccuracy gains with fine-tuned models compared to vanilla baselines. For\\ninstance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score\\nabsolute improvement on the MISSCI test split over its vanilla baseline. We\\ndemonstrate that introducing synthetic fallacy data to augment limited\\nannotated resources can significantly enhance zero-shot LLM classification\\nperformance on real-world scientific misinformation tasks, even with limited\\ncomputational resources. The code and synthetic dataset are available on\\nhttps://github.com/mxpoliakov/MisSynth.',\n",
       "  'link': 'http://arxiv.org/abs/2510.26345v1',\n",
       "  'published': datetime.datetime(2025, 10, 30, 10, 52, 43, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'SplitFlow: Flow Decomposition for Inversion-Free Text-to-Image Editing',\n",
       "  'summary': 'Rectified flow models have become a de facto standard in image generation due\\nto their stable sampling trajectories and high-fidelity outputs. Despite their\\nstrong generative capabilities, they face critical limitations in image editing\\ntasks: inaccurate inversion processes for mapping real images back into the\\nlatent space, and gradient entanglement issues during editing often result in\\noutputs that do not faithfully reflect the target prompt. Recent efforts have\\nattempted to directly map source and target distributions via ODE-based\\napproaches without inversion; however,these methods still yield suboptimal\\nediting quality. In this work, we propose a flow decomposition-and-aggregation\\nframework built upon an inversion-free formulation to address these\\nlimitations. Specifically, we semantically decompose the target prompt into\\nmultiple sub-prompts, compute an independent flow for each, and aggregate them\\nto form a unified editing trajectory. While we empirically observe that\\ndecomposing the original flow enhances diversity in the target space,\\ngenerating semantically aligned outputs still requires consistent guidance\\ntoward the full target prompt. To this end, we design a projection and\\nsoft-aggregation mechanism for flow, inspired by gradient conflict resolution\\nin multi-task learning. This approach adaptively weights the sub-target\\nvelocity fields, suppressing semantic redundancy while emphasizing distinct\\ndirections, thereby preserving both diversity and consistency in the final\\nedited output. Experimental results demonstrate that our method outperforms\\nexisting zero-shot editing approaches in terms of semantic fidelity and\\nattribute disentanglement. The code is available at\\nhttps://github.com/Harvard-AI-and-Robotics-Lab/SplitFlow.',\n",
       "  'link': 'http://arxiv.org/abs/2510.25970v1',\n",
       "  'published': datetime.datetime(2025, 10, 29, 21, 12, 58, tzinfo=datetime.timezone.utc)},\n",
       " {'title': '$Ï€_\\\\texttt{RL}$: Online RL Fine-tuning for Flow-based\\n  Vision-Language-Action Models',\n",
       "  'summary': 'Vision-Language-Action (VLA) models enable robots to understand and perform\\ncomplex tasks from multimodal input. Although recent work explores using\\nreinforcement learning (RL) to automate the laborious data collection process\\nin scaling supervised fine-tuning (SFT), applying large-scale RL to flow-based\\nVLAs (e.g., $\\\\pi_0$, $\\\\pi_{0.5}$) remains challenging due to intractable action\\nlog-likelihoods from iterative denoising.\\n  We address this challenge with $\\\\pi_{\\\\text{RL}}$, an open-source framework\\nfor training flow-based VLAs in parallel simulation. $\\\\pi_{\\\\text{RL}}$\\nimplements two RL algorithms: (1) {Flow-Noise} models the denoising process as\\na discrete-time MDP with a learnable noise network for exact log-likelihood\\ncomputation. (2) {Flow-SDE} integrates denoising with agent-environment\\ninteraction, formulating a two-layer MDP that employs ODE-to-SDE conversion for\\nefficient RL exploration.\\n  We evaluate $\\\\pi_{\\\\text{RL}}$ on LIBERO and ManiSkill benchmarks. On LIBERO,\\n$\\\\pi_{\\\\text{RL}}$ boosts few-shot SFT models $\\\\pi_0$ and $\\\\pi_{0.5}$ from 57.6%\\nto 97.6% and from 77.1% to 98.3%, respectively. In ManiSkill, we train\\n$\\\\pi_{\\\\text{RL}}$ in 320 parallel environments, improving $\\\\pi_0$ from 41.6% to\\n85.7% and $\\\\pi_{0.5}$ from 40.0% to 84.8% across 4352 pick-and-place tasks,\\ndemonstrating scalable multitask RL under heterogeneous simulation.\\n  Overall, $\\\\pi_{\\\\text{RL}}$ achieves significant performance gains and\\nstronger generalization over SFT-models, validating the effectiveness of online\\nRL for flow-based VLAs.',\n",
       "  'link': 'http://arxiv.org/abs/2510.25889v1',\n",
       "  'published': datetime.datetime(2025, 10, 29, 18, 37, 39, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context\\n  Learning',\n",
       "  'summary': 'Visual effects (VFX) are crucial to the expressive power of digital media,\\nyet their creation remains a major challenge for generative AI. Prevailing\\nmethods often rely on the one-LoRA-per-effect paradigm, which is\\nresource-intensive and fundamentally incapable of generalizing to unseen\\neffects, thus limiting scalability and creation. To address this challenge, we\\nintroduce VFXMaster, the first unified, reference-based framework for VFX video\\ngeneration. It recasts effect generation as an in-context learning task,\\nenabling it to reproduce diverse dynamic effects from a reference video onto\\ntarget content. In addition, it demonstrates remarkable generalization to\\nunseen effect categories. Specifically, we design an in-context conditioning\\nstrategy that prompts the model with a reference example. An in-context\\nattention mask is designed to precisely decouple and inject the essential\\neffect attributes, allowing a single unified model to master the effect\\nimitation without information leakage. In addition, we propose an efficient\\none-shot effect adaptation mechanism to boost generalization capability on\\ntough unseen effects from a single user-provided video rapidly. Extensive\\nexperiments demonstrate that our method effectively imitates various categories\\nof effect information and exhibits outstanding generalization to out-of-domain\\neffects. To foster future research, we will release our code, models, and a\\ncomprehensive dataset to the community.',\n",
       "  'link': 'http://arxiv.org/abs/2510.25772v1',\n",
       "  'published': datetime.datetime(2025, 10, 29, 17, 59, 53, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Interpreting LLMs as Credit Risk Classifiers: Do Their Feature\\n  Explanations Align with Classical ML?',\n",
       "  'summary': 'Large Language Models (LLMs) are increasingly explored as flexible\\nalternatives to classical machine learning models for classification tasks\\nthrough zero-shot prompting. However, their suitability for structured tabular\\ndata remains underexplored, especially in high-stakes financial applications\\nsuch as financial risk assessment. This study conducts a systematic comparison\\nbetween zero-shot LLM-based classifiers and LightGBM, a state-of-the-art\\ngradient-boosting model, on a real-world loan default prediction task. We\\nevaluate their predictive performance, analyze feature attributions using SHAP,\\nand assess the reliability of LLM-generated self-explanations. While LLMs are\\nable to identify key financial risk indicators, their feature importance\\nrankings diverge notably from LightGBM, and their self-explanations often fail\\nto align with empirical SHAP attributions. These findings highlight the\\nlimitations of LLMs as standalone models for structured financial risk\\nprediction and raise concerns about the trustworthiness of their self-generated\\nexplanations. Our results underscore the need for explainability audits,\\nbaseline comparisons with interpretable models, and human-in-the-loop oversight\\nwhen deploying LLMs in risk-sensitive financial environments.',\n",
       "  'link': 'http://arxiv.org/abs/2510.25701v1',\n",
       "  'published': datetime.datetime(2025, 10, 29, 17, 5, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic\\n  Health Record Analysis',\n",
       "  'summary': 'Electronic Health Records (EHRs) contain rich yet complex information, and\\ntheir automated analysis is critical for clinical decision-making. Despite\\nrecent advances of large language models (LLMs) in clinical workflows, their\\nability to analyze EHRs remains limited due to narrow task coverage and lack of\\nEHR-oriented reasoning capabilities. This paper aims to bridge the gap,\\nspecifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning\\ninstruction dataset, comprising 300k high-quality reasoning cases and 4M\\nnon-reasoning cases across 42 distinct EHR tasks. Its core innovation is a\\nthinking-graph-driven framework that enables to generate high-quality reasoning\\ndata at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced\\nLLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage\\ntraining paradigm, including domain adaptation, reasoning enhancement, and\\nreinforcement learning, EHR-R1 systematically acquires domain knowledge and\\ndiverse reasoning capabilities, enabling accurate and robust EHR analysis.\\nLastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning\\n42 tasks, to comprehensively assess reasoning and prediction across EHR\\nscenarios. In experiments, we show that the resulting EHR-R1 consistently\\noutperforms state-of-the-art commercial and open-source LLMs (including\\nDeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and\\nachieving a 10\\\\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins,\\nEHR-R1, and EHR-Bench have significantly advanced the development for more\\nreliable and clinically relevant EHR analysis.',\n",
       "  'link': 'http://arxiv.org/abs/2510.25628v1',\n",
       "  'published': datetime.datetime(2025, 10, 29, 15, 32, 47, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'SPADE: Sparsity Adaptive Depth Estimator for Zero-Shot, Real-Time,\\n  Monocular Depth Estimation in Underwater Environments',\n",
       "  'summary': 'Underwater infrastructure requires frequent inspection and maintenance due to\\nharsh marine conditions. Current reliance on human divers or remotely operated\\nvehicles is limited by perceptual and operational challenges, especially around\\ncomplex structures or in turbid water. Enhancing the spatial awareness of\\nunderwater vehicles is key to reducing piloting risks and enabling greater\\nautonomy. To address these challenges, we present SPADE: SParsity Adaptive\\nDepth Estimator, a monocular depth estimation pipeline that combines\\npre-trained relative depth estimator with sparse depth priors to produce dense,\\nmetric scale depth maps. Our two-stage approach first scales the relative depth\\nmap with the sparse depth points, then refines the final metric prediction with\\nour proposed Cascade Conv-Deformable Transformer blocks. Our approach achieves\\nimproved accuracy and generalisation over state-of-the-art baselines and runs\\nefficiently at over 15 FPS on embedded hardware, promising to support practical\\nunderwater inspection and intervention. This work has been submitted to IEEE\\nJournal of Oceanic Engineering Special Issue of AUV 2026.',\n",
       "  'link': 'http://arxiv.org/abs/2510.25463v1',\n",
       "  'published': datetime.datetime(2025, 10, 29, 12, 37, 34, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Critical Study of Automatic Evaluation in Sign Language Translation',\n",
       "  'summary': 'Automatic evaluation metrics are crucial for advancing sign language\\ntranslation (SLT). Current SLT evaluation metrics, such as BLEU and ROUGE, are\\nonly text-based, and it remains unclear to what extent text-based metrics can\\nreliably capture the quality of SLT outputs. To address this gap, we\\ninvestigate the limitations of text-based SLT evaluation metrics by analyzing\\nsix metrics, including BLEU, chrF, and ROUGE, as well as BLEURT on the one\\nhand, and large language model (LLM)-based evaluators such as G-Eval and GEMBA\\nzero-shot direct assessment on the other hand. Specifically, we assess the\\nconsistency and robustness of these metrics under three controlled conditions:\\nparaphrasing, hallucinations in model outputs, and variations in sentence\\nlength. Our analysis highlights the limitations of lexical overlap metrics and\\ndemonstrates that while LLM-based evaluators better capture semantic\\nequivalence often missed by conventional metrics, they can also exhibit bias\\ntoward LLM-paraphrased translations. Moreover, although all metrics are able to\\ndetect hallucinations, BLEU tends to be overly sensitive, whereas BLEURT and\\nLLM-based evaluators are comparatively lenient toward subtle cases. This\\nmotivates the need for multimodal evaluation frameworks that extend beyond\\ntext-based metrics to enable a more holistic assessment of SLT outputs.',\n",
       "  'link': 'http://arxiv.org/abs/2510.25434v1',\n",
       "  'published': datetime.datetime(2025, 10, 29, 11, 57, 3, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Prototype-Driven Adaptation for Few-Shot Object Detection',\n",
       "  'summary': 'Few-shot object detection (FSOD) often suffers from base-class bias and\\nunstable calibration when only a few novel samples are available. We propose\\nPrototype-Driven Alignment (PDA), a lightweight, plug-in metric head for DeFRCN\\nthat provides a prototype-based \"second opinion\" complementary to the linear\\nclassifier. PDA maintains support-only prototypes in a learnable\\nidentity-initialized projection space and optionally applies\\nprototype-conditioned RoI alignment to reduce geometric mismatch. During\\nfine-tuning, prototypes can be adapted via exponential moving average(EMA)\\nupdates on labeled foreground RoIs-without introducing class-specific\\nparameters-and are frozen at inference to ensure strict protocol compliance.\\nPDA employs a best-of-K matching scheme to capture intra-class multi-modality\\nand temperature-scaled fusion to combine metric similarities with detector\\nlogits. Experiments on VOC FSOD and GFSOD benchmarks show that PDA consistently\\nimproves novel-class performance with minimal impact on base classes and\\nnegligible computational overhead.',\n",
       "  'link': 'http://arxiv.org/abs/2510.25318v1',\n",
       "  'published': datetime.datetime(2025, 10, 29, 9, 32, 42, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Teaching Sarcasm: Few-Shot Multimodal Sarcasm Detection via Distillation\\n  to a Parameter-Efficient Student',\n",
       "  'summary': \"Multimodal sarcasm detection is challenging, especially in low-resource\\nsettings where subtle image-text contradictions are hard to learn due to scarce\\nannotated data, which hinders the model's performance. Parameter-efficient\\nfine-tuning (PEFT) methods like adapters, LoRA, and prompt tuning reduce\\noverfitting but struggle to reach optimal performance due to limited\\nsupervision from few-shot data. We propose PEKD, a unified framework that\\nenhances PEFT methods via distillation from an expert model trained on\\nlarge-scale sarcasm data, which acts as the teacher. To mitigate unreliable\\nsignals from the teacher, we introduce an entropy-aware gating mechanism that\\ndynamically adjusts the distillation strength based on teacher confidence.\\nExperiments on two public datasets demonstrate that our PEKD framework enables\\nPEFT methods to outperform both prior parameter-efficient approaches and large\\nmultimodal models, achieving strong results in the few-shot scenario. The\\nframework is modular and adaptable to a wide range of multimodal models and\\ntasks.\",\n",
       "  'link': 'http://arxiv.org/abs/2510.25303v1',\n",
       "  'published': datetime.datetime(2025, 10, 29, 9, 14, 41, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'LangHOPS: Language Grounded Hierarchical Open-Vocabulary Part\\n  Segmentation',\n",
       "  'summary': 'We propose LangHOPS, the first Multimodal Large Language Model (MLLM) based\\nframework for open-vocabulary object-part instance segmentation. Given an\\nimage, LangHOPS can jointly detect and segment hierarchical object and part\\ninstances from open-vocabulary candidate categories. Unlike prior approaches\\nthat rely on heuristic or learnable visual grouping, our approach grounds\\nobject-part hierarchies in language space. It integrates the MLLM into the\\nobject-part parsing pipeline to leverage its rich knowledge and reasoning\\ncapabilities, and link multi-granularity concepts within the hierarchies. We\\nevaluate LangHOPS across multiple challenging scenarios, including in-domain\\nand cross-dataset object-part instance segmentation, and zero-shot semantic\\nsegmentation. LangHOPS achieves state-of-the-art results, surpassing previous\\nmethods by 5.5% Average Precision (AP) (in-domain) and 4.8% (cross-dataset) on\\nthe PartImageNet dataset and by 2.5% mIOU on unseen object parts in ADE20K\\n(zero-shot). Ablation studies further validate the effectiveness of the\\nlanguage-grounded hierarchy and MLLM driven part query refinement strategy. The\\ncode will be released here.',\n",
       "  'link': 'http://arxiv.org/abs/2510.25263v1',\n",
       "  'published': datetime.datetime(2025, 10, 29, 8, 21, 59, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'LISTEN to Your Preferences: An LLM Framework for Multi-Objective\\n  Selection',\n",
       "  'summary': \"Human experts often struggle to select the best option from a large set of\\nitems with multiple competing objectives, a process bottlenecked by the\\ndifficulty of formalizing complex, implicit preferences. To address this, we\\nintroduce LISTEN, a framework that leverages a Large Language Model (LLM) as a\\nzero-shot preference oracle, guided only by an expert's high-level priorities\\nin natural language. To operate within LLM constraints like context windows and\\ninference costs, we propose two iterative algorithms: LISTEN-U, which uses the\\nLLM to refine a parametric utility function, and LISTEN-T, a non-parametric\\nmethod that performs tournament-style selections over small batches of\\nsolutions. Evaluated on diverse tasks including flight booking, shopping, and\\nexam scheduling, our results show LISTEN-U excels when preferences are\\nparametrically aligned (a property we measure with a novel concordance metric),\\nwhile LISTEN-T offers more robust performance. This work explores a promising\\ndirection for steering complex multi-objective decisions directly with natural\\nlanguage, reducing the cognitive burden of traditional preference elicitation.\",\n",
       "  'link': 'http://arxiv.org/abs/2510.25799v1',\n",
       "  'published': datetime.datetime(2025, 10, 29, 3, 17, 37, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome\\n  Supervision for KBQA',\n",
       "  'summary': 'Knowledge Base Question Answering (KBQA) aims to answer natural-language\\nquestions over a structured Knowledge Base (KB). Recent work improves KBQA by\\nadopting an agentic reasoning paradigm, in which Large Language Models (LLMs)\\niteratively decompose a question, generate its corresponding logical queries,\\nand interact with the KB to derive the answer. However, these methods typically\\nfine-tune LLMs on reasoning trajectories synthesized via process supervision,\\nwhich offers weak incentives for exploration and thus fails to strengthen the\\nagentic reasoning ability. In this paper, we propose KnowCoder-A1, an LLM that\\ncan autonomously perform agentic reasoning on KBs to obtain answers. To\\nincentivize autonomous exploration, KnowCoder-A1 trains the LLM under\\noutcome-only supervision via a multi-stage curriculum reinforcement learning\\nwith an easy-to-hard curriculum. To establish foundational agentic\\ncapabilities, KnowCoder-A1 first fine-tunes the LLM on a small set of\\nhigh-quality trajectories obtained through outcome-based rejection sampling.\\nThen, to alleviate the reward sparsity inherent in outcome-only supervision, it\\napplies multi-stage curriculum RL with reward schedules that progress from easy\\nto hard. Trained with outcome-only supervision, KnowCoder-A1 exhibits powerful\\nreasoning behaviors and consistently outperforms prior approaches across three\\nmainstream datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1\\nachieves up to an 11.1% relative improvement while using only one-twelfth of\\nthe training data, demonstrating strong agentic reasoning capabilities.',\n",
       "  'link': 'http://arxiv.org/abs/2510.25101v1',\n",
       "  'published': datetime.datetime(2025, 10, 29, 2, 12, 18, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI\\n  Detection',\n",
       "  'summary': 'Zero-shot Human-Object Interaction detection aims to localize humans and\\nobjects in an image and recognize their interaction, even when specific\\nverb-object pairs are unseen during training. Recent works have shown promising\\nresults using prompt learning with pretrained vision-language models such as\\nCLIP, which align natural language prompts with visual features in a shared\\nembedding space. However, existing approaches still fail to handle the visual\\ncomplexity of interaction, including (1) intra-class visual diversity, where\\ninstances of the same verb appear in diverse poses and contexts, and (2)\\ninter-class visual entanglement, where distinct verbs yield visually similar\\npatterns. To address these challenges, we propose VDRP, a framework for Visual\\nDiversity and Region-aware Prompt learning. First, we introduce a visual\\ndiversity-aware prompt learning strategy that injects group-wise visual\\nvariance into the context embedding. We further apply Gaussian perturbation to\\nencourage the prompts to capture diverse visual variations of a verb. Second,\\nwe retrieve region-specific concepts from the human, object, and union regions.\\nThese are used to augment the diversity-aware prompt embeddings, yielding\\nregion-aware prompts that enhance verb-level discrimination. Experiments on the\\nHICO-DET benchmark demonstrate that our method achieves state-of-the-art\\nperformance under four zero-shot evaluation settings, effectively addressing\\nboth intra-class diversity and inter-class visual entanglement. Code is\\navailable at https://github.com/mlvlab/VDRP.',\n",
       "  'link': 'http://arxiv.org/abs/2510.25094v1',\n",
       "  'published': datetime.datetime(2025, 10, 29, 1, 58, 35, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Vision-Language Integration for Zero-Shot Scene Understanding in\\n  Real-World Environments',\n",
       "  'summary': 'Zero-shot scene understanding in real-world settings presents major\\nchallenges due to the complexity and variability of natural scenes, where\\nmodels must recognize new objects, actions, and contexts without prior labeled\\nexamples. This work proposes a vision-language integration framework that\\nunifies pre-trained visual encoders (e.g., CLIP, ViT) and large language models\\n(e.g., GPT-based architectures) to achieve semantic alignment between visual\\nand textual modalities. The goal is to enable robust zero-shot comprehension of\\nscenes by leveraging natural language as a bridge to generalize over unseen\\ncategories and contexts. Our approach develops a unified model that embeds\\nvisual inputs and textual prompts into a shared space, followed by multimodal\\nfusion and reasoning layers for contextual interpretation. Experiments on\\nVisual Genome, COCO, ADE20K, and custom real-world datasets demonstrate\\nsignificant gains over state-of-the-art zero-shot models in object recognition,\\nactivity detection, and scene captioning. The proposed system achieves up to\\n18% improvement in top-1 accuracy and notable gains in semantic coherence\\nmetrics, highlighting the effectiveness of cross-modal alignment and language\\ngrounding in enhancing generalization for real-world scene understanding.',\n",
       "  'link': 'http://arxiv.org/abs/2510.25070v1',\n",
       "  'published': datetime.datetime(2025, 10, 29, 1, 16, 21, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'DRIP: Dynamic patch Reduction via Interpretable Pooling',\n",
       "  'summary': 'Recently, the advances in vision-language models, including contrastive\\npretraining and instruction tuning, have greatly pushed the frontier of\\nmultimodal AI. However, owing to the large-scale and hence expensive\\npretraining, the efficiency concern has discouraged researchers from attempting\\nto pretrain a vision language model from scratch. In this work, we propose\\nDynamic patch Reduction via Interpretable Pooling (DRIP), which adapts to the\\ninput images and dynamically merges tokens in the deeper layers of a visual\\nencoder. Our results on both ImageNet training from scratch and CLIP\\ncontrastive pretraining demonstrate a significant GFLOP reduction while\\nmaintaining comparable classification/zero-shot performance. To further\\nvalidate our proposed method, we conduct continual pretraining on a large\\nbiology dataset, extending its impact into scientific domains.',\n",
       "  'link': 'http://arxiv.org/abs/2510.25067v1',\n",
       "  'published': datetime.datetime(2025, 10, 29, 1, 10, 28, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'StorageXTuner: An LLM Agent-Driven Automatic Tuning Framework for\\n  Heterogeneous Storage Systems',\n",
       "  'summary': 'Automatically configuring storage systems is hard: parameter spaces are large\\nand conditions vary across workloads, deployments, and versions. Heuristic and\\nML tuners are often system specific, require manual glue, and degrade under\\nchanges. Recent LLM-based approaches help but usually treat tuning as a\\nsingle-shot, system-specific task, which limits cross-system reuse, constrains\\nexploration, and weakens validation. We present StorageXTuner, an LLM\\nagent-driven auto-tuning framework for heterogeneous storage engines.\\nStorageXTuner separates concerns across four agents - Executor (sandboxed\\nbenchmarking), Extractor (performance digest), Searcher (insight-guided\\nconfiguration exploration), and Reflector (insight generation and management).\\nThe design couples an insight-driven tree search with layered memory that\\npromotes empirically validated insights and employs lightweight checkers to\\nguard against unsafe actions. We implement a prototype and evaluate it on\\nRocksDB, LevelDB, CacheLib, and MySQL InnoDB with YCSB, MixGraph, and TPC-H/C.\\nRelative to out-of-the-box settings and to ELMo-Tune, StorageXTuner reaches up\\nto 575% and 111% higher throughput, reduces p99 latency by as much as 88% and\\n56%, and converges with fewer trials.',\n",
       "  'link': 'http://arxiv.org/abs/2510.25017v1',\n",
       "  'published': datetime.datetime(2025, 10, 28, 22, 33, 14, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality\\n  Games?',\n",
       "  'summary': \"Virtual Reality (VR) games require players to translate high-level semantic\\nactions into precise device manipulations using controllers and head-mounted\\ndisplays (HMDs). While humans intuitively perform this translation based on\\ncommon sense and embodied understanding, whether Large Language Models (LLMs)\\ncan effectively replicate this ability remains underexplored. This paper\\nintroduces a benchmark, ComboBench, evaluating LLMs' capability to translate\\nsemantic actions into VR device manipulation sequences across 262 scenarios\\nfrom four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II,\\nand Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o,\\nGemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared against\\nannotated ground truth and human performance. Our results reveal that while\\ntop-performing models like Gemini-1.5-Pro demonstrate strong task decomposition\\ncapabilities, they still struggle with procedural reasoning and spatial\\nunderstanding compared to humans. Performance varies significantly across\\ngames, suggesting sensitivity to interaction complexity. Few-shot examples\\nsubstantially improve performance, indicating potential for targeted\\nenhancement of LLMs' VR manipulation capabilities. We release all materials at\\nhttps://sites.google.com/view/combobench.\",\n",
       "  'link': 'http://arxiv.org/abs/2510.24706v1',\n",
       "  'published': datetime.datetime(2025, 10, 28, 17, 55, 42, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Zero-Shot Cross-Lingual Transfer using Prefix-Based Adaptation',\n",
       "  'summary': 'With the release of new large language models (LLMs) like Llama and Mistral,\\nzero-shot cross-lingual transfer has become increasingly feasible due to their\\nmultilingual pretraining and strong generalization capabilities. However,\\nadapting these decoder-only LLMs to new tasks across languages remains\\nchallenging. While parameter-efficient fine-tuning (PeFT) techniques like\\nLow-Rank Adaptation (LoRA) are widely used, prefix-based techniques such as\\nsoft prompt tuning, prefix tuning, and Llama Adapter are less explored,\\nespecially for zero-shot transfer in decoder-only models. We present a\\ncomprehensive study of three prefix-based methods for zero-shot cross-lingual\\ntransfer from English to 35+ high- and low-resource languages. Our analysis\\nfurther explores transfer across linguistic families and scripts, as well as\\nthe impact of scaling model sizes from 1B to 24B. With Llama 3.1 8B, prefix\\nmethods outperform LoRA-baselines by up to 6% on the Belebele benchmark.\\nSimilar improvements were observed with Mistral v0.3 7B as well. Despite using\\nonly 1.23M learning parameters with prefix tuning, we achieve consistent\\nimprovements across diverse benchmarks. These findings highlight the potential\\nof prefix-based techniques as an effective and scalable alternative to LoRA,\\nparticularly in low-resource multilingual settings.',\n",
       "  'link': 'http://arxiv.org/abs/2510.24619v1',\n",
       "  'published': datetime.datetime(2025, 10, 28, 16, 48, 3, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Beyond MCQ: An Open-Ended Arabic Cultural QA Benchmark with Dialect\\n  Variants',\n",
       "  'summary': 'Large Language Models (LLMs) are increasingly used to answer everyday\\nquestions, yet their performance on culturally grounded and dialectal content\\nremains uneven across languages. We propose a comprehensive method that (i)\\ntranslates Modern Standard Arabic (MSA) multiple-choice questions (MCQs) into\\nEnglish and several Arabic dialects, (ii) converts them into open-ended\\nquestions (OEQs), (iii) benchmarks a range of zero-shot and fine-tuned LLMs\\nunder both MCQ and OEQ settings, and (iv) generates chain-of-thought (CoT)\\nrationales to fine-tune models for step-by-step reasoning. Using this method,\\nwe extend an existing dataset in which QAs are parallelly aligned across\\nmultiple language varieties, making it, to our knowledge, the first of its\\nkind. We conduct extensive experiments with both open and closed models. Our\\nfindings show that (i) models underperform on Arabic dialects, revealing\\npersistent gaps in culturally grounded and dialect-specific knowledge; (ii)\\nArabic-centric models perform well on MCQs but struggle with OEQs; and (iii)\\nCoT improves judged correctness while yielding mixed n-gram-based metrics. The\\ndeveloped dataset will be publicly released to support further research on\\nculturally and linguistically inclusive evaluation.',\n",
       "  'link': 'http://arxiv.org/abs/2510.24328v1',\n",
       "  'published': datetime.datetime(2025, 10, 28, 11, 52, 51, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt\\n  Learning',\n",
       "  'summary': 'Remote sensing applications increasingly rely on deep learning for scene\\nclassification. However, their performance is often constrained by the scarcity\\nof labeled data and the high cost of annotation across diverse geographic and\\nsensor domains. While recent vision-language models like CLIP have shown\\npromise by learning transferable representations at scale by aligning visual\\nand textual modalities, their direct application to remote sensing remains\\nsuboptimal due to significant domain gaps and the need for task-specific\\nsemantic adaptation. To address this critical challenge, we systematically\\nexplore prompt learning as a lightweight and efficient adaptation strategy for\\nfew-shot remote sensing image scene classification. We evaluate several\\nrepresentative methods, including Context Optimization, Conditional Context\\nOptimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating\\nConstraints. These approaches reflect complementary design philosophies: from\\nstatic context optimization to conditional prompts for enhanced generalization,\\nmulti-modal prompts for joint vision-language adaptation, and semantically\\nregularized prompts for stable learning without forgetting. We benchmark these\\nprompt-learning methods against two standard baselines: zero-shot CLIP with\\nhand-crafted prompts and a linear probe trained on frozen CLIP features.\\nThrough extensive experiments on multiple benchmark remote sensing datasets,\\nincluding cross-dataset generalization tests, we demonstrate that prompt\\nlearning consistently outperforms both baselines in few-shot scenarios.\\nNotably, Prompting with Self-Regulating Constraints achieves the most robust\\ncross-domain performance. Our findings underscore prompt learning as a scalable\\nand efficient solution for bridging the domain gap in satellite and aerial\\nimagery, providing a strong foundation for future research in this field.',\n",
       "  'link': 'http://arxiv.org/abs/2510.24321v1',\n",
       "  'published': datetime.datetime(2025, 10, 28, 11, 39, 22, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Training-free Source Attribution of AI-generated Images via Resynthesis',\n",
       "  'summary': 'Synthetic image source attribution is a challenging task, especially in data\\nscarcity conditions requiring few-shot or zero-shot classification\\ncapabilities. We present a new training-free one-shot attribution method based\\non image resynthesis. A prompt describing the image under analysis is\\ngenerated, then it is used to resynthesize the image with all the candidate\\nsources. The image is attributed to the model which produced the resynthesis\\nclosest to the original image in a proper feature space. We also introduce a\\nnew dataset for synthetic image attribution consisting of face images from\\ncommercial and open-source text-to-image generators. The dataset provides a\\nchallenging attribution framework, useful for developing new attribution models\\nand testing their capabilities on different generative architectures. The\\ndataset structure allows to test approaches based on resynthesis and to compare\\nthem to few-shot methods. Results from state-of-the-art few-shot approaches and\\nother baselines show that the proposed resynthesis method outperforms existing\\ntechniques when only a few samples are available for training or fine-tuning.\\nThe experiments also demonstrate that the new dataset is a challenging one and\\nrepresents a valuable benchmark for developing and evaluating future few-shot\\nand zero-shot methods.',\n",
       "  'link': 'http://arxiv.org/abs/2510.24278v1',\n",
       "  'published': datetime.datetime(2025, 10, 28, 10, 39, 4, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Towards Transparent Reasoning: What Drives Faithfulness in Large\\n  Language Models?',\n",
       "  'summary': 'Large Language Models (LLMs) often produce explanations that do not\\nfaithfully reflect the factors driving their predictions. In healthcare\\nsettings, such unfaithfulness is especially problematic: explanations that omit\\nsalient clinical cues or mask spurious shortcuts can undermine clinician trust\\nand lead to unsafe decision support. We study how inference and training-time\\nchoices shape explanation faithfulness, focusing on factors practitioners can\\ncontrol at deployment. We evaluate three LLMs (GPT-4.1-mini, LLaMA 70B, LLaMA\\n8B) on two datasets-BBQ (social bias) and MedQA (medical licensing questions),\\nand manipulate the number and type of few-shot examples, prompting strategies,\\nand training procedure. Our results show: (i) both the quantity and quality of\\nfew-shot examples significantly impact model faithfulness; (ii) faithfulness is\\nsensitive to prompting design; (iii) the instruction-tuning phase improves\\nmeasured faithfulness on MedQA. These findings offer insights into strategies\\nfor enhancing the interpretability and trustworthiness of LLMs in sensitive\\ndomains.',\n",
       "  'link': 'http://arxiv.org/abs/2510.24236v1',\n",
       "  'published': datetime.datetime(2025, 10, 28, 9, 43, 49, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Enhancing Vision-Language Models for Autonomous Driving through\\n  Task-Specific Prompting and Spatial Reasoning',\n",
       "  'summary': 'This technical report presents our solution for the RoboSense Challenge at\\nIROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous driving\\nscene understanding across perception, prediction, planning, and corruption\\ndetection tasks. We propose a systematic framework built on four core\\ncomponents. First, a Mixture-of-Prompts router classifies questions and\\ndispatches them to task-specific expert prompts, eliminating interference\\nacross diverse question types. Second, task-specific prompts embed explicit\\ncoordinate systems, spatial reasoning rules, role-playing,\\nChain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored to\\neach task. Third, a visual assembly module composes multi-view images with\\nobject crops, magenta markers, and adaptive historical frames based on question\\nrequirements. Fourth, we configure model inference parameters (temperature,\\ntop-p, message roles) per task to optimize output quality. Implemented on\\nQwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (clean\\ndata) and 72.85% on Phase-2 (corrupted data), demonstrating that structured\\nprompting and spatial grounding substantially enhance VLM performance on\\nsafety-critical autonomous driving tasks. Code and prompt are available at\\nhttps://github.com/wuaodi/UCAS-CSU-phase2.',\n",
       "  'link': 'http://arxiv.org/abs/2510.24152v1',\n",
       "  'published': datetime.datetime(2025, 10, 28, 7, 43, 30, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Ko-MuSR: A Multistep Soft Reasoning Benchmark for LLMs Capable of\\n  Understanding Korean',\n",
       "  'summary': 'We present Ko-MuSR, the first benchmark to comprehensively evaluate\\nmultistep, soft reasoning in long Korean narratives while minimizing data\\ncontamination. Built following MuSR, Ko-MuSR features fully Korean narratives,\\nreasoning chains, and multiple-choice questions verified by human annotators\\nfor logical consistency and answerability. Evaluations of four large language\\nmodels -- two multilingual and two Korean-specialized -- show that multilingual\\nmodels outperform Korean-focused ones even in Korean reasoning tasks,\\nindicating cross-lingual generalization of reasoning ability. Carefully\\ndesigned prompting strategies, which combine few-shot examples, reasoning\\ntraces, and task-specific hints, further boost accuracy, approaching\\nhuman-level performance. Ko-MuSR offers a solid foundation for advancing Korean\\nNLP by enabling systematic evaluation of long-context reasoning and prompting\\nstrategies.',\n",
       "  'link': 'http://arxiv.org/abs/2510.24150v1',\n",
       "  'published': datetime.datetime(2025, 10, 28, 7, 42, 59, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained\\n  Classification',\n",
       "  'summary': \"Text-to-image (T2I) models are increasingly used for synthetic dataset\\ngeneration, but generating effective synthetic training data for classification\\nremains challenging. Fine-tuning a T2I model with a few real examples can help\\nimprove the quality of synthetic training data; however, it may also cause\\noverfitting and reduce diversity in the generated samples. We propose a\\nfine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for\\nfine-grained classification. Given a small set of real examples, we first\\nextract class-agnostic attributes such as scene background and object pose. We\\nthen explicitly condition on these attributes during fine-tuning of the T2I\\nmodel and marginalize them out during generation. This design mitigates\\noverfitting, preserves the T2I model's generative prior, reduces estimation\\nerrors, and further minimizes unintended inter-class associations. Extensive\\nexperiments across multiple T2I models, backbones, and datasets show that our\\nmethod achieves state-of-the-art performance in low-shot fine-grained\\nclassification when augmented with synthetic data. Concretely, BOB outperforms\\nDataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning\\na CLIP classifier with five real images augmented with 100 synthetic images).\\nIn three of the four benchmarks, fine-tuning downstream models with 5 real\\nimages augmented with BOB achieves better performance than fine-tuning with 10\\nreal images. Collectively, BOB outperforms prior art in 18 of 24 experimental\\nsettings, with 2+% accuracy improvements in 14 of these settings.\",\n",
       "  'link': 'http://arxiv.org/abs/2510.24078v1',\n",
       "  'published': datetime.datetime(2025, 10, 28, 5, 40, 14, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Enhancing CLIP Robustness via Cross-Modality Alignment',\n",
       "  'summary': \"Vision-language models (VLMs) such as CLIP demonstrate strong generalization\\nin zero-shot classification but remain highly vulnerable to adversarial\\nperturbations. Existing methods primarily focus on adversarial fine-tuning or\\nprompt optimization; they often overlook the gaps in CLIP's encoded features,\\nwhich is shown as the text and image features lie far apart from each other.\\nThis misalignment is significantly amplified under adversarial perturbations,\\nleading to severe degradation in classification performance. To address this\\nproblem, we propose Cross-modality Alignment, dubbed COLA, an optimal\\ntransport-based framework that explicitly addresses adversarial misalignment by\\nrestoring both global image-text alignment and local structural consistency in\\nthe feature space. (1) COLA first projects adversarial image embeddings onto a\\nsubspace spanned by class text features, effectively filtering out non-semantic\\ndistortions while preserving discriminative information. (2) It then models\\nimages and texts as discrete distributions over multiple augmented views and\\nrefines their alignment via OT, with the subspace projection seamlessly\\nintegrated into the cost computation. This design ensures stable cross-modal\\nalignment even under adversarial conditions. COLA is training-free and\\ncompatible with existing fine-tuned models. Extensive evaluations across 14\\nzero-shot classification benchmarks demonstrate the effectiveness of COLA,\\nespecially with an average improvement of 6.7% on ImageNet and its variants\\nunder PGD adversarial attacks, while maintaining high accuracy on clean\\nsamples.\",\n",
       "  'link': 'http://arxiv.org/abs/2510.24038v1',\n",
       "  'published': datetime.datetime(2025, 10, 28, 3, 47, 44, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven\\n  Adversarial Prompts',\n",
       "  'summary': 'Despite rapid advancements in text-to-image (T2I) models, their safety\\nmechanisms are vulnerable to adversarial prompts, which maliciously generate\\nunsafe images. Current red-teaming methods for proactively assessing such\\nvulnerabilities usually require white-box access to T2I models, and rely on\\ninefficient per-prompt optimization, as well as inevitably generate\\nsemantically meaningless prompts easily blocked by filters. In this paper, we\\npropose APT (AutoPrompT), a black-box framework that leverages large language\\nmodels (LLMs) to automatically generate human-readable adversarial suffixes for\\nbenign prompts. We first introduce an alternating optimization-finetuning\\npipeline between adversarial suffix optimization and fine-tuning the LLM\\nutilizing the optimized suffix. Furthermore, we integrates a dual-evasion\\nstrategy in optimization phase, enabling the bypass of both perplexity-based\\nfilter and blacklist word filter: (1) we constrain the LLM generating\\nhuman-readable prompts through an auxiliary LLM perplexity scoring, which\\nstarkly contrasts with prior token-level gibberish, and (2) we also introduce\\nbanned-token penalties to suppress the explicit generation of banned-tokens in\\nblacklist. Extensive experiments demonstrate the excellent red-teaming\\nperformance of our human-readable, filter-resistant adversarial prompts, as\\nwell as superior zero-shot transferability which enables instant adaptation to\\nunseen prompts and exposes critical vulnerabilities even in commercial APIs\\n(e.g., Leonardo.Ai.).',\n",
       "  'link': 'http://arxiv.org/abs/2510.24034v1',\n",
       "  'published': datetime.datetime(2025, 10, 28, 3, 32, 14, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial\\n  Representations',\n",
       "  'summary': \"Humans learn abstract concepts through multisensory synergy, and once formed,\\nsuch representations can often be recalled from a single modality. Inspired by\\nthis principle, we introduce Concerto, a minimalist simulation of human concept\\nlearning for spatial cognition, combining 3D intra-modal self-distillation with\\n2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more\\ncoherent and informative spatial features, as demonstrated by zero-shot\\nvisualizations. It outperforms both standalone SOTA 2D and 3D self-supervised\\nmodels by 14.2% and 4.8%, respectively, as well as their feature concatenation,\\nin linear probing for 3D scene perception. With full fine-tuning, Concerto sets\\nnew SOTA results across multiple scene understanding benchmarks (e.g., 80.7%\\nmIoU on ScanNet). We further present a variant of Concerto tailored for\\nvideo-lifted point cloud spatial understanding, and a translator that linearly\\nprojects Concerto representations into CLIP's language space, enabling\\nopen-world perception. These results highlight that Concerto emerges spatial\\nrepresentations with superior fine-grained geometric and semantic consistency.\",\n",
       "  'link': 'http://arxiv.org/abs/2510.23607v1',\n",
       "  'published': datetime.datetime(2025, 10, 27, 17, 59, 59, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Think Twice: Branch-and-Rethink Reasoning Reward Model',\n",
       "  'summary': 'Large language models (LLMs) increasingly rely on thinking models that\\nexternalize intermediate steps and allocate extra test-time compute, with\\nthink-twice strategies showing that a deliberate second pass can elicit\\nstronger reasoning. In contrast, most reward models (RMs) still compress many\\nquality dimensions into a single scalar in one shot, a design that induces\\njudgment diffusion: attention spreads across evaluation criteria, yielding\\ndiluted focus and shallow analysis. We introduce branch-and-rethink (BR-RM), a\\ntwo-turn RM that transfers the think-twice principle to reward modeling. Turn 1\\nperforms adaptive branching, selecting a small set of instance-critical\\ndimensions (such as factuality and safety) and sketching concise,\\nevidence-seeking hypotheses. Turn 2 executes branch-conditioned rethinking, a\\ntargeted reread that tests those hypotheses and scrutinizes only what matters\\nmost. We train with GRPO-style reinforcement learning over structured two-turn\\ntraces using a simple binary outcome reward with strict format checks, making\\nthe approach compatible with standard RLHF pipelines. By converting\\nall-at-oncescoringintofocused, second-lookreasoning,\\nBR-RMreducesjudgmentdiffusionandimproves sensitivity to subtle yet\\nconsequential errors while remaining practical and scalable. Experimental\\nresults demonstrate that our model achieves state-of-the-art performance on\\nthree challenging reward modeling benchmarks across diverse domains. The code\\nand the model will be released soon.',\n",
       "  'link': 'http://arxiv.org/abs/2510.23596v1',\n",
       "  'published': datetime.datetime(2025, 10, 27, 17, 58, 7, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'More Than Generation: Unifying Generation and Depth Estimation via\\n  Text-to-Image Diffusion Models',\n",
       "  'summary': 'Generative depth estimation methods leverage the rich visual priors stored in\\npre-trained text-to-image diffusion models, demonstrating astonishing zero-shot\\ncapability. However, parameter updates during training lead to catastrophic\\ndegradation in the image generation capability of the pre-trained model. We\\nintroduce MERGE, a unified model for image generation and depth estimation,\\nstarting from a fixed pre-trained text-to-image model. MERGE demonstrates that\\nthe pre-trained text-to-image model can do more than image generation, but also\\nexpand to depth estimation effortlessly. Specifically, MERGE introduces a\\nplay-and-plug framework that enables seamless switching between image\\ngeneration and depth estimation modes through simple and pluggable converters.\\nMeanwhile, we propose a Group Reuse Mechanism to encourage parameter reuse and\\nimprove the utilization of the additional learnable parameters. MERGE unleashes\\nthe powerful depth estimation capability of the pre-trained text-to-image model\\nwhile preserving its original image generation ability. Compared to other\\nunified models for image generation and depth estimation, MERGE achieves\\nstate-of-the-art performance across multiple depth estimation benchmarks. The\\ncode will be made available at https://github.com/H-EmbodVis/MERGE',\n",
       "  'link': 'http://arxiv.org/abs/2510.23574v1',\n",
       "  'published': datetime.datetime(2025, 10, 27, 17, 44, 56, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'MMTutorBench: The First Multimodal Benchmark for AI Math Tutoring',\n",
       "  'summary': \"Effective math tutoring requires not only solving problems but also\\ndiagnosing students' difficulties and guiding them step by step. While\\nmultimodal large language models (MLLMs) show promise, existing benchmarks\\nlargely overlook these tutoring skills. We introduce MMTutorBench, the first\\nbenchmark for AI math tutoring, consisting of 685 problems built around\\npedagogically significant key-steps. Each problem is paired with\\nproblem-specific rubrics that enable fine-grained evaluation across six\\ndimensions, and structured into three tasks-Insight Discovery, Operation\\nFormulation, and Operation Execution. We evaluate 12 leading MLLMs and find\\nclear performance gaps between proprietary and open-source systems, substantial\\nroom compared to human tutors, and consistent trends across input variants: OCR\\npipelines degrade tutoring quality, few-shot prompting yields limited gains,\\nand our rubric-based LLM-as-a-Judge proves highly reliable. These results\\nhighlight both the difficulty and diagnostic value of MMTutorBench for\\nadvancing AI tutoring.\",\n",
       "  'link': 'http://arxiv.org/abs/2510.23477v1',\n",
       "  'published': datetime.datetime(2025, 10, 27, 16, 11, 49, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Evaluating Large Language Models for Stance Detection on Financial\\n  Targets from SEC Filing Reports and Earnings Call Transcripts',\n",
       "  'summary': \"Financial narratives from U.S. Securities and Exchange Commission (SEC)\\nfiling reports and quarterly earnings call transcripts (ECTs) are very\\nimportant for investors, auditors, and regulators. However, their length,\\nfinancial jargon, and nuanced language make fine-grained analysis difficult.\\nPrior sentiment analysis in the financial domain required a large, expensive\\nlabeled dataset, making the sentence-level stance towards specific financial\\ntargets challenging. In this work, we introduce a sentence-level corpus for\\nstance detection focused on three core financial metrics: debt, earnings per\\nshare (EPS), and sales. The sentences were extracted from Form 10-K annual\\nreports and ECTs, and labeled for stance (positive, negative, neutral) using\\nthe advanced ChatGPT-o3-pro model under rigorous human validation. Using this\\ncorpus, we conduct a systematic evaluation of modern large language models\\n(LLMs) using zero-shot, few-shot, and Chain-of-Thought (CoT) prompting\\nstrategies. Our results show that few-shot with CoT prompting performs best\\ncompared to supervised baselines, and LLMs' performance varies across the SEC\\nand ECT datasets. Our findings highlight the practical viability of leveraging\\nLLMs for target-specific stance in the financial domain without requiring\\nextensive labeled data.\",\n",
       "  'link': 'http://arxiv.org/abs/2510.23464v1',\n",
       "  'published': datetime.datetime(2025, 10, 27, 16, 3, 20, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Symmetria: A Synthetic Dataset for Learning in Point Clouds',\n",
       "  'summary': \"Unlike image or text domains that benefit from an abundance of large-scale\\ndatasets, point cloud learning techniques frequently encounter limitations due\\nto the scarcity of extensive datasets. To overcome this limitation, we present\\nSymmetria, a formula-driven dataset that can be generated at any arbitrary\\nscale. By construction, it ensures the absolute availability of precise ground\\ntruth, promotes data-efficient experimentation by requiring fewer samples,\\nenables broad generalization across diverse geometric settings, and offers easy\\nextensibility to new tasks and modalities. Using the concept of symmetry, we\\ncreate shapes with known structure and high variability, enabling neural\\nnetworks to learn point cloud features effectively. Our results demonstrate\\nthat this dataset is highly effective for point cloud self-supervised\\npre-training, yielding models with strong performance in downstream tasks such\\nas classification and segmentation, which also show good few-shot learning\\ncapabilities. Additionally, our dataset can support fine-tuning models to\\nclassify real-world objects, highlighting our approach's practical utility and\\napplication. We also introduce a challenging task for symmetry detection and\\nprovide a benchmark for baseline comparisons. A significant advantage of our\\napproach is the public availability of the dataset, the accompanying code, and\\nthe ability to generate very large collections, promoting further research and\\ninnovation in point cloud learning.\",\n",
       "  'link': 'http://arxiv.org/abs/2510.23414v1',\n",
       "  'published': datetime.datetime(2025, 10, 27, 15, 18, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'VideoTG-R1: Boosting Video Temporal Grounding via Curriculum\\n  Reinforcement Learning on Reflected Boundary Annotations',\n",
       "  'summary': 'Video temporal grounding (VTG) aims to locate precise segments in videos\\nbased on language queries, which is a fundamental challenge in video\\nunderstanding. While recent Multimodal Large Language Models (MLLMs) have shown\\npromise in tackling VTG through reinforcement learning (RL), they overlook the\\nchallenges arising from both the quality and difficulty of training samples.\\n(1) Partially annotated samples. Many samples contain relevant segments beyond\\nthe annotated interval, introducing ambiguous supervision. (2) Hard-to-ground\\nsamples. Samples with poor zero-shot performance produce consistently low and\\nindistinguishable rewards during RL training, exhibiting no clear preference\\namong multiple outputs and thus hindering learning efficiency. To address these\\nchallenges, we propose VideoTG-R1, a novel curriculum RL framework with\\nreflected boundary annotations, enabling data-efficient training. Specifically,\\nwe propose a Boundary Reflection Agent that utilizes MLLMs to predict\\nquery-relevant timestamps outside the annotated intervals, allowing us to\\nidentify and filter out partially annotated samples, thereby reducing\\nambiguity. Furthermore, we introduce a Difficulty Estimation Agent to assess\\nthe training difficulty of each sample and design a curriculum RL strategy that\\ndynamically masks the videos of hard-to-ground samples according to the\\ntraining steps, easing the training difficulty and providing clearer\\npreference. Experiments on the VTG and grounded VideoQA tasks demonstrate the\\neffectiveness of our method. Remarkably, with only 10% of the training samples\\nand 21% of the computational budget, VideoTG-R1 outperforms full-data\\ncounterparts under both group relative policy optimization (GRPO) and\\nsupervised fine-tuning (SFT). The code is available at\\nhttps://github.com/ldong1111/VideoTG-R1.',\n",
       "  'link': 'http://arxiv.org/abs/2510.23397v1',\n",
       "  'published': datetime.datetime(2025, 10, 27, 14, 55, 38, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Detecting Religious Language in Climate Discourse',\n",
       "  'summary': 'Religious language continues to permeate contemporary discourse, even in\\nostensibly secular domains such as environmental activism and climate change\\ndebates. This paper investigates how explicit and implicit forms of religious\\nlanguage appear in climate-related texts produced by secular and religious\\nnongovernmental organizations (NGOs). We introduce a dual methodological\\napproach: a rule-based model using a hierarchical tree of religious terms\\nderived from ecotheology literature, and large language models (LLMs) operating\\nin a zero-shot setting. Using a dataset of more than 880,000 sentences, we\\ncompare how these methods detect religious language and analyze points of\\nagreement and divergence. The results show that the rule-based method\\nconsistently labels more sentences as religious than LLMs. These findings\\nhighlight not only the methodological challenges of computationally detecting\\nreligious language but also the broader tension over whether religious language\\nshould be defined by vocabulary alone or by contextual meaning. This study\\ncontributes to digital methods in religious studies by demonstrating both the\\npotential and the limitations of approaches for analyzing how the sacred\\npersists in climate discourse.',\n",
       "  'link': 'http://arxiv.org/abs/2510.23395v1',\n",
       "  'published': datetime.datetime(2025, 10, 27, 14, 54, 51, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Evaluation of Vision-LLMs in Surveillance Video',\n",
       "  'summary': 'The widespread use of cameras in our society has created an overwhelming\\namount of video data, far exceeding the capacity for human monitoring. This\\npresents a critical challenge for public safety and security, as the timely\\ndetection of anomalous or criminal events is crucial for effective response and\\nprevention. The ability for an embodied agent to recognize unexpected events is\\nfundamentally tied to its capacity for spatial reasoning. This paper\\ninvestigates the spatial reasoning of vision-language models (VLMs) by framing\\nanomalous action recognition as a zero-shot, language-grounded task, addressing\\nthe embodied perception challenge of interpreting dynamic 3D scenes from sparse\\n2D video. Specifically, we investigate whether small, pre-trained vision--LLMs\\ncan act as spatially-grounded, zero-shot anomaly detectors by converting video\\ninto text descriptions and scoring labels via textual entailment. We evaluate\\nfour open models on UCF-Crime and RWF-2000 under prompting and\\nprivacy-preserving conditions. Few-shot exemplars can improve accuracy for some\\nmodels, but may increase false positives, and privacy filters -- especially\\nfull-body GAN transforms -- introduce inconsistencies that degrade accuracy.\\nThese results chart where current vision--LLMs succeed (simple, spatially\\nsalient events) and where they falter (noisy spatial cues, identity\\nobfuscation). Looking forward, we outline concrete paths to strengthen spatial\\ngrounding without task-specific training: structure-aware prompts, lightweight\\nspatial memory across clips, scene-graph or 3D-pose priors during description,\\nand privacy methods that preserve action-relevant geometry. This positions\\nzero-shot, language-grounded pipelines as adaptable building blocks for\\nembodied, real-world video understanding. Our implementation for evaluating\\nVLMs is publicly available at:\\nhttps://github.com/pascalbenschopTU/VLLM_AnomalyRecognition',\n",
       "  'link': 'http://arxiv.org/abs/2510.23190v1',\n",
       "  'published': datetime.datetime(2025, 10, 27, 10, 27, 2, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Finding 3D Scene Analogies with Multimodal Foundation Models',\n",
       "  'summary': 'Connecting current observations with prior experiences helps robots adapt and\\nplan in new, unseen 3D environments. Recently, 3D scene analogies have been\\nproposed to connect two 3D scenes, which are smooth maps that align scene\\nregions with common spatial relationships. These maps enable detailed transfer\\nof trajectories or waypoints, potentially supporting demonstration transfer for\\nimitation learning or task plan transfer across scenes. However, existing\\nmethods for the task require additional training and fixed object vocabularies.\\nIn this work, we propose to use multimodal foundation models for finding 3D\\nscene analogies in a zero-shot, open-vocabulary setting. Central to our\\napproach is a hybrid neural representation of scenes that consists of a sparse\\ngraph based on vision-language model features and a feature field derived from\\n3D shape foundation models. 3D scene analogies are then found in a\\ncoarse-to-fine manner, by first aligning the graph and refining the\\ncorrespondence with feature fields. Our method can establish accurate\\ncorrespondences between complex scenes, and we showcase applications in\\ntrajectory and waypoint transfer.',\n",
       "  'link': 'http://arxiv.org/abs/2510.23184v1',\n",
       "  'published': datetime.datetime(2025, 10, 27, 10, 23, 31, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Seeing the Unseen: Towards Zero-Shot Inspection for Wind Turbine Blades\\n  using Knowledge-Augmented Vision Language Models',\n",
       "  'summary': 'Wind turbine blades operate in harsh environments, making timely damage\\ndetection essential for preventing failures and optimizing maintenance.\\nDrone-based inspection and deep learning are promising, but typically depend on\\nlarge, labeled datasets, which limit their ability to detect rare or evolving\\ndamage types. To address this, we propose a zero-shot-oriented inspection\\nframework that integrates Retrieval-Augmented Generation (RAG) with\\nVision-Language Models (VLM). A multimodal knowledge base is constructed,\\ncomprising technical documentation, representative reference images, and\\ndomain-specific guidelines. A hybrid text-image retriever with keyword-aware\\nreranking assembles the most relevant context to condition the VLM at\\ninference, injecting domain knowledge without task-specific training. We\\nevaluate the framework on 30 labeled blade images covering diverse damage\\ncategories. Although the dataset is small due to the difficulty of acquiring\\nverified blade imagery, it covers multiple representative defect types. On this\\ntest set, the RAG-grounded VLM correctly classified all samples, whereas the\\nsame VLM without retrieval performed worse in both accuracy and precision. We\\nfurther compare against open-vocabulary baselines and incorporate uncertainty\\nClopper-Pearson confidence intervals to account for the small-sample setting.\\nAblation studies indicate that the key advantage of the framework lies in\\nexplainability and generalizability: retrieved references ground the reasoning\\nprocess and enable the detection of previously unseen defects by leveraging\\ndomain knowledge rather than relying solely on visual cues. This research\\ncontributes a data-efficient solution for industrial inspection that reduces\\ndependence on extensive labeled datasets.',\n",
       "  'link': 'http://arxiv.org/abs/2510.22868v1',\n",
       "  'published': datetime.datetime(2025, 10, 26, 23, 19, 28, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Semantic Surgery: Zero-Shot Concept Erasure in Diffusion Models',\n",
       "  'summary': 'Concept erasure in text-to-image diffusion models is crucial for mitigating\\nharmful content, yet existing methods often compromise generative quality. We\\nintroduce Semantic Surgery, a novel training-free, zero-shot framework for\\nconcept erasure that operates directly on text embeddings before the diffusion\\nprocess. It dynamically estimates the presence of target concepts in a prompt\\nand performs a calibrated vector subtraction to neutralize their influence at\\nthe source, enhancing both erasure completeness and locality. The framework\\nincludes a Co-Occurrence Encoding module for robust multi-concept erasure and a\\nvisual feedback loop to address latent concept persistence. As a training-free\\nmethod, Semantic Surgery adapts dynamically to each prompt, ensuring precise\\ninterventions. Extensive experiments on object, explicit content, artistic\\nstyle, and multi-celebrity erasure tasks show our method significantly\\noutperforms state-of-the-art approaches. We achieve superior completeness and\\nrobustness while preserving locality and image quality (e.g., 93.58 H-score in\\nobject erasure, reducing explicit content to just 1 instance, and 8.09 H_a in\\nstyle erasure with no quality degradation). This robustness also allows our\\nframework to function as a built-in threat detection system, offering a\\npractical solution for safer text-to-image generation.',\n",
       "  'link': 'http://arxiv.org/abs/2510.22851v1',\n",
       "  'published': datetime.datetime(2025, 10, 26, 22, 4, 17, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Once Upon an Input: Reasoning via Per-Instance Program Synthesis',\n",
       "  'summary': 'Large language models (LLMs) excel at zero-shot inference but continue to\\nstruggle with complex, multi-step reasoning. Recent methods that augment LLMs\\nwith intermediate reasoning steps such as Chain of Thought (CoT) and Program of\\nThought (PoT) improve performance but often produce undesirable solutions,\\nespecially in algorithmic domains. We introduce Per-Instance Program Synthesis\\n(PIPS), a method that generates and refines programs at the instance-level\\nusing structural feedback without relying on task-specific guidance or explicit\\ntest cases. To further improve performance, PIPS incorporates a confidence\\nmetric that dynamically chooses between direct inference and program synthesis\\non a per-instance basis. Experiments across three frontier LLMs and 30\\nbenchmarks including all tasks of Big Bench Extra Hard (BBEH), visual question\\nanswering tasks, relational reasoning tasks, and mathematical reasoning tasks\\nshow that PIPS improves the absolute harmonic mean accuracy by up to 8.6% and\\n9.4% compared to PoT and CoT respectively, and reduces undesirable program\\ngenerations by 65.1% on the algorithmic tasks compared to PoT with\\nGemini-2.0-Flash.',\n",
       "  'link': 'http://arxiv.org/abs/2510.22849v1',\n",
       "  'published': datetime.datetime(2025, 10, 26, 21, 58, 33, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Semantic-Preserving Cross-Style Visual Reasoning for Robust Multi-Modal\\n  Understanding in Large Vision-Language Models',\n",
       "  'summary': 'The \"style trap\" poses a significant challenge for Large Vision-Language\\nModels (LVLMs), hindering robust semantic understanding across diverse visual\\nstyles, especially in in-context learning (ICL). Existing methods often fail to\\neffectively decouple style from content, hindering generalization. To address\\nthis, we propose the Semantic-Preserving Cross-Style Visual Reasoner (SP-CSVR),\\na novel framework for stable semantic understanding and adaptive cross-style\\nvisual reasoning. SP-CSVR integrates a Cross-Style Feature Encoder (CSFE) for\\nstyle-content disentanglement, a Semantic-Aligned In-Context Decoder (SAICD)\\nfor efficient few-shot style adaptation, and an Adaptive Semantic Consistency\\nModule (ASCM) employing multi-task contrastive learning to enforce cross-style\\nsemantic invariance. Extensive experiments on a challenging multi-style dataset\\ndemonstrate SP-CSVR\\'s state-of-the-art performance across visual captioning,\\nvisual question answering, and in-context style adaptation. Comprehensive\\nevaluations, including ablation studies and generalization analysis, confirm\\nSP-CSVR\\'s efficacy in enhancing robustness, generalization, and efficiency\\nacross diverse visual styles.',\n",
       "  'link': 'http://arxiv.org/abs/2510.22838v1',\n",
       "  'published': datetime.datetime(2025, 10, 26, 21, 11, 46, tzinfo=datetime.timezone.utc)}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = fetch_arxiv_papers(query='zero shot', max_results=50, timedeltaindays=7)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "004556ee-6c51-486a-be3d-9afa98eb96bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with\\n  the MME-CoF Benchmark',\n",
       " 'summary': 'Recent video generation models can produce high-fidelity, temporally coherent\\nvideos, indicating that they may encode substantial world knowledge. Beyond\\nrealistic synthesis, they also exhibit emerging behaviors indicative of visual\\nperception, modeling, and manipulation. Yet, an important question still\\nremains: Are video models ready to serve as zero-shot reasoners in challenging\\nvisual reasoning scenarios? In this work, we conduct an empirical study to\\ncomprehensively investigate this question, focusing on the leading and popular\\nVeo-3. We evaluate its reasoning behavior across 12 dimensions, including\\nspatial, geometric, physical, temporal, and embodied logic, systematically\\ncharacterizing both its strengths and failure modes. To standardize this study,\\nwe curate the evaluation data into MME-CoF, a compact benchmark that enables\\nin-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our\\nfindings reveal that while current video models demonstrate promising reasoning\\npatterns on short-horizon spatial coherence, fine-grained grounding, and\\nlocally consistent dynamics, they remain limited in long-horizon causal\\nreasoning, strict geometric constraints, and abstract logic. Overall, they are\\nnot yet reliable as standalone zero-shot reasoners, but exhibit encouraging\\nsigns as complementary visual engines alongside dedicated reasoning models.\\nProject page: https://video-cof.github.io',\n",
       " 'link': 'http://arxiv.org/abs/2510.26802v1',\n",
       " 'published': datetime.datetime(2025, 10, 30, 17, 59, 55, tzinfo=datetime.timezone.utc)}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a4f02ec-a8a2-4a82-a8a8-8ebfb031a97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"cache\"\n",
    "\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir, exist_ok=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf407fb8-ac18-41fd-b6c3-2aa766e99d03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, item in enumerate(response):\n",
    "    pdf_url = item['link']\n",
    "    pdf_response = requests.get(pdf_url.replace('/abs/', '/pdf/'))\n",
    "    filename = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + f\"_{i:04d}\"\n",
    "    temp_file_path = f\"{cache_dir}/{filename}.pdf\"\n",
    "    with open(temp_file_path, \"wb\") as f:\n",
    "        f.write(pdf_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84e76b6e-c8ca-4ef3-b7b8-63759a069b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with\\n  the MME-CoF Benchmark',\n",
       " 'summary': 'Recent video generation models can produce high-fidelity, temporally coherent\\nvideos, indicating that they may encode substantial world knowledge. Beyond\\nrealistic synthesis, they also exhibit emerging behaviors indicative of visual\\nperception, modeling, and manipulation. Yet, an important question still\\nremains: Are video models ready to serve as zero-shot reasoners in challenging\\nvisual reasoning scenarios? In this work, we conduct an empirical study to\\ncomprehensively investigate this question, focusing on the leading and popular\\nVeo-3. We evaluate its reasoning behavior across 12 dimensions, including\\nspatial, geometric, physical, temporal, and embodied logic, systematically\\ncharacterizing both its strengths and failure modes. To standardize this study,\\nwe curate the evaluation data into MME-CoF, a compact benchmark that enables\\nin-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our\\nfindings reveal that while current video models demonstrate promising reasoning\\npatterns on short-horizon spatial coherence, fine-grained grounding, and\\nlocally consistent dynamics, they remain limited in long-horizon causal\\nreasoning, strict geometric constraints, and abstract logic. Overall, they are\\nnot yet reliable as standalone zero-shot reasoners, but exhibit encouraging\\nsigns as complementary visual engines alongside dedicated reasoning models.\\nProject page: https://video-cof.github.io',\n",
       " 'link': 'http://arxiv.org/abs/2510.26802v1',\n",
       " 'published': datetime.datetime(2025, 10, 30, 17, 59, 55, tzinfo=datetime.timezone.utc)}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98025f5e-603a-4985-b071-42d7cde3986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_pdf = temp_file_path = r\"F:\\AI_ML_Notebooks\\Research_Paper_Agent\\cache\\2025-11-02_00-49-27_0000.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48d287bc-0c31-4368-b3e1-bb8ad461252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_reader = PyMuPDFLoader(path_to_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "457cd5e7-0729-48bd-b1d3-e0956c0da0e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='Are Video Models Ready as Zero-Shot Reasoners?\\nAn Empirical Study with the MME-COF Benchmark\\nZiyu Guoâˆ—â€ 1, Xinyan Chenâˆ—2, Renrui Zhangâˆ—â€¡2, Ruichuan Anâˆ—3, Yu Qiâˆ—4, Dongzhi Jiang2\\nXiangtai Li3, Manyuan Zhang2, Hongsheng Li2, Pheng-Ann Heng1\\nCUHK 1IMIXR & 2MMLab\\n3Peking University\\n4Northeastern University\\nâˆ—Equal Contribution\\nâ€ Project Lead\\nâ€¡Corresponding Author\\nProject Page: https://video-cof.github.io\\nAbstract\\nRecent video generation models can produce high-fidelity, temporally coherent\\nvideos, indicating that they may encode substantial world knowledge. Beyond real-\\nistic synthesis, they also exhibit emerging behaviors indicative of visual perception,\\nmodeling, and manipulation [70]. Yet, an important question still remains: Are\\nvideo models ready to serve as zero-shot reasoners in challenging visual reasoning\\nscenarios? In this work, we conduct an empirical study to comprehensively\\ninvestigate this question, focusing on the leading and popular Veo-3 [21]. We\\nevaluate its reasoning behavior across 12 dimensions, including spatial, geometric,\\nphysical, temporal, and embodied logic, systematically characterizing both its\\nstrengths and failure modes. To standardize this study, we curate the evaluation\\ndata into MME-COF, a compact benchmark that enables in-depth and thorough\\nassessment of Chain-of-Frame (CoF) reasoning. Our findings reveal that while\\ncurrent video models demonstrate promising reasoning patterns on short-horizon\\nspatial coherence, fine-grained grounding, and locally consistent dynamics, they\\nremain limited in long-horizon causal reasoning, strict geometric constraints, and\\nabstract logic. Overall, they are not yet reliable as standalone zero-shot reasoners,\\nbut exhibit encouraging signs as complementary visual engines alongside dedicated\\nreasoning models.\\n1\\nIntroduction\\nVideo models [21, 63, 55, 81, 11], including text-to-video and video-to-text generation models, have\\nmade rapid progress in recent years. Thanks to advances in diffusion [75, 7, 84] and autoregressive [36,\\n76, 16] architectures, current video models can produce high-fidelity videos maintaining consistent\\nobject relations and realistic motion dynamics across frames. This suggests that the models may\\nhave internalized substantial visual and structural knowledge about the world. Recent research from\\nGoogle [70] further hints that, such models are evolving beyond pure content generation: Veo-3 [21]\\nhas been shown to perform dozens of distinct vision tasks across perception, modeling, manipulation,\\nand reasoning, without any task-specific training. These emergent capabilities have led researchers to\\nposit that video models could serve as unified, generalist vision models, much like large language\\nmodels (LLMs) [1, 13, 3, 30] have become foundation models for natural language.\\nCrucially, the sequential nature of video generation provides a new perspective on how such models\\nmight reason. Each generated frame builds upon the last, creating a temporal chain of information\\npropagation. This has been dubbed â€œChain-of-Frameâ€ (CoF) reasoning [70], an analogy to the chain-\\nof-thought (CoT) process in LLMs [69, 35, 82, 23, 79] and their multi-modal variants (MLLMs) [12,\\n4, 40, 31, 10]. In essence, as a video model generates a sequence of frames, it can iteratively refine\\nand update the scene, thereby working through a problem step-by-step in time and space. This CoF\\nconcept suggests that, beyond surface-level pattern generation, general-purpose visual reasoning may\\nemerge from video generative models.\\narXiv:2510.26802v1  [cs.CV]  30 Oct 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='Embodied\\nReasoning\\n3D Geometry\\nReasoning\\nReal-world Spatial\\nReasoning\\nPhysics-based\\nReasoning\\n2D Geometry\\nReasoning\\nTable and Chart\\nReasoning\\nObject Counting\\nReasoning\\nRotation\\nReasoning\\nVisual Detail\\nReasoning\\nVisual Trace\\nReasoning\\nGUI\\nReasoning\\nMedical\\nReasoning\\nVeo\\nSora\\nSeedance\\nVideo Models\\n...\\nZero-shot Reasoning?\\nKling\\nFigure 1: Overview of Our Study on the Reasoning Potential of Video Models. We investigate\\nwhether state-of-the-art video models exhibit emergent reasoning potentials beyond content synthesis.\\nThe analysis spans 12 reasoning dimensions under a unified perspective, exploring whether large-scale\\nvideo models can serve as zero-shot visual reasoners via CoF reasoning.\\nHowever, it remains unclear to what extent current video models truly exhibit reasoning about the\\ncontent they create. Strong generative performance does not automatically imply robust reasoning\\npotential. Emerging evidence [22, 47, 5, 78] shows that a model may produce coherent videos by\\nlearning surface-level patterns in the training data, rather than by internalizing general principles. For\\ninstance, a video model can maintain object continuity yet fail to grasp physical plausibility across\\na long sequence, or it may mimic observed visual sequences without understanding the underlying\\ncause-and-effect relationships. This motivates our central question: Are video models, purely through\\nlarge-scale visual learning, obtain the zero-shot reasoning potential?\\nTo this end, we present the first empirical study to systematically probe the CoF reasoning capabili-\\nties of modern video models, spanning 12 dimensions such as spatial, geometric, physical, temporal,\\nand embodied logic, as detailed in 1. We carry out our analysis on Veo-3, which has been system-\\natically examined as a zero-shot learner in prior work [70]. Our preliminary observations suggest\\nthat current leading video models exhibit comparable reasoning patterns, making Veo-3 a represen-\\ntative choice. Our analysis builds on reasoning scenarios distilled from diverse reasoning-oriented\\nbenchmarks [25, 67, 45, 71, 29, 34], as well as those we design ourselves, providing a compact yet\\nexpressive foundation. The prompts for video models are meticulously crafted by transforming the\\nunderlying, textual reasoning process of problem-solving into a clear, video-presentation format.\\nEach case receives a qualitative assessment across three performance levels, i.e., good, moderate, and\\nbad, complemented by a quantitative success rate to measure robustness.\\nTo standardize evaluation, we curate these tasks into the MME-COF benchmark, as illustrated in\\nFigure 2 and Section 3.2. Leveraging this benchmark, we measure several state-of-the-art video\\nmodels, i.e., Veo-3 [21], Sora-2 [56], Kling [38], and Seedance [19], to obtain directly comparable\\nscores and qualitative behaviors across categories. Our investigation reveals that the models exhibit\\npromising reasoning patterns in short-horizon spatial coherence, fine-grained grounding, and con-\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='Medical\\nReasoning\\nEmbodied\\nReasoning\\nObject Counting\\nReasoning\\nGUI\\nReasoning\\nTable & Chart\\nReasoning\\nRotation\\nReasoning\\n3D Geometry\\nReasoning\\nReal-world Spatial\\nReasoning\\nVisual Trace\\nReasoning \\nVisual Detail\\nReasoning \\n2D Geometry\\nReasoning\\nPhysics-based\\nReasoning\\nKling-v1\\nSeedance-1.0-pro\\nSora-2-pro\\nSora-2\\nVeo-3-fast\\nVeo-3-preview\\n(a) Evaluation Radar Map.\\n(b) Word Cloud.\\nFigure 2: Illustration of the MME-COF Benchmark. It showcases that different models specialize\\nin distinct reasoning aspects, but most models exhibit limited reasoning capability across all tasks.\\nsistent local dynamics; however, they struggle with complex reasoning conditions, particularly in\\nlong-horizon causal consistency, geometric constraint adherence, and abstract logic. Overall, current\\nvideo models are not yet ready as standalone zero-shot reasoners. Still, they show encouraging signs\\nof emergent reasoning, suggesting strong potential as complementary reasoning agents alongside\\nspecialized models.\\nOur main contributions are summarized as follows:\\nâ€¢ A Comprehensive Empirical Study. We provide the first investigation of video models\\n(Veo-3) to analyze their visual reasoning potential, detailing representative successes, char-\\nacteristic errors, and the conditions under which CoF reasoning emerges, holds, or breaks.\\nâ€¢ The MME-COF Benchmark. We curate MME-COF, a compact benchmark providing a\\nstandardized taxonomy and an evaluation protocol aligned with CoF reasoning, enabling\\nconsistent and category-wise assessment beyond surface-level visual fidelity.\\nâ€¢ Insights and Directions. We summarize common success patterns (e.g., short-horizon\\ncoherence and stable spatial layout) and failure patterns (e.g., long-horizon degradation,\\nviolations of basic geometry/physics, and temporal logic), making clear when the behavior\\nreflects genuine reasoning versus pattern replay.\\n2\\nDeep-Dive Analysis on Veo-3\\n2.1\\nOverview\\nTo ensure a rigorous empirical study, we detail our core methodology in this section, including the\\ntaxonomy of reasoning tasks, test case curation process, the standardized style for prompt design,\\nand the analysis setup.\\nTask Taxonomy.\\nTo capture different dimensions of reasoning, our study starts from dozens of\\nreasoning-oriented tasks, which can be organized into the following 12 categories:\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='1) Visual Detail Reasoning\\n2) Visual Trace Reasoning\\n3) Real-world Spatial Reasoning\\n4) 3D Geometry Reasoning\\n5) 2D Geometry Reasoning\\n6) Physics-based Reasoning\\n7) Rotation Reasoning\\n8) Table and Chart Reasoning\\n9) Object Counting Reasoning\\n10) GUI Reasoning\\n11) Embodied Reasoning\\n12) Medical Reasoning\\nEach category comprises several representative cases selected to test specific aspects of reasoning.\\nTest Case Curation.\\nWe recruit five PhD-level experts with deep expertise in text-image reasoning,\\nwho are tasked with selecting representative cases from benchmarks [25, 67, 45, 52, 74] corresponding\\nto each task category. For each reasoning case, the experts manually constructed text prompts that\\nexplicitly or unambiguously define the target reasoning objective, aiming to evaluate the potential of\\nvideo models for multi-modal reasoning.\\nPrompt Design Style.\\nTo ensure consistency and fairness, all prompts follow a unified style\\nemphasizing explicit visual constraints, controlled motion, and minimal linguistic ambiguity. Prompts\\nare encouraged to be written in imperative form and designed to reduce variance from language\\ninterpretation, focusing the modelâ€™s behavior on the intended visual reasoning objective. The overall\\ndesign principles are as follows:\\n1) Static camera and fixed viewpoint, unless motion is explicitly required by the task.\\n2) Stable spatial composition, consistent framing, and unchanging scene layout across\\nframes.\\n3) Clear specification of allowed and disallowed changes (e.g., â€œno zoom, no pan, no dollyâ€)\\nto constrain camera dynamics.\\n4) Explicit temporal phrasing to control the pace of motion, using cues such as â€œinstantlyâ€,\\nâ€œsmoothlyâ€, or â€œstep-by-stepâ€.\\n5) Avoidance of direct textual hints toward the answer; instructions are purely visual and\\ntask-oriented.\\n6) Inclusion of realistic phrasing and scene context to align with the modelâ€™s natural video\\npriors while minimizing artifacts.\\nThe standardized prompt style ensures that differences in output primarily reflect the modelâ€™s internal\\nreasoning potential rather than prompt variability.\\nAnalysis Setup.\\nFor every reasoning case, we construct a text prompt that explicitly or implicitly\\nspecifies the target reasoning objective. Each prompt produces six video samples at a resolution of\\n1280Ã—720, 24 FPS, and a duration of 8 seconds. All experiments are conducted in a unified zero-shot\\nsetup without fine-tuning, additional supervision, or auxiliary tools.\\nWe evaluate model outputs through qualitative judgments along three levels of performance, i.e.,\\nGood , Moderate , and Bad, based on the clarity, correctness, and temporal stability of the visual\\nreasoning process. Detailed definitions and examples of these evaluation criteria are provided in the\\ncorresponding task subsections. Note that, since we observe that most video models struggle to follow\\nthe requirement of â€˜static shotâ€™ reliably, we apply more permissive qualitative criteria for static-shot\\nevaluations. We further define a success rate to measure robustness across generations for each case,\\ncomputed as the proportion of successful samples among the six generated. For cases categorized as\\nBad, the success rate is always 0. Non-zero success rates only appear in cases evaluated as Good or\\nModerate , indicating that Veo-3 exhibits some potential to perform as a visual reasoner. A higher\\nsuccess rate reflects a more stable reasoning capability of the model.\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content=\"Input Image:\\nReasoning Video:\\nZoom in on the black bag with the Apple logo to focus on the \\nlogo's color. Static shot.\\nI. Question:\\nText-to-Video Prompt:\\nQ: What is the color of the Apple logo?\\n1st frame\\nInput Image:\\nReasoning Video:\\nII. Question:\\nText-to-Video Prompt:\\n1st frame\\nIII. Question:\\nText-to-Video Prompt:\\nA: The color of the Apple logo is polychromatic.\\nIV. Question:\\n~ Moderate\\nSuccess Rate: 17%\\nGradually zoom in on the group of people walking along the path, centering \\non the person carrying the handbag. Keep the surrounding park and benches \\nsoftly blurred to emphasize the handbagâ€™s color. Static shot.\\nQ: What is the color of the handbag? \\nA: The color of the handbag is white.\\nâœ“ Good\\nSuccess Rate: 33%\\nInput Image:\\nReasoning Video:\\n1st frame\\nSmoothly zoom in on the dog near the lower right corner of the \\nscene, then highlight the motorcycle parked near it. Keep the \\nsurrounding jeeps and people slightly blurred to emphasize \\nspatial relation. Static shot. \\nQ: Is the motorcycle on the left or right side of \\nthe dog?\\nA: The motorcycle is on the left side of the dog.\\nâœ“ Good\\nSuccess Rate: 83%\\nText-to-Video Prompt:\\nInput Image:\\nReasoning Video:\\n1st frame\\nGradually zoom in on the area near the cone along the pathway, \\ncentering both the cone and the baby carriage in the frame. Keep \\nthe surrounding trees and grass softly blurred to emphasize these \\ntwo objects. Static shot.\\nQ: Is the baby carriage on the left or right side \\nof the cone?\\nA: The baby carriage is on the right side of the \\ncone.\\nâœ— Bad\\nFigure 3: Showcase of Visual Detail Reasoning by Veo-3. It illustrates Veo-3â€™s ability to localize\\ntargets and maintain fine-grained visual attributes across frames, together with common failure modes\\nwhen targets are small, occluded, or embedded in clutter.\\n5\"),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='2.2\\nVisual Detail Reasoning\\nTask Description and Evaluated Aspects.\\nIn the visual detail reasoning category, the objective is\\nto assess a modelâ€™s ability to discern and maintain fine-grained visual attributes and spatial relations\\nwithin generated video sequences. It covers attribute recognition, e.g., identifying color, texture or\\nmaterial of an object, and spatial relation identification, e.g., recognizing that one object is on the\\nleft of or behind another object. The model is evaluated on the capacity both to attend to the correct\\ntarget region and to maintain visual consistency, across frames, of the attribute or relation in question.\\nDefinition of Good / Moderate / Bad.\\nWe define the three-level evaluation criteria as follows:\\nâœ“Good: The reasoning video accurately centers on the correct target region, clearly resolves\\nthe relevant attribute, such as color, texture or position, and maintains sharp, stable and natural\\nrendering throughout the sequence. There are no visible frame drops, artifacts or unintended\\nmotion.\\n~ Moderate: The region of interest is approximately correct, and the attribute remains\\ninferable, but the sequence suffers from minor blur, incomplete framing, slight instability\\nmild unnatural motion, or sometimes deviates from the textual instruction and produces a\\nplausible but unaligned or self-directed visual interpretation, limiting confident interpretation.\\nâœ—Bad: The target region is incorrect or ambiguous, the attribute cannot be reliably inferred,\\nor the video exhibits severe artifacts: abrupt frame jumps, major jitter, unintended zoom or\\ncrop, extraneous objects interfering, or conspicuous quality degradation that obstructs the\\nreasoning task altogether.\\nData Source.\\nWe sample data from the Vâˆ—Bench [71], which provides a comprehensive set of\\nevaluation dimensions including spatial relationship and color/attribute consistency tasks.\\nExample and Analysis.\\nWe illustrate typical behaviors of Veo-3 in visual detail reasoning through\\nfour representative cases in Figure 3. In case I, the model performs well in localizing the target:\\nalthough it does not strictly execute the â€œzoom inâ€ instruction, it instead achieves an equivalent\\nvisual outcome through a semantically consistent motion with a personâ€™s hand. This slight deviation\\nsuggests that the model may exhibit certain generation preferences in how it interprets and realizes\\nspatial instructions, possibly reflecting stylistic tendencies learned from training data. In cases II\\nand III, the model achieves better success rates when the targets are visually salient and contextually\\ndistinct. For the handbag and dog-motorcycle scenes, Veo-3 attends to the correct regions and\\nmaintains smooth temporal coherence. However, when the object (e.g., the motorcycle) is small or\\nsurrounded by distracting elements, the model occasionally fails to locate it accurately, indicating\\nlimited fine-grained spatial discrimination in cluttered scenes. In case IV, when the target object is\\ntiny and visually indistinct, Veo-3 cannot identify it even with explicit positional hints, highlighting\\nthat the modelâ€™s perceptual grounding and reasoning weaken sharply when object size and salience\\nare too low for reliable attention.\\nTakeaway 1\\nVeo-3 performs well in fine-grained attribute and spatial reasoning for salient, well-grounded\\ntargets, but fails when objects are small, occluded, or cluttered. It sometimes exhibits stylistic\\ngeneration biases that lead to plausible yet instruction-divergent outcomes.\\n2.3\\nVisual Trace Reasoning\\nTask Description and Evaluated Aspects.\\nThe visual trace reasoning category evaluates a modelâ€™s\\nability to represent and maintain causal continuity across sequential actions. Typical tasks include\\nmaze navigation, path following, and multi-step object manipulation, where the video must visually\\nencode a coherent sequence of intermediate decisions that lead to the correct goal. Performance is\\nassessed based on two major aspects: (i) temporal coherence, which is the smoothness and logical\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='Input Image:\\nReasoning Video:\\nStarting at the red dot in the middle-right cell, animate step-by-\\nstep moves: go down 1 cell, left 1, left 1, up 1, and up 1,\\ndrawing arrows for each step and finishing with a glow around\\nthe final cell. Static shot.\\nI. Question:\\nText-to-Video Prompt:\\nQ: Starting from the red dot, follow the given\\nmovement instructions and determine the final\\nposition. Down 1, left 1, left 1, up 1, up 1.\\nA: A\\n1st frame\\nInput Image:\\nReasoning Video:\\nII. Questionâ€ :\\nText-to-Video Prompt:\\n1st frame\\nIII. Questionâ€ :\\nText-to-Video Prompt:\\nIV. Question:\\nAnimate the elf moving step by step toward the gift while carefully\\navoiding the icy frozen lake. Highlight the successful path and end\\nwith the elf standing beside the gift. Static shot.\\nQ: The character must avoid falling into the\\nfrozen lake and reach the gift pack safely.\\nInput Image:\\nReasoning Video:\\n1st frame\\nAnimate the red triangle moving step by step toward the white printer,\\npicking it up once it reaches it. Then have the triangle carry the printer\\nupward and place it on the brown area representing the table. End with a\\nsubtle highlight around the printer to show it is toggled on. Static shot.\\nQ: Move the character (red triangle)\\nto pick up the white printer and\\nplace it anywhere on the desk.\\nText-to-Video Prompt:\\nInput Image:\\nReasoning Video:\\n1st frame\\nAnimate a bright path tracing from the blue point at the\\ntop through the mazeâ€™s open corridors toward the red\\npoint at the bottom, highlighting each green numbered\\nmark it passes. Keep the maze and all walls fixed while\\nthe glowing path moves smoothly through the correct\\nroute. Static shot.\\nQ: The given picture is a maze, and the black lines\\nrepresent walls that cannot be walked. Now you want to\\nwalk from the blue point to the red point. Is there a\\nfeasible path? If so, which of the green marks numbered\\n1-5 In the picture must be passed in the path?\\nA: Yes, 3.\\nâœ—Bad\\nâœ“Good\\nSuccess Rate: 17%\\nâœ—Bad\\nâœ—Bad\\nFigure 4: Showcase of Visual Trace Reasoning by Veo-3 (Part I). It shows short-horizon path-\\nfollowing successes, object-grounding failures, and a certain bias that causes step omissions/mistakes\\nin multi-step traces. â€  The ground-truth answers of cases II and III are intuitive and non-unique,\\nwhich are omitted to highlight the key reasoning behaviors.\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content=\"Input Image:\\nReasoning Video:\\nCreate a 2D animation based on the provided diagram. The red arrow is\\nthe initial arrow, and the green arrow is the final arrow. The arrow can\\nmove in four directions (forward, backward, left, right), where 'forward'\\nalways refers to the current direction the arrow is pointing. After each\\nmovement, the arrow's direction is updated to the direction of movement.\\nMovement commands:\\n- The red arrow moves forward for 1 unit.\\n- The red arrow moves left for 1 unit (relative to its new current direction\\nafter step 1). Then turns green.\\nScene:\\n- No change in scene composition.\\n- No change in the layout of the diagram.\\nCamera: Static camera. No zoom. No pan. No glitches, noise, or artifacts.\\nV. Question:\\nText-to-Video Prompt:\\nQ: In the diagram, the red arrow is\\nthe initial arrow, and the green arrow\\nis the final arrow. The arrow can\\nmove in four directions (forward,\\nbackward,\\nleft,\\nright),\\nwhere\\n'forward' always refers to the current\\ndirection the arrow is pointing. After\\neach\\nmovement,\\nthe\\narrow's\\ndirection is updated to the direction\\nof\\nmovement.\\nWhich\\nof\\nthe\\nfollowing paths can make the arrow\\nmove from the starting position to\\nthe ending position?\\nA: (Forward, 1 unit) - (Left, 1 unit)\\n1st frame\\nInput Image:\\nReasoning Video:\\nVI. Question:\\nText-to-Video Prompt:\\n1st frame\\nTwo small characters start from the same purple origin at the same\\ntime, and move along the red and green paths toward another purple\\ndestination at the same speed. Static camera, no zoom, no pan.\\nQ: What are the advantages of the green\\nroute and the red route respectively?\\nâœ—Bad\\nâœ—Bad\\nFigure 5: Showcase of Visual Trace Reasoning (Part II) by Veo-3. The examples highlight\\nlong-horizon planning breakdowns, inconsistent arrow/trajectory rendering, and failures to preserve\\ncomparative or sequential information across frames.\\nprogression between consecutive steps; and (ii) goal consistency, which means whether the full\\nsequence visually completes the intended reasoning trajectory without deviation or contradiction.\\nDefinition of Good / Moderate / Bad.\\nWe rate the performance according to the following criteria:\\nâœ“Good: Each movement step is depicted continuously and logically toward the correct goal.\\nThe motion is smooth, temporally consistent, and follows causal order with no skipping,\\nstuttering, or direction reversal.\\n~ Moderate: The overall trajectory roughly aligns with the intended sequence, but small\\ndiscontinuities, timing irregularities, or partial missteps occur. The reasoning path remains\\ninterpretable, and the goal can still be inferred.\\nâœ—Bad: Key steps are missing, reversed, or illogical. The sequence shows abrupt jumps,\\ninconsistent object trajectories, or goal confusion, breaking the temporal and causal coherence\\nof the reasoning process.\\nData Source.\\nWe select samples from MVoT [41], FrozenLake [8, 72], MiniBehavior [32], RBench-\\nV [25], SpatialViz-Bench [66], and OmniSpatial [29], which provide controlled multi-step envi-\\nronments for evaluating temporal reasoning, sequential planning, and causal continuity in visual\\nsimulations.\\n8\"),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='Example and Analysis.\\nIn Figure 4 and Figure 5, we showcase six representative visual-trace\\nexamples. In case I, the model repeatedly fails to execute the exact step sequence and instead drifts\\ntoward a visually salient central cell. However, case II is one of the few successes: the model can\\nproduce a coherent step-by-step path in simple, low-branching settings, but this behavior is not\\nrobust across trials. Case III largely fails, where the model often does not ground the specified object\\n(printer), sometimes hallucinating its appearance or placement rather than performing a consistent\\npickup-and-place. Case IV shows near-uniform failure on long-horizon, highly branched navigation:\\noutputs contain wrong turns, discontinuities, and no faithful global plan. Case V reveals difficulty\\ngrounding abstract movement rules, producing inconsistent arrow trajectories. Case VI produces\\nvisually plausible motions along individual paths but fails to preserve or present the comparative\\ninformation required for contrastive reasoning. Taken together, these examples indicate that the\\nmodel can simulate locally coherent short traces but systematically fails at long-horizon planning,\\nrule-grounded execution, and object-persistent manipulations.\\nTakeaway 2\\nVeo-3 can produce locally coherent, short-horizon trace animations in simple, low-branching\\nscenarios, but it does not reliably execute long-horizon plans or rule-grounded sequences.\\n2.4\\nReal-World Spatial Reasoning\\nTask Description and Evaluated Aspects.\\nThis task investigates Veo-3 [21]â€™s ability to perceive\\nand maintain spatial relations within natural scenes, with a focus on reasoning about viewpoint change,\\norientation consistency, and reference-frame alignment. We assess whether the model preserves a\\nstable global coordinate frame and coherent scene orientation under varying viewpoints, and whether\\nobjects retain correct relative positions and orientations with respect to each other across different\\nviews.\\nDefinition of Good / Moderate / Bad.\\nWe define the evaluation criteria in three levels:\\nâœ“Good: Scene orientation, reference frame, and viewpoint are consistent and correctly\\nrepresent spatial relations. The camera remains steady and the motion is natural.\\n~ Moderate: Scene roughly matches the instruction but contains small perspective errors,\\nunnatural transitions, or partial mirroring. Motion remains interpretable but not physically\\ncoherent.\\nâœ—Bad: Reference frame or direction is wrong; viewpoint shifts abruptly or inconsistently.\\nVideo suffers from strong camera drift, disorienting motion, or spatial chaos.\\nData Source.\\nTo evaluate on orientation and layout reasoning, we specifically sample data from\\nMMSI-Bench [74]. Also, the tasks of perspective taking and spatial interaction are selected from the\\nOmniSpatial dataset [29].\\nExample and Analysis.\\nAs shown in Figure 6, Veo-3 can correctly handle basic spatial layouts\\nin case I, but struggles with complex viewpoints or orientation changes in case II. The perspective\\ntransformations are sometimes inaccurate or even incorrect, suggesting that the model tends to\\nprioritize visual plausibility over precise spatial reasoning, which hinders further reasoning in case IV.\\nMoreover, case III demonstrates that Veo-3 has difficulty understanding depth, further limiting its\\nspatial reasoning capability.\\nTakeaway 3\\nWhile Veo-3 exhibits an emerging ability for simple real-world spatial reasoning, its capability\\nremains insufficient for handling more complex spatial understanding tasks.\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content=\"Input Image:\\nReasoning Video:\\nâœ“Good\\nSuccess Rate: 33%\\nI. Question:\\nText-to-Video Prompt:\\n1st frame\\nInput Image:\\nReasoning Video:\\nAn arrow points from the player wearing jersey number 10 in purpleto the\\nbasketball. Static camera view, no zoom or pan.\\nII. Question:\\nText-to-Video Prompt:\\nQuestion: From the perspective of\\nthe player wearing jersey number 10\\nin purple, where is the basketball?\\nA: Left front.\\n1st frame\\nInput Image:\\nReasoning Video:\\nThe camera slowly and smoothly elevates from its current isometric\\nview, gradually rising upwards while maintaining focus on the\\napartment layout. It continues to ascend until it reaches a complete\\noverhead, bird's-eye perspective, providing a full top-down view of the\\nentire floor plan, displaying all rooms and furniture clearly from above.\\nThe movement is fluid and controlled, ending with a static shot from\\nthe high vantage point.\\nIV. Question:\\nText-to-Video Prompt:\\nQ: If you are facing the washing machine,\\nhow should you walk to the stove and face\\nthe stove?\\nA: Turn around and go straight, then turn\\nleft and go straight, then turn right and go\\nstraight, finally turn left to face the stove.\\n1st frame\\nA red arrow point from the green chair toward the balcony. Another red arrow point\\nfrom the door to the balcony. Static camera view, no zoom or pan.\\nQuestion: The balcony is\\nnorth relative to the door,\\nin which direction\\non\\nthe balcony is the chair?\\nA: Southwest.\\nInput Image:\\nReasoning Video:\\nIII. Question:\\nText-to-Video Prompt:\\n1st frame\\nâœ—Bad\\nThe image transitions to a depth-map of the scene: Darker colors represent pixels further\\nfrom the camera, lighter colors represent pixels closer to the camera. The exact color map\\nto use is provided on the right side of the image. Static scene, no pan, no zoom, no dolly.\\nQ:\\nFrom\\nthe\\ndunker's\\nviewpoint, which white-\\nuniformed player is the\\nfarthest from them?\\nA: Five.\\n~ Moderate\\nSuccess Rate: 17%\\n~ Moderate\\nSuccess Rate: 20%\\nFigure 6: Showcase of Real-World Spatial Reasoning by Veo-3. Although Veo-3 can reason\\nabout simple spatial layouts, it still struggles to maintain consistency under complex perspective or\\norientation changes.\\n10\"),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='Input Image:\\nReasoning Video:\\nâœ“Good\\nSuccess Rate: 83%\\nI. Question:\\nText-to-Video Prompt:\\n1st frame\\nInput Image:\\nReasoning Video:\\nA hand moves the object to the left along the y-axis and then moves it up. Static camera \\nview, no zoom or pan, and the perspective of the object remains unchanged throughout.\\nII. Question:\\nText-to-Video Prompt:\\nQ: Move the object to the \\nleft along the y-axis and up.\\n1st frame\\nInput Image:\\nReasoning Video:\\nâœ—Bad\\nSmoothly zoom in to the \"Initial State\" figure. The yellow block, starting at (1,0,0), moves one \\nunit in the positive Y direction to position (1,1,0). Then, move back to (1,0,0). The cyan block, \\nstarting at (2,0,0), moves one unit in the positive Y direction to position (2,1,0), exchange the \\nposition with the purple block. Static shot. No pan. No glitches, noise, or artifacts.\\nIV. Question:\\nText-to-Video Prompt:\\nQ: The sequence of moves that \\nturns the first cube stack into \\nthe final one is _______.\\nA: (1,0,0)y+ (1,1,0)y- (2,1,0)y+\\n1st frame\\nA: D.\\nâœ—Bad\\nMove the object up. Static camera view, no zoom or pan, and the perspective of the \\nobject remains unchanged throughout.\\nQ: Move the object up.\\nA: D.\\nInput Image:\\nReasoning Video:\\nIII. Question:\\nText-to-Video Prompt:\\n1st frame\\nâœ—Bad\\nThe net is folded to form a single cube, with folding edges clearly shown. Static camera \\nperspective, no zoom or pan.\\nQ: Check out a net with 6 \\nfaces below: Can the net \\nbe folded to form a cube, \\nyes or no? \\nA: Yes.\\nFigure 7: Showcase of 3D Geometry Reasoning by Veo-3 (Part I). While Veo-3 shows certain\\npotential in basic 3D geometry reasoning, its performance remains unstable for complex geometry\\ntransformations.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='cropped_0.jpg\\nInput Image:\\nReasoning Video:\\n \\nRotate only the left 3D shape in place with a small, constant-speed yaw at fixed scale; keep \\neverything else static.\\nV. Question:\\nText-to-Video Prompt:\\nQ: From any angle, which one \\non the right is not a view of \\nthe three dimensional shape \\ngiven on the left?\\nA: C\\n1st frame\\n \\n~ Moderate\\nSuccess Rate: 17%\\nFigure 8: Showcase of 3D Geometry Reasoning by Veo-3 (Part II). The model often generates\\nmisaligned or self-intersecting structures, compromising geometric consistency.\\n2.5\\n3D Geometry Reasoning\\nTask Description and Evaluated Aspects.\\nWe also evaluate Veo-3â€™s potential on 3D geometry\\nreasoning tasks, such as geometric object motion and three-dimensional structural transformations\\nlike reconstructing a cube net. The assessment focuses on three key dimensions: geometric accuracy,\\nstructural completeness throughout the transformation, and visual continuity across frames.\\nDefinition of Good / Moderate / Bad.\\nWe categorize the modelâ€™s performance into three levels:\\nâœ“Good: Transformations like folding, rotation and assembly are geometrically correct,\\nvisually smooth, and continuous, maintaining structural integrity and realistic motion. No\\nbroken edges, jumps, or spatial artifacts.\\n~ Moderate: Transformations are partially correct but show local misalignment, unrealistic\\ndeformation, or discontinuous motion; geometry is roughly interpretable but imperfect.\\nâœ—Bad: Transformation fails. For example, wrong fold, structure collapse, or impossible\\ngeometry. Motion is erratic, discontinuous, or visually implausible, breaking the sense of\\nphysical realism.\\nData Source.\\nTo construct diverse and representative evaluation data, we adapt tasks from estab-\\nlished geometric spatial reasoning datasets, including the 3D-Text-Instruct and Folding Nets subsets\\nof the STARE benchmark [43], the BlockMoving subset from the SpatialViz-Bench [66], as well as\\nVisuLogic [73] benchmark.\\nExample and Analysis.\\nWe showcase the results of Veo-3 on 3D geometry reasoning tasks\\nin Figure 7 and Figure 8. Veo-3 demonstrates a degree of potential on 3D geometry reasoning,\\nperforming reasonably well on simple, single-step geometric transformations, as shown in case I.\\nHowever, its performance degrades noticeably when facing multi-step or compositionally complex\\ntransformations in case II. As presented in cases III and V, the model frequently produces misaligned\\nor self-intersecting structures, leading to a loss of geometric consistency. Further observations in case\\nIV, show that while the model can partially understand the geometric shape of individual objects, it\\nlacks a coherent understanding of coordinate systems and the spatial relationships among multiple\\nobjects.\\nTakeaway 4\\nVeo-3 exhibits emerging reasoning potential on basic 3D transformations but breaks down on\\ncomplex or multi-step geometry, often yielding misaligned or self-intersecting structures. Its\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='Takeaway 5\\n3D geometric reasoning remains fragile, revealing substantial gaps in its ability to function as a\\nreliable 3D geometry reasoner.\\n2.6\\n2D Geometry Reasoning\\nTask Description and Evaluated Aspects.\\nTo assess a modelâ€™s competence in 2D geometric\\nreasoning, we evaluate its zero-shot performance on planar geometric construction tasks. These\\ntasks involve drawing geometric relations by connecting points, adding auxiliary lines, and moving\\ngeometric shapes. The evaluation focuses on whether the generated constructions or movements\\naccurately reflect the described geometric relationships and adhere to the given instructions, while\\nmaintaining smooth, stable operations that ensure visual clarity and coherence throughout the process.\\nDefinition of Good / Moderate / Bad.\\nWe rate the performance according to the following criteria:\\nâœ“Good: Constructions and movements are geometrically accurate and visually smooth.\\nEndpoints, intersections, angles, and motion trajectories align correctly with the instructions.\\nBoth drawing and movement processes are stable, fluid, and natural, resembling human\\nsketching or manipulation.\\n~ Moderate: Constructions and movements roughly follow the intended geometry but exhibit\\nminor inaccuracies in line placement, shape alignment, trajectory, or smoothness. Some local\\njitter or abrupt motion may appear, but the overall structure and motion remain interpretable.\\nâœ—Bad: Constructions or movements deviate substantially from geometric correctness. Lines\\nor shapes may be misplaced, disconnected, or moved in a chaotic or discontinuous manner\\n(e.g., jittering, overlapping, or distorted paths), leading to visual instability and loss of\\ninterpretability.\\nData Source.\\nThe evaluation data are drawn from multiple established sources, including the\\nGeo170k dataset [18], the VarsityTutors subset of Math-PUMA [85] dataset, the line-connection\\nsubset of RBench-V [25], the MAVIS-Gen [80], Tangram Puzzle subsets of the STARE [43] benchmark,\\nand data from VAT [46].\\nExample and Analysis.\\nThe representative examples of the 2D geometry reasoning task are\\npresented in Figures 9 and 10. Veo-3 demonstrates a foundational capability for simple geometric\\nconnection tasks, correctly identifying and linking elements in straightforward scenarios like in case\\nIII. However, this basic competence is inconsistent. The model often prioritizes producing visually\\nsymmetric or semantically meaningful patterns rather than strictly adhering to geometric instructions\\n(cases I and II). Furthermore, case II reveals instances where the model unintentionally modifies the\\noriginal figures, indicating a limited awareness of geometric constraints and poor spatial consistency.\\nWhen tackling more complex connection tasks, the model frequently fails to interpret the intended\\ndrawing order or point indices, resulting in incorrect connection sequences, as demonstrated in cases\\nV, VI, and VII. This is often coupled with an inability to control task termination, as the model tends\\nto continue drawing beyond the required constructions. Finally, for tasks involving the movement\\nof geometric shapes in cases IV and VIII, the model struggles to maintain geometric structural\\nconsistency throughout the motion.\\nTakeaway 6\\nVeo-3 shows initial 2D geometric reasoning ability but still falls short of consistent, constraint-\\naware geometric understanding, remaining far from a robust geometric reasoner.\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content=\"Input Image:\\nReasoning Video:\\nA line connecting point A and point C. The\\nvideo ends once the connection process is\\ncomplete. Static view, no zoom or pan.\\nI. Question:\\nText-to-Video Prompt:\\nQ: In the figure shown, let 'n' represent the length of side AB of the inscribed\\nrectangle ABCD, where n is an undetermined value. With BC equal to 6.0\\nand the diameter of circle O equal to 10.0, what is the value of 'nâ€™?\\nA: 8\\n1st frame\\nInput Image:\\nReasoning Video:\\nII. Question:\\nText-to-Video Prompt:\\n1st frame\\nIII. Question:\\nText-to-Video Prompt:\\nIV. Question:\\n~ Moderate\\nSuccess Rate: 83%\\nSmoothly\\nconnecting\\npoint\\nM and point N. The video\\nends\\nonce\\nthe\\nconnection\\nprocess is complete. Static\\nview, no zoom or pan.\\nQ: The figure presented depicts a square designated as ABCD. Within this square, point\\nM is identified as the midpoint of the side AB, while point N is the midpoint of the\\nopposing side CD. Additionally, point O is located at the midpoint of segment CN. Your\\ntask is to draw the segment MO. It is given that the length of segment AM is represented\\nby t. The objective is to determine which of the following expressions accurately\\nrepresents the length of the segment MO in terms of t.\\nA: 17\\n4 ð‘¡\\nInput Image:\\nReasoning Video:\\n1st frame\\nSmoothly connecting point C and point D with a line. The video\\nends once the connection process is complete. Static view, no\\nzoom or pan.\\nQ: AB equals to 8.0. What would the area of \\nthe entire shape ABCD be?\\nA: 62.87\\nText-to-Video Prompt:\\nInput Image:\\nReasoning Video:\\n1st frame\\nPlace piece A with its upper-\\nleft corner at (x, y) = (0, 3).\\nQ: Check out an Tangram puzzle below. The left panel is an empty Tangram puzzle,\\nwhile the right panel shows available pieces to complete the puzzle. Keep in mind\\nthat you can rotate or flip the pieces. Can the Tangram puzzle be completed with\\nthe available pieces, yes or no?\\nA: Yes.\\nâœ—Bad\\nâœ—Bad\\n~ Moderate\\nSuccess Rate: 33%\\nFigure 9: Showcase of 2D Geometry Reasoning by Veo-3 (Part I). While Veo-3 shows potential in\\nrecognizing simple patterns, it lacks the robust constraint awareness essential for accurate geometric\\nmanipulation.\\n14\"),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='Input Image:\\nReasoning Video:\\nAnimate the dots connecting sequentially from 1 to\\n25, each straight line appearing smoothly until the\\nfull outline emerges. Keep the background with the\\nsmiling sun and plants unchanged. Static shot.\\nV. Question:\\nText-to-Video Prompt:\\nQ: Connect the black dots in the image sequentially with straight\\nlines according to the edge numbers (i.e., connect dot 1 to dot 2,\\ndot 2 to dot 3, and so on). The final result will form a simple line\\ndrawing. What does this drawing represent?\\nA: Duck.\\n1st frame\\nInput Image:\\nReasoning Video:\\nVI. Question:\\nText-to-Video Prompt:\\n1st frame\\nVII. Question:\\nText-to-Video Prompt:\\n~ Moderate\\nSuccess Rate: 83%\\nInput Image:\\nReasoning Video:\\n1st frame\\nâœ—Bad\\nAnimate\\nthe\\nnumbered\\ndots\\nconnecting\\nsequentially from 1 to 118, each straight line\\nappearing\\nsmoothly\\nas\\nthe\\noutline\\ngradually\\nemerges. Keep all numbers and dots visible while\\nthe connecting lines form step by step. Static shot.\\nQ: Connect the black dots in the image sequentially with straight\\nlines according to the edge numbers (i.e., connect dot 1 to dot 2,\\ndot 2 to dot 3, and so on). The final result will form a simple line\\ndrawing. What does this drawing represent?\\nA: Lion.\\nAnimate\\nthe\\nnumbered\\ndots\\nconnecting\\nsequentially from 1 to 118, each straight line\\nappearing\\nsmoothly\\nas\\nthe\\noutline\\ngradually\\nemerges. Keep all numbers and dots visible while\\nthe connecting lines form step by step. Static shot.\\nQ: Connect the black dots in the image sequentially with straight\\nlines according to the edge numbers (i.e., connect dot 1 to dot 2,\\ndot 2 to dot 3, and so on). The final result will form a simple line\\ndrawing. What does this drawing represent?\\nA: Bird.\\nâœ—Bad\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nPan vertically at a steady speed to center the top yellow line, then the bottom one, keeping \\nboth visible with identical scale and exposure. Ensure all elements stay unchanged and finish \\nwith a full view showing both lines together.\\nVIII. Question:\\nText-to-Video Prompt:\\nQ: Is the top yellow line \\nshorter than the bottom \\nyellow line?\\nA: No.\\n1st frame\\nâœ—Bad\\nFigure 10: Showcase of 2D Geometry Reasoning by Veo-3 (Part II). Veo-3â€™s reasoning abilities are\\nfurther challenged by complex sequential instructions and the need to preserve structural integrity.\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content=\"Input Image:\\nReasoning Video:\\nShow the rough semicircular track with\\nheight label h and a block at P; release it,\\nadd faint friction streaks as it slides down\\nand up the right side, stopping below the\\nrim.\\nShow\\nthe\\nmove\\nquickly\\nand\\ncompletely. Static shot.\\nI. Question:\\nText-to-Video Prompt:\\nQ: The figure shows a rough semicircular track whose ends are at a\\nvertical height h. A block placed at point P at one end of the track is\\nreleased from rest and slides past the bottom of the track. Which of the\\nfollowing is true of the height to which the block rises on the other side\\nof the track?\\nA: It is between zero and h; the exact height depends on how much\\nenergy is lost to friction.\\n1st frame\\nInput Image:\\nReasoning Video:\\nII. Question:\\nText-to-Video Prompt:\\n1st frame\\nIII. Question:\\nText-to-Video Prompt:\\nIV. Question:\\nAnimate the red ball moving along the blue\\narrow's direction, bouncing off\\nthe black\\nwalls according to reflection rules, keeping\\nspeed consistent. Continue its path upward\\nuntil it reaches and collides with one of the\\nnumbered top bricks. Static shot.\\nQ: The red ball moves in the direction indicated by the blue arrow\\nand bounces off the black side walls upon collision; the component\\nof its velocity perpendicular to the wall reverses in direction but\\nmaintains its magnitude, while the component parallel to the wall\\nremains unchanged. Based on this behavior, please estimate which\\nnumbered brick (from 1 to 10) at the top the red ball will hit first.\\nA: 1.\\nInput Image:\\nReasoning Video:\\n1st frame\\nDynamically\\ndepict\\nthe\\nattraction\\nbetween\\nmagnets,\\npaying\\nattention to speed and intensity. Static shot.\\nQ: Think about the magnetic force between\\nthe magnets in each pair.\\nA: The magnetic force is stronger in Pair 2.\\nText-to-Video Prompt:\\nInput Image:\\nReasoning Video:\\n1st frame\\nThe orange gear rotates counterclockwise in the given view.\\nAnimate the provided planetary gear system. The orange gear is\\nfixed on the green gear. The central orange sun gear rotates\\ncounterclockwise, driving the yellow planet gear. All components\\nmust maintain their relative axial positions and proper gear\\nmeshing. The camera is static, with no zoom or pan.\\nQ: The orange gear is fixed on the\\nstationary green gear. If the orange\\ngear rotates counterclockwise in the\\ngiven view, what is the motion of the\\nyellow gear relative to the orange gear?\\nA: Clockwise rotation.\\nCounterclockwise revolution.\\nâœ—Bad\\nâœ—Bad\\nâœ—Bad\\n~ Moderate\\nSuccess Rate: 83%\\nFigure 11: Showcase of Physics-based Reasoning by Veo-3. The physics scenarios demonstrate lo-\\ncally plausible dynamics and reflections, alongside systematic quantitative and causal inconsistencies\\nunder frictional, force-driven, or constrained interactions.\\n16\"),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='2.7\\nPhysics-based Reasoning\\nTask Description and Evaluated Aspects.\\nThe physics-based reasoning category assesses a\\nmodelâ€™s capacity to depict and reason about motion dynamics, physical causality, and rule-based\\ninteractions between objects. Tasks in this group involve gravity, collisions, reflection, momentum, or\\nenergy conservation, requiring the model to generate physically plausible and temporally coherent\\nmotion. Evaluation focuses on two complementary aspects: (i) physical plausibility, which means\\nwhether the simulated motion obeys common physical principles; and (ii) causal correctness, which is\\nwhether object interactions are consistent with the underlying cause-and-effect relationships described\\nin the prompt.\\nDefinition of Good / Moderate / Bad.\\nWe rate the performance according to the following criteria:\\nâœ“Good: The motion sequence adheres to physical laws such as gravity, momentum, and\\nenergy conservation. Object interactions are realistic and temporally smooth, and the visual\\noutcome remains coherent and credible throughout.\\n~ Moderate: The physical relations are approximately correct but include minor inconsisten-\\ncies, such as irregular acceleration, timing mismatch, or slight violation of conservation. The\\noverall motion remains interpretable and visually plausible.\\nâœ—Bad: The motion is physically implausible or visually chaoticâ€”objects float, stop abruptly,\\nor behave contrary to basic causal principles. Severe artifacts or temporal discontinuities\\ndisrupt the perception of a coherent physical process.\\nData Source.\\nWe draw samples from MMMU [77], ScienceQA [49], and related physical reasoning\\nsubsets of RBench-V [26] and SpatialViz-Bench [66], covering scenarios such as object collisions,\\npendulum motion, frictional sliding, and optical or magnetic interactions.\\nExample and Analysis.\\nFigure 11 presents four representative physics tasks and their outputs.\\nCase I shows that the model can produce a visually coherent slide, but the behavior violates basic\\nphysical laws. Case II is the most reliable, where reflections and general trajectory shape are rendered\\nplausibly and the task attains a high success rate, although small angular or timing offsets are common.\\nIn case III, the model conveys attraction through motion, yet the depicted dynamics do not reliably\\ntrack the intended force magnitudes or causal ordering. Finally, case IV exposes structural failures,\\nincorrect meshing, inconsistent relative rotations, and nonphysical contact behavior occur frequently,\\nso the mechanical constraints are not respected. Overall, the model can synthesize locally plausible\\ndynamics and handle simple reflection rules, but it fails to maintain quantitative physical constraints\\nand causal fidelity in frictional, force-driven, or mechanically constrained scenarios.\\nTakeaway 7\\nVeo-3 often generates visually plausible short-term dynamics, but it systematically fails to\\npreserve quantitative physical constraints (energy, momentum), causal ordering, and contact\\nmechanics in frictional, force-driven, or mechanically constrained scenarios. Thus, its outputs\\nare somewhat useful for qualitative illustration but are not reliable for quantitative physics\\ninference or causal prediction.\\n2.8\\nRotation Reasoning\\nTask Description and Evaluated Aspects.\\nThe rotation reasoning task assesses the ability to\\nreason about planar object rotation and maintain consistent spatial grounding under rotational\\ntransformations, thereby supporting subsequent reasoning processes. In each instance, the model is\\nrequired to accurately rotate target objects within a fixed 2D plane while preserving the overall scene\\nstructure and structural consistency, followed by performing reasoning tasks like grounding and OCR.\\nThe evaluation focuses on both the accuracy of the rotation in terms of angle and direction, and the\\nprecision of the resulting reasoning tasks.\\n17'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content=\"Input Image:\\nReasoning Video:\\nI. Question:\\nText-to-Video Prompt:\\n1st frame\\nInput Image:\\nReasoning Video:\\nRotate the scene 180 degrees clockwise. Then draw a bounding \\nbox around the leftmost vending machine.\\nII. Question:\\nText-to-Video Prompt:\\nQ: Looking up from the floor, how many rows\\nof drinks are in the leftmost vending machine?\\nA: 2\\n1st frame\\nInput Image:\\nReasoning Video:\\nâœ—Bad\\nThe entire 'Original' grid figure performs one smooth, continuous 360-degree \\nrotation clockwise within its own 2D plane. The camera stays static, with no pan.\\nIV. Question:\\nText-to-Video Prompt:\\nQï¼šWhich grid can be obtained \\nby rotating the grid only?\\n1st frame\\nâœ—Bad\\nRotate the scene 45 degrees clockwise. Then draw bounding boxes around the\\nfrontmost skiing character.\\nQ: Is the frontmost skier\\nwearing a scarf?\\nA: No.\\nInput Image:\\nReasoning Video:\\nIII. Question:\\nText-to-Video Prompt:\\n1st frame\\nâœ—Bad\\nRotate the video frame 90 degrees counterclockwise in the 2D\\nplane, then draw bounding boxes around each 'IKEA' label.\\nQ: On which floors are the 'IKEA' labels located?\\nA: One on the top floor, one on the middle floor, \\nand one on the bottom floor.\\n~ Moderate\\nSuccess Rate: 83%\\nA: A\\nFigure 12: Showcase of Rotation Reasoning by Veo-3. Veo-3 struggles in complex scenes. However,\\nits foundational grasp of simple rotations signals its potential to support rotation-based reasoning\\ntasks.\\n18\"),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 18}, page_content='Definition of Good / Moderate / Bad.\\nModel outputs are categorized into three quality levels:\\nâœ“Good: The rotation is accurate, complete, and strictly confined to the 2D plane, with no\\nextraneous scene motion. The following reasoning tasks are completed correctly. Target\\nobjects remain precisely grounded after rotation.\\n~ Moderate: The rotation is largely correct but may be incomplete or slightly off-angle,\\nthough still confined to the 2D plane. The following reasoning tasks are mostly completed.\\nMinor temporal or visual inconsistencies may appear, but do not alter the core 2D structure or\\nobject grounding.\\nâœ—Bad: The model fails to perform the correct rotation, extends the transformation into 3D\\nspace, or introduces substantial scene distortion. Cannot complete the following reasoning\\ntask. The original 2D structure is altered, leading to inaccurate grounding of the target objects.\\nData Source.\\nTo specifically assess the rotation reasoning task, we recruit some PhD-level experts\\nwith deep expertise in text-image reasoning to design the evaluation data manually, followed by the\\nnecessary review process, as mentioned in Section 3.2. Each question is designed following the\\nprinciple that it must involve a 2D rotation to reach the correct solution, ensuring the task genuinely\\nprobes rotational understanding rather than simple visual matching. Moreover, we sample data from\\nthe 2DRotation subset from the SpatialViz-Bench [66], and reformulate the question into instructions\\nfor the video models.\\nExample and Analysis.\\nThe results are shown in Figure 12. In case I, we find that Veo-3 handles\\nsmall-angle rotations and simple planar scenes reasonably well, demonstrating a basic grasp of\\nrotational motion. However, in more complex scenarios like cases II, III, and IV, the model often\\nignores the 2D rotation constraint and inadvertently alters the 3D structure, resulting in incorrect\\nrotations and degraded spatial grounding. Such errors frequently propagate to downstream tasks, such\\nas OCR in case III, or object localization in case II, due to inconsistencies in post-rotation alignment.\\nThese observations suggest that the reasoning behavior of Veo-3 remains more pattern-driven rather\\nthan principle-driven. However, as it demonstrates a partial understanding of planar rotation, this can\\nto some extent facilitate subsequent reasoning tasks.\\nTakeaway 8\\nVeo-3 exhibits only a superficial understanding of rotation reasoning. While it can approximate\\nsmall planar rotations, it fails to preserve geometric consistency under larger or compound\\ntransformations.\\n2.9\\nTable and Chart Reasoning\\nTask Description and Evaluated Aspects.\\nThe table and chart reasoning task requires the model\\nto identify and focus on the key elements within visualizations or tabular data. For evaluation, we\\nfurther consider how effectively the model identifies the regions relevant to the query and whether\\nit can transition smoothly and visually coherently to these areas, preserving clarity, continuity, and\\nproper scaling.\\nDefinition of Good / Moderate / Bad.\\nWe rate the performance according to the following criteria:\\nâœ“Good: Camera precisely focuses on the correct chart or table segment, smoothly high-\\nlighting or zooming into the queried data (e.g., correct year, category, or value). Motion is\\ncontinuous, the chart and table remain clear, and no distortion or overexposure occurs.\\n~ Moderate: Camera approximately focuses on the right region but partially misses boundaries,\\nintroduces slight blur, or transitions abruptly. Data can still be inferred.\\nâœ—Bad: Video fails to locate the correct region or changes the chart or table geometry\\nunnaturally. Motion jitter, scaling errors, or artifacts make data unreadable or misleading.\\n19'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 19}, page_content=\"Input Image:\\nReasoning Video:\\nStart with smoothly zooming in to focus on the 'Nova Scotia' row. Then,\\nsmoothly zoom out to the full view of the chart. End with smoothly zooming\\nin to focus on the 'Manitoba' row. The chart itself, including all its data, lines,\\nand labels, must remain completely static and unchanged throughout the video.\\nI. Question:\\nText-to-Video Prompt:\\nQ: What is the sum of footwear\\nmanufacturing establishments in\\nNova Scotia and Mantioba as of\\nDecember 2020?\\nA: 3\\n1st frame\\nInput Image:\\nReasoning Video:\\nII. Question:\\nText-to-Video Prompt:\\n1st frame\\nIII. Question:\\nText-to-Video Prompt:\\nIV. Question:\\nStart with a static, full view of the chart. Then, smoothly zoom the camera in to focus on\\nthe vertical area corresponding to the year 2014. The chart itself, including all its data,\\nlines, and labels, must remain completely static and unchanged throughout the video.\\nQ: In the year 2014, which\\nopinion is dominant?\\nA: Unfavorable.\\nInput Image:\\nReasoning Video:\\n1st frame\\nZoom in to focus on the smallest section in the chart. The chart itself, including all its data,\\nlines, and labels, must remain completely static and unchanged throughout the video.\\nQ: What' the color of \\nsmallest section in the chart?\\nA: Gray.\\nText-to-Video Prompt:\\nInput Image:\\nReasoning Video:\\n1st frame\\nDraw a bounding box around the end market for the Engineered\\nSystems segment. The table itself, including all its text, lines, and labels,\\nmust remain completely static and unchanged throughout the video.\\nQ: What is the end market for the \\nEngineered Systems segment?\\nA: Printing & Identification, Industrials.\\nâœ—Bad\\n~ Moderate\\nSuccess Rate: 83%\\nâœ—Bad\\nâœ—Bad\\nFigure 13: Showcase of Table and Chart Reasoning by Veo-3. Veo-3 demonstrates an initial ability\\nto focus on relevant data regions but lacks the precision and consistency required for reliable visual\\nanalysis.\\n20\"),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 20}, page_content='Data Source.\\nWe use samples from the ChartQA [52] dataset and TableVQA-Bench [34].\\nExample and Analysis.\\nFor charts, as presented in cases I, II and III in Figure 13, Veo-3 can often\\nzoom into an approximately correct region but lacks the precision needed to accurately locate the\\nqueried data. For tables, as shown in case IV, Veo-3 fails to correctly identify the required element\\nand tends to select entries randomly. The model also frequently adds, modifies, or distorts existing\\nchart and table elements, resulting in visual inconsistencies that undermine the accuracy of chart\\ninterpretation.\\nTakeaway 9\\nVeo-3 demonstrates emerging competence and potential in structured visual understanding, but\\nstill falls short of functioning as a precise and reliable chart-table reasoner.\\n2.10\\nObject Counting Reasoning\\nTask Description and Evaluated Aspects.\\nIn this category, we focus on the ability to accurately\\nenumerate objects within a 2D or 3D scene. In each instance, the model is required to identify, ground,\\nand count target objects, typically by highlighting, drawing bounding boxes, applying numerical\\nlabels, or panning. The evaluation focuses on the accuracy of the count and the precision of the\\nspatial grounding, performed within a scene that remains static or experiences only minimal motion,\\nensuring the counting process is not influenced.\\nDefinition of Good / Moderate / Bad.\\nModel outputs are categorized into three quality levels:\\nâœ“Good: The model precisely highlights, draws bounding boxes around, or labels the objects\\nwith correct numbers, and performs smooth and controlled panning when necessary to cover\\nall targets. Motion is continuous, and the scene remains static or experiences only slight\\nchanges that do not influence the counting process.\\n~ Moderate: The model approximately highlights or draws bounding boxes around the objects,\\nor performs panning with minor instability or incomplete coverage. Objects or the scene may\\nmove or change slightly, but this does not strongly affect the counting process.\\nâœ—Bad: The model fails to correctly highlight, label, or draw bounding boxes around the\\nobjects, or pans erratically such that parts of the scene are missed or revisited unnecessarily.\\nObjects or the scene move or change substantially, severely affecting the counting process.\\nData Source.\\nThe 2D object counting data are sampled from the counting subset of RBench-V [25].\\nThe 3D object counting data are from the Super-CLEVER dataset [45] and VAT [46].\\nExample and Analysis.\\nThe results are shown in Figures 14 and 15. In the 2D counting tasks from\\ncases I to III, objects frequently move or change during the process, negatively impacting counting\\nstability and accuracy. In the 3D counting tasks, Veo-3 successfully handles simple grounding and\\ncounting scenarios, as demonstrated in case V, but struggles with scenes involving complex materials\\nor geometric variations in cases VI and VII, leading to inaccurate counts. Additionally, in the panning\\nprocess of case VII, the camera fails to precisely move to the regions containing all target objects,\\nfurther hindering the counting process.\\nTakeaway 10\\nVeo-3 demonstrates basic counting capability but lacks the spatial control and robustness\\nrequired for reliable object enumeration in dynamic or complex scenes.\\n21'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 21}, page_content='cropped_0.jpg\\nInput Image:\\nReasoning Video:\\nA scanner dot moves along the black line from bottom-left to top-right. As soon as this\\ndot enters a new grid square, that entire square is instantly filled with yellow color and\\nstays yellow. A square only turns yellow if the scanner dot on the line has entered it.\\nStatic camera, no zoom.\\nI. Question:\\nText-to-Video Prompt:\\nQ:\\nHow\\nmany\\nunit\\nsquares\\ndoes\\nthe\\nline\\nsegment pass through in\\nthe given grid diagram?\\nA: 16\\n1st frame\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nHighlight only the rectangles in the figure with a bright yellow color. Not highlight \\nany other shapes like squares, triangles, circles, or irregular polygons. Static camera, \\nno zoom, no pan.\\nII. Question:\\nText-to-Video Prompt:\\nQ: How many rectangles \\nare there in the figure?\\nA: 8\\n1st frame\\nâœ—Bad\\nâœ—Bad\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nLabel all the fish with increasing numbers (1, 2, 3, ...). The fish keep static. Static \\ncamera, no zoom, no pan.\\nIII. Question:\\nText-to-Video Prompt:\\nQ: How many rectangles \\nare there in the figure?\\nA: 18\\n1st frame\\nâœ—Bad\\nFigure 14: Showcase of 2D Object Counting Reasoning by Veo-3. Veo-3â€™s lack of spatial control\\noften introduces object motion, undermining the stability and accuracy of the counting process.\\n2.11\\nGUI Reasoning\\nTask Description and Evaluated Aspects.\\nIn the Graphical User Interface (GUI) reasoning task,\\nwe focus on the capability to understand and interact with graphical user interfaces across different\\noperating systems, including Android, Linux, and Web environments. In each instance, the model is\\nrequired to perform actions, such as clicking on specific UI elements. The evaluation focuses on the\\naccuracy of the click and the temporal coherence of the interaction, ensuring the scene and irrelevant\\nUI elements remain consistent.\\nDefinition of Good / Moderate / Bad.\\nWe define the evaluation criteria in three levels:\\nâœ“Good: The click is precise, with no extraneous actions. No superfluous icons appear, and\\nthe original data and icons remain unchanged.\\n~ Moderate: The click is precise but may be accompanied by minor extraneous actions.\\nSuperfluous icons might appear but do not obscure the click target, and original data or icons\\nshow only slight alterations.\\n22'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 22}, page_content='cropped_0.jpg\\nInput Image:\\nReasoning Video:\\nDraw bounding boxes around the brown metal mountain bikes to the right of \\nthe origami crane. Static shot.\\nV. Question:\\nText-to-Video Prompt:\\nQ: There is a small yellow object that is \\nto the left of the tiny metal motorbike; \\nhow many brown metal mountain bikes \\nare to the right of it?\\nA: 1\\n1st frame\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nDraw bounding boxes around any matte tandem bikes and metal cruisers present in the \\nscene. Static shot.\\nVI. Question:\\nText-to-Video Prompt:\\nQ: How many cyan things \\nare matte tandem bikes or \\nmetal cruisers?\\nA: 1\\n1st frame\\nâœ“Good\\nSuccess Rate: 100%\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nPan smoothly to include both the lidâ€“body interface and the spout or cap in view at a fixed \\nscale, keeping exposure steady and avoiding any visual or geometric changes.\\nVII. Question:\\nText-to-Video Prompt:\\nQ: How many burners are \\non the stove?\\nA: 4\\n1st frame\\n~ Moderate\\nSuccess Rate: 17%\\nâœ“Good\\nSuccess Rate: 33%\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nDraw bounding boxes around the tiny things that have the same material as the green \\nmotorbike. Static shot.\\nIV. Question:\\nText-to-Video Prompt:\\nQ: How many tiny things \\nhave the same material as \\nthe green motorbike?\\nA: 1\\n1st frame\\nâœ—Bad\\nFigure 15: Showcase of 3D Object Counting Reasoning by Veo-3. Veo-3â€™s basic 3D counting\\nabilities are challenged by complex materials, geometric variations, and imprecise camera control.\\n23'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 23}, page_content='cropped_0.jpg\\nInput Image:\\nReasoning Video:\\n \\n \\nClick the pkgs folder to collapse it. Static shot.\\nI. Question:\\nText-to-Video Prompt:\\nQ: Collapse the pkgs folder.\\nA: \\n1st frame\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\n \\nClick the calendar icon located to the right of the flight date options, \\nnext to the price display for June 6. Static shot.\\nII. Question:\\nText-to-Video Prompt:\\nQ: A calendar icon \\nlocated to the right of \\nthe flight date options, \\nnext to the price \\ndisplay for June 6.\\n1st frame\\nâœ— Bad\\n  \\nâœ— Bad\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\n \\nClick the navigation arrow located at the right edge of the browse by category \\ncarousel. Static shot.\\nIII. Question:\\nText-to-Video Prompt:\\nQ: A navigation \\narrow located at the \\nright edge of the \\nbrowse by category \\ncarousel.\\n1st frame\\n  \\nâœ— Bad\\nA:\\nA:\\nFigure 16: Showcase of GUI Reasoning by Veo-3. Veo-3â€™s attempts at graphical interface interaction\\nexhibit visual inconsistencies and logical inaccuracies, indicating only a shallow grasp of underlying\\nGUI logic. Note that the answer to each question is a bounding box. For visual clarity, screenshots\\nwith the ground-truth bounding boxes are shown.\\nâœ—Bad: The click is imprecise or erratic. Original data and icons are significantly altered,\\nhindering judgment and assessment.\\nData Source.\\nThe Linux data are selected from the Common Linux Screenshot subset of ScreenSpot-\\nPro [42], while the Android and Web data are drawn from the OS Android and OS Web subsections\\nof MMBench-GUI [67], respectively.\\nExample and Analysis.\\nAcross the three cases in Figure 16, Veo-3 fails to accurately capture the\\ncorrect click position and often exhibits inconsistencies between the click location and the resulting\\non-screen effect. In addition, it occasionally alters or generates new icons and text, which can\\ninterfere with judgment. In the Web system in case III, however, the model demonstrates partial GUI\\nresponsiveness and provides some degree of visual feedback.\\n24'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 24}, page_content='cropped_0.jpg\\nInput Image:\\nReasoning Video:\\nPan to the banana while keeping tray edges in view. Fix scale (banana ~two-thirds of \\nthe frame, axis horizontal). Sweep once along the inner concave edge from stem to tip \\nat constant speed, then stop and hold at its midpoint.\\nI. Questionâ€ :\\nText-to-Video Prompt:\\nQ: Which point \\ncorresponds to \\nthe affordance for \\nmanipulating the \\nbanana?\\n1st frame\\n~ Moderate\\nSuccess Rate: 33%\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nKeep the cucumberâ€™s start and the pot opening in view. Sweep once from start to pot \\nat fixed scale and speed, briefly dwelling at four evenly spaced waypoints (p1â†’p4) \\nalong the path, then hold on both endpoints.\\nII. Questionâ€ :\\nText-to-Video Prompt:\\nQ: Which set of 4 \\npoints is a right \\ntrajectory when \\ndoing place a \\ncucumber into a pot?\\n1st frame\\nâœ—Bad\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nPan smoothly to include both the lidâ€“body interface and the spout or cap in view at a fixed \\nscale, keeping exposure steady and avoiding any visual or geometric changes.\\nIII. Questionâ€ :\\nText-to-Video Prompt:\\nQ: Is the container sealed?\\nA: No.\\n1st frame\\n~ Moderate\\nSuccess Rate: 17%\\nA:\\nA:\\nFigure 17: Showcase of Embodied Reasoning by Veo-3. It illustrates plausible static affordance\\ndetection in simple settings, common workaround/hallucination behaviors for dynamic manipulations,\\nand failures to reliably localize or preserve manipulation-relevant context. â€  Green points in the\\nanswer image denote ground-truth points or trajectories.\\nTakeaway 11\\nVeo-3 demonstrates a limited awareness of GUI click actions, imitating interaction behaviors\\nwithout fully grasping the underlying functional logic.\\n2.12\\nEmbodied Reasoning\\nTask Description and Evaluated Aspects.\\nThis category evaluates the modelâ€™s potential to perceive\\nand reason about object affordances and manipulation dynamics. It involves recognizing both static\\nand dynamic affordances, as well as identifying manipulation-relevant object and scene attributes.\\nEvaluation focuses on two aspects: (i) the generation of stable and contextually relevant visual\\nsequences, and (ii) the maintenance of reasoning fidelity without resorting to implausible planning\\nshortcuts or hallucinated interactions.\\n25'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 25}, page_content='Definition of Good / Moderate / Bad.\\nWe define the evaluation criteria in three levels:\\nâœ“Good: The sweep/framing covers all candidates fairly (equal or near-equal dwell), centers\\nthe manipulation-relevant geometry (e.g., handle + frame/gap, lid-body interface, hinge side)\\nwith crisp focus and stable scale; no cropping of key context; no content alterations.\\n~ Moderate: The view roughly includes the right region(s) but with minor bias or coverage\\nissues: slight off-center, brief under-exposure of one candidate, small motion jitter, or shallow\\ncontext (still enough to infer).\\nâœ—Bad: The camera misses or biases the evidence (e.g., lingers only on one point, crops\\naway the hinge/rail, over-zooms a non-relevant patch), introduces distortion/content edits, or\\nproduces footage from which a fair decision cannot be made.\\nData Source.\\nWe select samples from Robobench [51] for the analysis. In addition to a general\\nunderstanding of static attributes, we also sample data to assess whether Veo-3 can perform direct\\nreasoning on tasks involving the generation of static and dynamic affordances.\\nExample and Analysis.\\nAs shown in Figure 17, Veo-3 demonstrates the ability to comprehend\\nobjects within real-world scenes. However, its capacity for assisting visual reasoning in embodied\\nscenarios remains constrained by insufficient stability. As illustrated in case I, when provided with a\\nclearly defined object for manipulation, Veo-3 is capable of generating plausible manipulation affor-\\ndances. When it comes to dynamic affordances, Veo-3 tends to employ workarounds to compensate\\nfor its planning deficiencies, as evidenced in case II, where it generated a new cucumber instead of\\nthe intended object. With respect to static attributes, Veo-3 struggles to accurately differentiate visual\\nprompts and misidentifies the position of containers. As shown in case III, the green box, intended to\\nspecify the location of the container, inadvertently led Veo-3 to produce hallucinations.\\nTakeaway 12\\nVeo-3â€™s capabilities are currently limited to basic object recognition rather than true embodied\\nreasoning. It lacks the necessary planning and stability to reliably interpret and act upon\\ndynamic or spatially constrained instructions, indicating its limitations in understanding and\\nreasoning of real-world interactions.\\n2.13\\nMedical Reasoning\\nTask Description and Evaluated Aspects.\\nThis category assesses the modelâ€™s ability to localize\\nlesions or structures, identify relevant attributes (e.g., side, lobe), recognize pathological patterns\\n(e.g., â€œjump distributionâ€), and make binary decisions (e.g., presence or absence). The evaluation\\nfocuses on both the correctness of object manipulation and the visual stability of the surrounding\\nregions.\\nDefinition of Good / Moderate / Bad.\\nWe define the evaluation criteria in three levels:\\nâœ“Good: The camera cleanly settles on the correct anatomical level/lesion, with clear margins\\nand readable context; motion is reasonable; no geometric distortion or content alteration.\\n~ Moderate: The view roughly covers the right area but is slightly off (partial coverage, mild\\nblur, small framing mistakes). The general shape of the tissue or organ can still be observed.\\nâœ—Bad: The video misses the target region or introduces distortions/crops that hide key cues.\\nTissues or organs begin to distort. Misleading results due to confusion of medical terminology.\\nData Source.\\nWe select samples representing different body parts from the ViTAR [9] dataset.\\nExample and Analysis.\\nWe showcase the evaluation results in Figure 18. Veo-3 retains the\\nability to manipulate images when dealing with medical images. However, due to its lack of medical\\n26'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 26}, page_content='cropped_0.jpg\\nInput Image:\\nReasoning Video:\\nShow the full sagittal lumbar view, then sweep smoothly from L1 to S1 at constant speed \\nwithout pausing. End on a view showing adjacent disc spaces, including narrow and normal \\nlevels. Keep image content and geometry unchanged.\\nI. Question:\\nText-to-Video Prompt:\\nQ: What is the distribution \\npattern of stenotic segments?\\nA: Jump distribution.\\n1st frame\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nShow the full PA chest view, then adjust framing to include both the heart silhouette and\\nthe widest inner thoracic diameter at a fixed scale. Keep contrast and geometry\\nunchanged, holding steady for visual CTR estimation.\\nII. Question:\\nText-to-Video Prompt:\\nQ: Is cardiomegaly presentï¼Ÿ\\nA: No.\\n1st frame\\nâœ—Bad\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nShow the full axial CT, then pan and zoom smoothly to the right lung so the nodule and \\nnearby fissure appear together. Keep windowing standard and geometry unchanged.\\nIII. Question:\\nText-to-Video Prompt:\\nQ: Which lobe contains \\nthe pulmonary nodule?\\nA: Left lobe.\\n1st frame\\nâœ—Bad\\nâœ—Bad\\nFigure 18: Showcase of Medical Reasoning by Veo-3. As shown in cases I and III, Veo-3 fails to\\nmaintain the shape of the rest of medical organization. Veo-3 also can not understand and precisely\\nlocate the mentioned medical terminology in the prompt, as demonstrated in case II.\\nknowledge, Veo-3 struggles to accurately manipulate the correct objects when instructions include\\nmedical terminology. This phenomenon is evident across all cases. Furthermore, Veo-3 cannot model\\nmedical organs effectively. When performing operations such as zooming in, the medical images\\nsuffer from significant distortion, resulting in a substantial loss of detail.\\nTakeaway 13\\nVeo-3â€™s failure to handle the reasoning in the medical domain, causing distortion even on simple\\nzoom-ins, highlights its limited grasp of specialized, non-general knowledge.\\n27'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 27}, page_content='MME-COF\\nFigure 19: Category Distribution.\\nTable 1: Key Statistics of MME-COF.\\nStatistic\\nNumber\\nTotal entries\\n59\\nTotal categories\\n12\\nMax prompt length\\n124\\nAvg prompt length\\n36.7\\nMax entries per category\\n7\\nAvg entries per category\\n4.9\\n3\\nMME-COF\\n3.1\\nBenchmark Overview\\nTo standardize the empirical study and systematically evaluate the reasoning potential of state-of-the-\\nart generative video models [21, 55, 56], we introduce MME-COF, which, to our knowledge, is the\\nfirst benchmark specifically designed to reveal and quantify the reasoning potential of video models.\\n3.2\\nBenchmark Composition\\nData Curation and Distribution.\\nAligning with the task taxonomy in Section 2.1, the MME-COF\\nbenchmark is curated from the cases used in our empirical study. It comprises 59 curated entries and\\ninstruction prompts spanning 12 diverse reasoning categories. The key statistics of MME-COF and\\nits overall composition are summarized in Table 1, Figure 2b and Figure 19.\\nReview Process.\\nFollowing the prompt design protocol in Section 2.1, all prompts undergo a\\ntwo-stage review process. In the cross-validation phase, each prompt was independently reviewed by\\nanother expert to ensure semantic clarity, alignment with the intended reasoning task, and the absence\\nof linguistic bias. In the final adjudication phase, discrepancies were jointly discussed and resolved\\nthrough consensus. This multi-step procedure ensured that every prompt was conceptually precise,\\nvisually grounded, and fully aligned with the evaluation objectives of MME-COF.\\n3.3\\nEvaluation Protocol\\nModels and Generation Settings.\\nWe evaluate the leading video models in a zero-shot setting,\\nincluding Kling-v1 [38], Seedance-1.0-pro [19], Veo-3.0-preview [70], Veo-3.0-fast [70], Sora-2 [56],\\nSora-2-pro [56]. Each model generates six video samples per prompt, and final scores were computed\\nas the mean across all samples. All videos are generated at a 16:9 aspect ratio. We adopt the\\ndefault 8-second duration for the Sora and Veo series, while retaining the default 5-second length for\\nKling and Seedance. Note that, since most video models apply automated safety filters and content\\nmoderation, which may block sensitive content, we exclude videos that are suppressed by such filters\\nfrom our evaluation.\\nEvaluation Metrics.\\nWe employ Gemini-2.5-Pro [12] as an automatic verifier to evaluate each\\ngenerated video. Gemini is prompted with the following evaluation criteria and returns structured\\nscores between 0 and 4, where higher values indicate better performance:\\n1) Instruction Alignment (0-4): Measures how well the video follows the described structure\\nand sequence in the prompt. A high score indicates that the visual steps faithfully reflect\\nthe textual instructions.\\n2) Temporal Consistency (0-4): Evaluates the smoothness and continuity between frames.\\nDisjointed or abrupt transitions will lead to a lower score.\\n28'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 28}, page_content='Table 2: Model-level Overall and Per-dimension Performance on MME-COF. Mean scores and\\nstandard deviations are reported on a 0â€“4 scale, as graded by Gemini-2.5-Pro.\\nModel\\nOverall\\nInstruction\\nAlignment\\nTemporal\\nConsistency\\nVisual\\nStability\\nContent\\nFidelity\\nFocus\\nRelevance\\nKling-v1 [38]\\n0.64 Â± 0.91\\n0.01 Â± 0.09\\n0.15 Â± 0.75\\n2.43 Â± 1.86\\n0.21 Â± 0.79\\n0.43 Â± 1.07\\nSeedance-1.0-pro [19]\\n1.41 Â± 1.51\\n0.30 Â± 0.86\\n1.65 Â± 1.57\\n2.00 Â± 1.72\\n1.13 Â± 1.65\\n1.98 Â± 1.75\\nVeo-3.0-fast [21]\\n1.44 Â± 1.51\\n0.56 Â± 1.09\\n1.37 Â± 1.51\\n1.88 Â± 1.73\\n1.10 Â± 1.52\\n2.27 Â± 1.69\\nVeo-3.0-preview [21]\\n1.45 Â± 1.50\\n0.54 Â± 1.06\\n1.43 Â± 1.53\\n1.89 Â± 1.71\\n1.12 Â± 1.49\\n2.26 Â± 1.73\\nSora-2-pro [56]\\n1.66 Â± 1.53\\n0.48 Â± 0.96\\n1.36 Â± 1.59\\n2.39 Â± 1.65\\n1.64 Â± 1.72\\n2.44 Â± 1.73\\nSora-2 [56]\\n1.72 Â± 1.59\\n0.59 Â± 1.12\\n1.52 Â± 1.69\\n2.32 Â± 1.68\\n1.62 Â± 1.75\\n2.52 Â± 1.71\\nTable 3: Per-category Scores on MME-COF. Mean scores and standard deviations are reported on\\na 0â€“4 scale, as graded by Gemini-2.5-Pro.\\nCategory\\nKling-v1 [38]\\nSeedance-1.0\\nPro [19]\\nVeo-3.0\\nFast [21]\\nVeo-3.0\\nPreview [21]\\nSora-2 [56]\\nSora-2\\nPro [56]\\nVisual Detail\\n0.72 Â± 0.69\\n1.37 Â± 1.39\\n1.10 Â± 1.24\\n1.59 Â± 1.68\\n1.14 Â± 1.32\\n1.08 Â± 1.89\\nVisual Trace\\n0.49 Â± 0.65\\n1.23 Â± 1.13\\n1.43 Â± 1.26\\n1.48 Â± 1.24\\n1.51 Â± 1.37\\n1.75 Â± 1.31\\nReal-world Spatial\\n0.77 Â± 0.76\\n1.79 Â± 1.53\\n2.07 Â± 1.54\\n2.10 Â± 1.46\\n1.84 Â± 1.43\\n1.77 Â± 1.35\\n3D Geometry\\n0.61 Â± 0.58\\n1.95 Â± 1.64\\n1.71 Â± 1.54\\n1.54 Â± 1.43\\n1.37 Â± 1.49\\n1.42 Â± 1.45\\n2D Geometry\\n0.49 Â± 0.67\\n0.96 Â± 1.11\\n1.18 Â± 1.15\\n1.27 Â± 1.20\\n1.77 Â± 1.45\\n1.77 Â± 1.21\\nPhysics-based\\n0.60 Â± 0.62\\n1.27 Â± 1.25\\n1.44 Â± 1.39\\n1.44 Â± 1.35\\n2.13 Â± 1.32\\n2.10 Â± 1.33\\nRotation\\n0.22 Â± 0.34\\n2.30 Â± 1.46\\n1.83 Â± 1.44\\n1.60 Â± 1.29\\n1.62 Â± 1.37\\n1.44 Â± 1.28\\nTable & Chart\\n0.87 Â± 0.72\\n0.71 Â± 1.18\\n0.82 Â± 1.30\\n0.96 Â± 1.44\\n1.84 Â± 1.61\\n1.48 Â± 1.59\\nGUI\\n1.09 Â± 0.51\\n0.70 Â± 0.76\\n1.11 Â± 1.09\\n1.18 Â± 0.89\\n1.88 Â± 1.64\\n1.52 Â± 1.48\\nObject Counting\\n0.64 Â± 0.58\\n1.15 Â± 0.97\\n2.03 Â± 1.42\\n1.84 Â± 1.42\\n2.06 Â± 1.48\\n1.86 Â± 1.41\\nEmbodied\\n0.80 Â± 0.00\\n1.82 Â± 1.67\\n1.33 Â± 1.57\\n1.18 Â± 1.46\\n1.30 Â± 1.51\\n1.40 Â± 1.42\\nMedical\\n1.15 Â± 1.17\\n1.56 Â± 1.41\\n0.27 Â± 0.39\\n0.30 Â± 0.58\\n2.08 Â± 1.56\\n1.81 Â± 1.42\\n3) Visual Stability (0-4): Assesses the stability of the video in terms of camera motion, object\\nappearance, and scene composition. Shaky or glitchy outputs are penalized.\\n4) Content Fidelity (0-4): Determines how accurately the key elements described in the\\nprompt are preserved. Hallucinated or missing objects/events will reduce the score.\\n5) Focus Relevance (0-4): Examines whether the videoâ€™s visual attention remains focused on\\nthe correct objects or regions throughout. Irrelevant distractions or poorly framed targets\\nare penalized.\\nWe adopt a direct prompting strategy, instructing Gemini with the prompt, videos, and evaluation\\ncriteria to produce numerical scores in JSON format directly.\\n3.4\\nQuantitative Results and Analysis\\nWe report the quantitative scores of the five evaluated models across the five reasoning dimensions in\\nTable 2, and provide detailed per-category results in Table 3 and Figure 2a.\\nOverall, most models exhibit limited reasoning capability across all tasks in MME-COF, reflected\\nby generally low scores. Among the five dimensions, Visual Stability achieves the highest average,\\nindicating that current video models can generate smooth and coherent sequences. Yet, their behavior\\nremains largely at the level of pattern replay rather than genuine reasoning.\\nThe Sora-2 series [56] shows relative advantages in physics-based, embodied, and medical reason-\\ning, while the Veo-3.0 series [21] performs comparatively better in real-world spatial reasoning.\\nSeedance-1.0-pro [19] demonstrates relative strength in rotation and 3D geometry reasoning. These\\ntrends suggest that different models specialize in distinct reasoning aspects. However, their mean\\nscores remain below 2.0 out of 4, highlighting substantial room for improvement and pointing to\\nopportunities for more targeted enhancement in future development.\\n29'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 29}, page_content='4\\nRelated Work\\nVideo Models.\\nVideo models have been progressively evolving both in the fields of video under-\\nstanding and generation. For video understanding methods, earlier approaches, such as MViT [14],\\nVideo Swin Transformer [48], and VideoMAE [62], aim to learn a robust representation that fosters\\ndownstream tasks. With the rise of LLMs, recent approaches encode videos as tokens and exploit the\\nlanguage backbone for captioning [61], event localization [59], and high-level reasoning [28, 83].\\nVideo generation models have also attracted much attention. Closed system, including OpenAIâ€™s\\nSora [55, 56], Runwayâ€™s Gen-3 [58], Pika Labs [57], Luma AI [50], and Google DeepMindâ€™s\\nVeo series [20, 21], have exhibited impressive results. However, they remain inaccessible due to\\ntheir closed-source nature. Open-source alternatives have recently become available: Stable Video\\nDiffusion [6] introduces efficient training strategies, Hunyan-Video [37] proposes systematic scaling,\\nand Wan-2.1 [64] presents an efficient 3D VAE with expanded pipelines.\\nReasoning with Video.\\nThe advent of large reasoning models [24, 60, 27, 69], such as OpenAI\\no1 [54] and DeepSeek-R1 [23], has spurred the development of video reasoning benchmarks. Most\\ncurrent methods [15, 44, 53] employ MLLMs specialized in video reasoning understanding. For\\nexample, Video-R1 [15] specifically targets temporal reasoning capabilities by introducing a temporal\\ngroup relative policy optimization (GRPO) loss. VideoChat-R1 [44] focuses on spatio-temporal\\nreasoning abilities by training with GRPO and rule-based rewards. A two-stage training strategy,\\ncombining SFT and RL, is used by VideoRFT [65]. When trained on vast collections of images\\nand videos, this strategy boosts the modelâ€™s ability to handle QA tasks, whether in general con-\\ntexts or reasoning-focused ones. These methods primarily focus on enhancing specific types of\\nquestion-answering or captioning tasks. Concurrently, [70] demonstrates the large potential of video\\ngenerative models in video reasoning. These models have implicitly acquired world knowledge\\nthroughdemonstrates impressive performance on various tasks, includinging and reasoning capability.\\nYet, this direction has rarely been explored and only experimented with in zero-shot settings.\\nEvaluation of Video Models as Zero Shot Learner.\\nRecently, several works have been exploring\\nthe zero-shot capability of video generation models in various domains, including general-purpose\\nvision understanding [70, 17], medical imaging [39], and world models [68]. [70] conducts experi-\\nments on Veo 3 with a variety of vision tasks that have not been explicitly included during training.\\nThe video model showcases surprising performance on multiple tasks like object segmentation, image\\nediting, and even maze solving. [39] later adopts a similar paradigm to medical images understanding\\ntasks and finds video generation models also show powerful capabilities, e.g., delineation of anatomi-\\ncal structures in CT scans, medical image segmentation, and even forecasting of future 3D CT phases.\\nBesides, [68] shows that video generation models could also understand complex temporal causality\\nand world knowledge in the real world, thereby serving as a world model [2, 33].\\n5\\nConclusions and Insights\\nVideo models demonstrate an intuitive understanding of the simple visual world.\\nRecent\\nvideo models can generate high-fidelity videos with realistic motion dynamics, suggesting that they\\nhave internalized substantial visual and structural knowledge about the world. Through qualitative\\nresults from our empirical study and quantitative results from the MME-COF benchmark, our work\\nconfirms that these models do exhibit intuitive yet local reasoning potential. This emergent behavior,\\nwhich aligns with the â€œChain-of-Frameâ€ (CoF) mechanism, is revealed across several common\\nsuccess patterns. (i) Fine-grained Grounding. Models demonstrate a capability for fine-grained\\nattribute and spatial grounding, especially when targets are visually distinct, as presented in visual\\ndetail reasoning tasks. (ii) Short-horizon Trace Consistency. In Visual Trace Reasoning tasks,\\nmodels can maintain short-term consistency in visual traces. (iii) Emergent Tool-Use Simulation.\\nAn emergent ability to follow CoF instructions that mimic tool-use is presented, such as drawing\\nlines in 2D geometry, highlighting targets in object counting, or controlling the camera in table\\nand chart reasoning. (iv) Foundational Spatial and Geometric Grasp. This includes single-step 3D\\ngeometry transformations, understanding basic real-world spatial layouts, finding coherent sequential\\npaths, and handling small-angle Rotations. (v) Preliminary Real-world Interaction. Models display\\na preliminary comprehension of real-world interaction, generating coherent manipulation paths in\\nembodied reasoning.\\n30'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 30}, page_content='Complex visual reasoning reveals fundamental limitations.\\nHowever, visual reasoning demands\\nmore than these foundational skills. It tests a modelâ€™s ability to maintain long-horizon logical\\nconsistency, adhere to abstract constraints, and understand functional principles. In these complex\\nareas, our study reveals fundamental limitations and several common failure patterns. (i) Causal and\\nPhysical Logic. This is evident in physics-based reasoning, where the model generates implausible\\nmotion that violates basic causal principles, and in visual trace reasoning, where the generated\\nsequences break causal order with illogical steps. (ii) Long-horizon and Rule-grounded Reasoning.\\nIn visual trace reasoning, models fail to maintain state and adhere to task-specific rules over extended\\nsequences. (iii) Geometric and Spatial Logic. Models fail at multi-step or complex transformations\\nin 3D/2D geometry and real-world spatial tasks, often breaking constraints or prioritizing visual\\nplausibility over correctness. (iv) Functional and Interaction Logic. They merely imitate GUI actions\\nwithout grasping their purpose and lack the necessary planning and stability for reliable Embodied\\ntasks, often resorting to workarounds. (v) Perceptual Precision and Specialized Knowledge. This\\nweakness appears when models fail to identify small or indistinct targets in visual detail reasoning,\\ndistort data in table and chart tasks, and fail to process specialized medical imagery due to a lack of\\ndomain understanding.\\nCurrent video models are not yet ready as standalone zero-shot reasoners.\\nOverall, our findings\\nshow that current video models are not yet reliable as standalone zero-shot reasoners. Strong\\ngenerative performance does not automatically imply robust reasoning during inference. The modelâ€™s\\nbehavior appears to be driven more by learning surface-level patterns and correlations rather than by\\ninternalizing general principles. It excels at short-term coherence rather than long-horizon causality.\\nThis is evident when the model prioritizes visual plausibility over precise spatial reasoning, or favors\\nvisually symmetric patterns over strictly adhering to geometric instructions. This tendency to produce\\nplausible but instructionally flawed outputs reveals a reasoning process that is pattern-driven, not\\nprinciple-driven, thereby undermining its ability to function as a standalone zero-shot reasoner.\\nThe potential in advancing next-generation collaborative visual reasoning.\\nDespite these\\nlimitations, the emergent behaviors observed in video models signal strong potential. The CoF\\nconcept suggests a novel modality for reasoning through visual problems step by step. While these\\nmodels are not yet robust standalone reasoners, their foundational capabilities demonstrate that they\\ncan be guided through carefully designed prompts. This suggests a path where video models exhibit\\nencouraging signs as complementary visual engines alongside dedicated reasoning models.\\nReferences\\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4\\ntechnical report. arXiv preprint arXiv:2303.08774, 2023.\\n[2] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit\\nChattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model\\nplatform for physical ai. arXiv preprint arXiv:2501.03575, 2025.\\n[3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge,\\nYu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.\\n[4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang\\nZhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding,\\nlocalization, text reading, and beyond, 2023.\\n[5] Zechen Bai, Hai Ci, and Mike Zheng Shou. Impossible videos. arXiv preprint arXiv:2503.14378,\\n2025.\\n[6] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Do-\\nminik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion:\\nScaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023.\\n[7] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja\\nFidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent\\n31'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 31}, page_content='diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition, pages 22563â€“22575, 2023.\\n[8] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\\nand Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\\n[9] Kaitao Chen, Shaohao Rui, Yankai Jiang, Jiamin Wu, Qihao Zheng, Chunfeng Song, Xiaosong\\nWang, Mu Zhou, and Mianxin Liu. Think twice to see more: Iterative visual reasoning in\\nmedical vlms. arXiv preprint arXiv:2510.10052, 2025.\\n[10] Xinyan Chen, Renrui Zhang, Dongzhi Jiang, Aojun Zhou, Shilin Yan, Weifeng Lin, and\\nHongsheng Li. Mint-cot: Enabling interleaved visual tokens in mathematical chain-of-thought\\nreasoning. arXiv preprint arXiv:2506.05331, 2025.\\n[11] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu,\\nWenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling\\nand audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024.\\n[12] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit\\nDhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the\\nfrontier with advanced reasoning, multimodality, long context, and next generation agentic\\ncapabilities. arXiv preprint arXiv:2507.06261, 2025.\\n[13] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,\\nAiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd\\nof models. arXiv e-prints, pages arXivâ€“2407, 2024.\\n[14] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and\\nChristoph Feichtenhofer. Multiscale vision transformers. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision (ICCV), pages 6824â€“6835, 2021.\\n[15] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei\\nWu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning\\nin mllms. arXiv preprint arXiv:2503.21776, 2025.\\n[16] Zhanzhou Feng, Qingpei Guo, Xinyu Xiao, Ruihan Xu, Ming Yang, and Shiliang Zhang. Unified\\nvideo generation via next-set prediction in continuous domain. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision, pages 19427â€“19438, 2025.\\n[17] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang,\\nChenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive\\nevaluation benchmark of multi-modal llms in video analysis. CVPR 2025 Highlight, 2024.\\n[18] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong,\\nJianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal\\nlarge language model. arXiv preprint arXiv:2312.11370, 2023.\\n[19] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li,\\nJiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation\\nmodels. arXiv preprint arXiv:2506.09113, 2025.\\n[20] Google DeepMind. Veo 2, 12 2024. Accessed: 2024.\\n[21] Google DeepMind. Veo-3 technical report. Technical report, Google DeepMind, May 2025.\\n[22] Kaisi Guan, Zhengfeng Lai, Yuchong Sun, Peng Zhang, Wei Liu, Kieran Liu, Meng Cao, and\\nRuihua Song. Etva: Evaluation of text-to-video alignment via fine-grained question generation\\nand answering. arXiv preprint arXiv:2503.16867, 2025.\\n[23] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\\nShirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in\\nllms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.\\n32'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 32}, page_content='[24] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan,\\nJian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint\\narXiv:2505.07062, 2025.\\n[25] Meng-Hao Guo, Xuanyu Chu, Qianrui Yang, Zhe-Han Mo, Yiqing Shen, Pei-lin Li, Xinjie Lin,\\nJinnian Zhang, Xin-Sheng Chen, Yi Zhang, et al. Rbench-v: A primary assessment for visual\\nreasoning models with multi-modal outputs. arXiv preprint arXiv:2505.16770, 2025.\\n[26] Meng-Hao Guo, Xuanyu Chu, Qianrui Yang, Zhe-Han Mo, Yiqing Shen, Pei-Lin Li, Xinjie Lin,\\nJinnian Zhang, Xin-Sheng Chen, Yi Zhang, Kiyohiro Nakayama, Zhengyang Geng, Houwen\\nPeng, Han Hu, and Shi-Min Hu. Rbench-v: A primary assessment for visual reasoning models\\nwith multi-modal outputs. 2025.\\n[27] Ziyu Guo*, Renrui Zhang*, Chengzhuo Tong*, Zhizheng Zhao*, Peng Gao, Hongsheng Li, and\\nPheng-Ann Heng. Can we generate images with cot? letâ€™s verify and reinforce image generation\\nstep by step. CVPR 2025, 2025.\\n[28] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei\\nLiu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos.\\narXiv preprint arXiv:2501.13826, 2025.\\n[29] Mengdi Jia, Zekun Qi, Shaochen Zhang, Wenyao Zhang, Xinqiang Yu, Jiawei He, He Wang, and\\nLi Yi. Omnispatial: Towards comprehensive spatial reasoning benchmark for vision language\\nmodels. arXiv preprint arXiv:2506.03135, 2025.\\n[30] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\\nSaulnier, LÃ©lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\\nLavril, Thomas Wang, TimothÃ©e Lacroix, and William El Sayed. Mistral 7b, 2023.\\n[31] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan\\nJin, Claire Guo, Shen Yan, et al. Mme-cot: Benchmarking chain-of-thought in large multimodal\\nmodels for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621,\\n2025.\\n[32] Emily Jin, Jiaheng Hu, Zhuoyi Huang, Ruohan Zhang, Jiajun Wu, Li Fei-Fei, and Roberto\\nMartÃ­n-MartÃ­n. Mini-behavior: A procedurally generated benchmark for long-horizon decision-\\nmaking in embodied ai. arXiv preprint arXiv:2310.01824, 2023.\\n[33] Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Huang Gao, and Jiashi\\nFeng. How far is video generation from world model? â€“ a physical law perspective. arXiv\\npreprint arXiv:2406.16860, 2024.\\n[34] Yoonsik Kim, Moonbin Yim, and Ka Yeon Song. Tablevqa-bench: A visual question answering\\nbenchmark on multiple table domains. arXiv preprint arXiv:2404.19205, 2024.\\n[35] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\\nlanguage models are zero-shot reasoners. Advances in neural information processing systems,\\n35:22199â€“22213, 2022.\\n[36] Dan Kondratyuk, Lijun Yu, Xiuye Gu, JosÃ© Lezama, Jonathan Huang, Grant Schindler, Rachel\\nHornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: A large language\\nmodel for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023.\\n[37] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin\\nLi, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: A systematic framework for large video\\ngenerative models. arXiv preprint arXiv:2412.03603, 2024.\\n[38] Kuaishou Technology. Kling ai: Next-generation ai creative studio. https://klingai.com/,\\nJune 2024.\\n[39] Yuxiang Lai, Jike Zhong, Ming Li, Yuheng Li, and Xiaofeng Yang. Are video models emerging\\nas zero-shot learners and reasoners in medical imaging? arXiv preprint arXiv:2510.10254,\\n2025.\\n33'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 33}, page_content='[40] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan\\nZhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint\\narXiv:2408.03326, 2024.\\n[41] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan VuliÂ´c,\\nand Furu Wei. Imagine while reasoning in space: Multimodal visualization-of-thought. arXiv\\npreprint arXiv:2501.07542, 2025.\\n[42] Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang,\\nand Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer\\nuse. arXiv preprint arXiv:2504.07981, 2025.\\n[43] Linjie Li, Mahtab Bigverdi, Jiawei Gu, Zixian Ma, Yinuo Yang, Ziang Li, Yejin Choi, and Ranjay\\nKrishna. Unfolding spatial cognition: Evaluating multimodal models on visual simulations.\\narXiv preprint arXiv:2506.04633, 2025.\\n[44] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao,\\nYi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforce-\\nment fine-tuning. arXiv preprint arXiv:2504.06958, 2025.\\n[45] Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin\\nVan Durme, and Alan L Yuille. Super-clevr: A virtual benchmark to diagnose domain robustness\\nin visual reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition, pages 14963â€“14973, 2023.\\n[46] Dairu Liu, Ziyue Wang, Minyuan Ruan, Fuwen Luo, Chi Chen, Peng Li, and Yang Liu. Visual\\nabstract thinking empowers multimodal reasoning. arXiv preprint arXiv:2505.20164, 2025.\\n[47] Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, and\\nLu Hou. Fetv: A benchmark for fine-grained evaluation of open-domain text-to-video generation.\\nAdvances in Neural Information Processing Systems, 36:62352â€“62387, 2023.\\n[48] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin\\ntransformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 3202â€“3211, 2022.\\n[49] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind\\nTafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought\\nchains for science question answering. Advances in Neural Information Processing Systems,\\n35:2507â€“2521, 2022.\\n[50] LumaLabs. Dream machine, 06 2024. Accessed: 2024.\\n[51] Yulin Luo, Chun-Kai Fan, Menghang Dong, Jiayu Shi, Mengdi Zhao, Bo-Wen Zhang, Cheng\\nChi, Jiaming Liu, Gaole Dai, Rongyu Zhang, Ruichuan An, Kun Wu, Zhengping Che, Shaoxuan\\nXie, Guocai Yao, Zhongxia Zhao, Pengwei Wang, Guang Liu, Zhongyuan Wang, Tiejun Huang,\\nand Shanghang Zhang. Robobench: A comprehensive evaluation benchmark for multimodal\\nlarge language models as embodied brain, 2025.\\n[52] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: A benchmark\\nfor question answering about charts with visual and logical reasoning. In Findings of the\\nAssociation for Computational Linguistics: ACL 2022, pages 2263â€“2279, Dublin, Ireland, May\\n2022. Association for Computational Linguistics.\\n[53] Jiahao Meng, Xiangtai Li, Haochen Wang, Yue Tan, Tao Zhang, Lingdong Kong, Yunhai Tong,\\nAnran Wang, Zhiyang Teng, Yujing Wang, and Zhuochen Wang. Open-o3 video: Grounded\\nvideo reasoning with explicit spatio-temporal evidence. arXiv preprint arXiv:2510.20579, 2025.\\n[54] OpenAI.\\nOpenai\\no1\\nsystem\\ncard.\\nhttps://openai.com/index/\\nopenai-o1-system-card/, December 2024. Accessed: 2024-12-05.\\n[55] OpenAI. Video generation models as world simulators. Technical report, OpenAI, 2024.\\n[56] OpenAI. Sora 2 system card. Technical report, OpenAI, September 2025.\\n34'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 34}, page_content='[57] PikaLabs. Pika 1.5, 10 2024. Accessed: 2024.\\n[58] Runway. Introducing gen-3 alpha: A new frontier for video generation. https://runwayml.\\ncom/research/introducing-gen-3-alpha/, June 2024.\\n[59] Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu. Audio-visual event\\nlocalization in unconstrained videos. In Proceedings of the European conference on computer\\nvision (ECCV), pages 247â€“263, 2018.\\n[60] Chengzhuo Tong*, Ziyu Guo*, Renrui Zhang*, Wenyu Shan*, Xinyu Wei, Zhenghao Xing,\\nHongsheng Li, and Pheng-Ann Heng. Delving into rl for image generation with cot: A study on\\ndpo vs. grpo. arXiv preprint arXiv:2505.17017, 2025.\\n[61] Tony Cheng Tong, Sirui He, Zhiwen Shao, and Dit-Yan Yeung. G-veval: A versatile metric for\\nevaluating image and video captions using gpt-4o. In Proceedings of the AAAI Conference on\\nArtificial Intelligence, volume 39, pages 7419â€“7427, 2025.\\n[62] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are\\ndata-efficient learners for self-supervised video pre-training. In Advances in Neural Information\\nProcessing Systems (NeurIPS), 2022.\\n[63] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu,\\nHaiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative\\nmodels. arXiv preprint arXiv:2503.20314, 2025.\\n[64] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao,\\nJianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative\\nmodels. arXiv preprint arXiv:2503.20314, 2025.\\n[65] Qi Wang, Yanrui Yu, Ye Yuan, Rui Mao, and Tianfei Zhou. Videorft: Incentivizing video\\nreasoning capability in mllms via reinforced fine-tuning. arXiv preprint arXiv:2505.12434,\\n2025.\\n[66] Siting Wang, Minnan Pei, Luoyang Sun, Cheng Deng, Kun Shao, Zheng Tian, Haifeng Zhang,\\nand Jun Wang. Spatialviz-bench: An mllm benchmark for spatial visualization. arXiv preprint\\narXiv:2507.07610, 2025.\\n[67] Xuehui Wang, Zhenyu Wu, JingJing Xie, Zichen Ding, Bowen Yang, Zehao Li, Zhaoyang\\nLiu, Qingyun Li, Xuan Dong, Zhe Chen, et al. Mmbench-gui: Hierarchical multi-platform\\nevaluation framework for gui agents. arXiv preprint arXiv:2507.19478, 2025.\\n[68] Zeqing Wang, Xinyu Wei, Bairui Li, Zhen Guo, Jinrui Zhang, Hongyang Wei, Keze Wang, and\\nLei Zhang. Videoverse: How far is your t2v generator from a world model? arXiv preprint\\narXiv:2510.08398, 2025.\\n[69] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\\nAdvances in neural information processing systems, 35:24824â€“24837, 2022.\\n[70] ThaddÃ¤us Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky,\\nBeen Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners.\\narXiv preprint arXiv:2509.20328, 2025.\\n[71] Penghao Wu and Saining Xie. V?: Guided visual search as a core mechanism in multimodal\\nllms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\\npages 13084â€“13094, 2024.\\n[72] Qiucheng Wu, Handong Zhao, Michael Saxon, Trung Bui, William Yang Wang, Yang Zhang,\\nand Shiyu Chang. Vsp: Assessing the dual challenges of perception and reasoning in spatial\\nplanning tasks for vlms. arXiv preprint arXiv:2407.01863, 2024.\\n[73] Weiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu,\\nHouqiang Li, Xiaohua Wang, Xizhou Zhu, Wenhai Wang, Jifeng Dai, and Jinguo Zhu. Visulogic:\\nA benchmark for evaluating visual reasoning in multi-modal large language models. arXiv\\npreprint arXiv:2504.15279, 2025.\\n35'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 35}, page_content='[74] Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen\\nChen, Haodong Duan, Xiangyu Yue, et al. Mmsi-bench: A benchmark for multi-image spatial\\nintelligence. arXiv preprint arXiv:2505.23764, 2025.\\n[75] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming\\nYang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion\\nmodels with an expert transformer. arXiv preprint arXiv:2408.06072, 2024.\\n[76] Lijun Yu, Yong Cheng, Kihyuk Sohn, JosÃ© Lezama, Han Zhang, Huiwen Chang, Alexander G\\nHauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video\\ntransformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 10459â€“10469, 2023.\\n[77] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens,\\nDongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal\\nunderstanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition, pages 9556â€“9567, 2024.\\n[78] Chenyu Zhang, Daniil Cherniavskii, Andrii Zadaianchuk, Antonios Tragoudaras, Antonios\\nVozikis, Thijmen Nijdam, Derck WE Prinzhorn, Mark Bodracska, Nicu Sebe, and Efstratios\\nGavves. Morpheus: Benchmarking physical reasoning of video generative models with real\\nphysical experiments. arXiv preprint arXiv:2504.02918, 2025.\\n[79] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun\\nZhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly\\nsee the diagrams in visual math problems? ECCV 2024, 2024.\\n[80] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming\\nLiu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual instruction\\ntuning. arXiv e-prints, pages arXivâ€“2407, 2024.\\n[81] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu,\\nand Chunyuan Li. Llava-next: A strong zero-shot video understanding model, April 2024.\\n[82] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting\\nin large language models. arXiv preprint arXiv:2210.03493, 2022.\\n[83] Yilun Zhao, Haowei Zhang, Lujing Xie, Tongyan Hu, Guo Gan, Yitao Long, Zhiyuan Hu,\\nWeiyuan Chen, Chuhan Li, Zhijian Xu, et al. Mmvu: Measuring expert-level multi-discipline\\nvideo understanding. In Proceedings of the Computer Vision and Pattern Recognition Confer-\\nence, pages 8475â€“8489, 2025.\\n[84] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun\\nZhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all.\\narXiv preprint arXiv:2412.20404, 2024.\\n[85] Wenwen Zhuang, Xin Huang, Xiantao Zhang, and Jin Zeng. Math-puma: Progressive up-\\nward multimodal alignment to enhance mathematical reasoning. In Proceedings of the AAAI\\nConference on Artificial Intelligence, volume 39, pages 26183â€“26191, 2025.\\n36')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = pdf_reader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e179df0-ffbd-4568-a34e-c78f4880d2b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae2258a5-6732-4828-b04f-e7080ff71bc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 421 images: ['extracted_images/page2_img1.png', 'extracted_images/page2_img2.png', 'extracted_images/page2_img3.png', 'extracted_images/page2_img4.png', 'extracted_images/page2_img5.png', 'extracted_images/page2_img6.png', 'extracted_images/page2_img7.png', 'extracted_images/page2_img8.png', 'extracted_images/page2_img9.png', 'extracted_images/page2_img10.png', 'extracted_images/page2_img11.png', 'extracted_images/page2_img12.png', 'extracted_images/page2_img13.png', 'extracted_images/page2_img14.png', 'extracted_images/page2_img15.png', 'extracted_images/page2_img16.png', 'extracted_images/page2_img17.png', 'extracted_images/page2_img18.png', 'extracted_images/page2_img19.jpeg', 'extracted_images/page2_img20.jpeg', 'extracted_images/page2_img21.jpeg', 'extracted_images/page2_img22.jpeg', 'extracted_images/page2_img23.png', 'extracted_images/page3_img1.png', 'extracted_images/page3_img2.jpeg', 'extracted_images/page3_img3.jpeg', 'extracted_images/page3_img4.jpeg', 'extracted_images/page3_img5.jpeg', 'extracted_images/page5_img1.jpeg', 'extracted_images/page5_img2.jpeg', 'extracted_images/page5_img3.jpeg', 'extracted_images/page5_img4.jpeg', 'extracted_images/page5_img5.jpeg', 'extracted_images/page5_img6.jpeg', 'extracted_images/page5_img7.jpeg', 'extracted_images/page5_img8.jpeg', 'extracted_images/page5_img9.jpeg', 'extracted_images/page5_img10.jpeg', 'extracted_images/page5_img11.jpeg', 'extracted_images/page5_img12.jpeg', 'extracted_images/page5_img13.jpeg', 'extracted_images/page5_img14.jpeg', 'extracted_images/page5_img15.jpeg', 'extracted_images/page5_img16.jpeg', 'extracted_images/page5_img17.jpeg', 'extracted_images/page5_img18.jpeg', 'extracted_images/page5_img19.jpeg', 'extracted_images/page5_img20.jpeg', 'extracted_images/page5_img21.jpeg', 'extracted_images/page5_img22.jpeg', 'extracted_images/page5_img23.jpeg', 'extracted_images/page5_img24.jpeg', 'extracted_images/page5_img25.jpeg', 'extracted_images/page5_img26.jpeg', 'extracted_images/page5_img27.jpeg', 'extracted_images/page5_img28.jpeg', 'extracted_images/page5_img29.jpeg', 'extracted_images/page5_img30.jpeg', 'extracted_images/page5_img31.jpeg', 'extracted_images/page5_img32.jpeg', 'extracted_images/page7_img1.png', 'extracted_images/page7_img2.jpeg', 'extracted_images/page7_img3.jpeg', 'extracted_images/page7_img4.jpeg', 'extracted_images/page7_img5.jpeg', 'extracted_images/page7_img6.jpeg', 'extracted_images/page7_img7.jpeg', 'extracted_images/page7_img8.jpeg', 'extracted_images/page7_img9.jpeg', 'extracted_images/page7_img10.jpeg', 'extracted_images/page7_img11.jpeg', 'extracted_images/page7_img12.jpeg', 'extracted_images/page7_img13.jpeg', 'extracted_images/page7_img14.jpeg', 'extracted_images/page7_img15.jpeg', 'extracted_images/page7_img16.jpeg', 'extracted_images/page7_img17.jpeg', 'extracted_images/page7_img18.jpeg', 'extracted_images/page7_img19.jpeg', 'extracted_images/page7_img20.jpeg', 'extracted_images/page7_img21.jpeg', 'extracted_images/page7_img22.jpeg', 'extracted_images/page7_img23.jpeg', 'extracted_images/page7_img24.jpeg', 'extracted_images/page7_img25.jpeg', 'extracted_images/page7_img26.jpeg', 'extracted_images/page7_img27.jpeg', 'extracted_images/page7_img28.jpeg', 'extracted_images/page7_img29.jpeg', 'extracted_images/page7_img30.jpeg', 'extracted_images/page7_img31.jpeg', 'extracted_images/page7_img32.jpeg', 'extracted_images/page7_img33.jpeg', 'extracted_images/page7_img34.jpeg', 'extracted_images/page7_img35.jpeg', 'extracted_images/page7_img36.png', 'extracted_images/page8_img1.png', 'extracted_images/page8_img2.jpeg', 'extracted_images/page8_img3.jpeg', 'extracted_images/page8_img4.jpeg', 'extracted_images/page8_img5.jpeg', 'extracted_images/page8_img6.jpeg', 'extracted_images/page8_img7.jpeg', 'extracted_images/page8_img8.jpeg', 'extracted_images/page8_img9.jpeg', 'extracted_images/page8_img10.jpeg', 'extracted_images/page8_img11.jpeg', 'extracted_images/page8_img12.jpeg', 'extracted_images/page8_img13.jpeg', 'extracted_images/page8_img14.jpeg', 'extracted_images/page8_img15.jpeg', 'extracted_images/page8_img16.jpeg', 'extracted_images/page8_img17.jpeg', 'extracted_images/page10_img1.png', 'extracted_images/page10_img2.png', 'extracted_images/page10_img3.jpeg', 'extracted_images/page10_img4.jpeg', 'extracted_images/page10_img5.jpeg', 'extracted_images/page10_img6.jpeg', 'extracted_images/page10_img7.jpeg', 'extracted_images/page10_img8.jpeg', 'extracted_images/page10_img9.jpeg', 'extracted_images/page10_img10.jpeg', 'extracted_images/page10_img11.png', 'extracted_images/page10_img12.jpeg', 'extracted_images/page10_img13.jpeg', 'extracted_images/page10_img14.jpeg', 'extracted_images/page10_img15.jpeg', 'extracted_images/page10_img16.jpeg', 'extracted_images/page10_img17.jpeg', 'extracted_images/page10_img18.jpeg', 'extracted_images/page10_img19.jpeg', 'extracted_images/page10_img20.jpeg', 'extracted_images/page10_img21.jpeg', 'extracted_images/page10_img22.jpeg', 'extracted_images/page10_img23.jpeg', 'extracted_images/page10_img24.jpeg', 'extracted_images/page10_img25.jpeg', 'extracted_images/page11_img1.jpeg', 'extracted_images/page11_img2.jpeg', 'extracted_images/page11_img3.jpeg', 'extracted_images/page11_img4.jpeg', 'extracted_images/page11_img5.jpeg', 'extracted_images/page11_img6.png', 'extracted_images/page11_img7.png', 'extracted_images/page11_img8.jpeg', 'extracted_images/page11_img9.jpeg', 'extracted_images/page11_img10.jpeg', 'extracted_images/page11_img11.jpeg', 'extracted_images/page11_img12.jpeg', 'extracted_images/page11_img13.jpeg', 'extracted_images/page11_img14.jpeg', 'extracted_images/page11_img15.jpeg', 'extracted_images/page11_img16.jpeg', 'extracted_images/page11_img17.jpeg', 'extracted_images/page11_img18.jpeg', 'extracted_images/page11_img19.jpeg', 'extracted_images/page11_img20.jpeg', 'extracted_images/page11_img21.jpeg', 'extracted_images/page11_img22.jpeg', 'extracted_images/page11_img23.jpeg', 'extracted_images/page11_img24.jpeg', 'extracted_images/page11_img25.jpeg', 'extracted_images/page11_img26.jpeg', 'extracted_images/page11_img27.jpeg', 'extracted_images/page11_img28.jpeg', 'extracted_images/page11_img29.jpeg', 'extracted_images/page11_img30.jpeg', 'extracted_images/page12_img1.jpeg', 'extracted_images/page12_img2.jpeg', 'extracted_images/page12_img3.jpeg', 'extracted_images/page12_img4.jpeg', 'extracted_images/page12_img5.jpeg', 'extracted_images/page12_img6.jpeg', 'extracted_images/page12_img7.jpeg', 'extracted_images/page12_img8.jpeg', 'extracted_images/page12_img9.jpeg', 'extracted_images/page12_img10.jpeg', 'extracted_images/page12_img11.jpeg', 'extracted_images/page12_img12.jpeg', 'extracted_images/page14_img1.png', 'extracted_images/page14_img2.png', 'extracted_images/page14_img3.png', 'extracted_images/page14_img4.jpeg', 'extracted_images/page14_img5.jpeg', 'extracted_images/page14_img6.jpeg', 'extracted_images/page14_img7.jpeg', 'extracted_images/page14_img8.jpeg', 'extracted_images/page14_img9.jpeg', 'extracted_images/page14_img10.jpeg', 'extracted_images/page14_img11.jpeg', 'extracted_images/page14_img12.jpeg', 'extracted_images/page14_img13.jpeg', 'extracted_images/page14_img14.jpeg', 'extracted_images/page14_img15.jpeg', 'extracted_images/page14_img16.jpeg', 'extracted_images/page14_img17.jpeg', 'extracted_images/page14_img18.jpeg', 'extracted_images/page14_img19.jpeg', 'extracted_images/page14_img20.jpeg', 'extracted_images/page14_img21.jpeg', 'extracted_images/page14_img22.jpeg', 'extracted_images/page14_img23.jpeg', 'extracted_images/page14_img24.jpeg', 'extracted_images/page14_img25.jpeg', 'extracted_images/page14_img26.jpeg', 'extracted_images/page14_img27.jpeg', 'extracted_images/page14_img28.jpeg', 'extracted_images/page14_img29.jpeg', 'extracted_images/page14_img30.jpeg', 'extracted_images/page14_img31.jpeg', 'extracted_images/page15_img1.png', 'extracted_images/page15_img2.png', 'extracted_images/page15_img3.jpeg', 'extracted_images/page15_img4.jpeg', 'extracted_images/page15_img5.jpeg', 'extracted_images/page15_img6.jpeg', 'extracted_images/page15_img7.jpeg', 'extracted_images/page15_img8.jpeg', 'extracted_images/page15_img9.jpeg', 'extracted_images/page15_img10.jpeg', 'extracted_images/page15_img11.jpeg', 'extracted_images/page15_img12.jpeg', 'extracted_images/page15_img13.jpeg', 'extracted_images/page15_img14.jpeg', 'extracted_images/page15_img15.jpeg', 'extracted_images/page15_img16.jpeg', 'extracted_images/page15_img17.jpeg', 'extracted_images/page15_img18.jpeg', 'extracted_images/page15_img19.jpeg', 'extracted_images/page15_img20.jpeg', 'extracted_images/page15_img21.jpeg', 'extracted_images/page15_img22.jpeg', 'extracted_images/page15_img23.jpeg', 'extracted_images/page15_img24.jpeg', 'extracted_images/page15_img25.jpeg', 'extracted_images/page15_img26.jpeg', 'extracted_images/page15_img27.jpeg', 'extracted_images/page15_img28.jpeg', 'extracted_images/page15_img29.jpeg', 'extracted_images/page15_img30.jpeg', 'extracted_images/page15_img31.jpeg', 'extracted_images/page16_img1.png', 'extracted_images/page16_img2.jpeg', 'extracted_images/page16_img3.jpeg', 'extracted_images/page16_img4.jpeg', 'extracted_images/page16_img5.jpeg', 'extracted_images/page16_img6.jpeg', 'extracted_images/page16_img7.jpeg', 'extracted_images/page16_img8.jpeg', 'extracted_images/page16_img9.jpeg', 'extracted_images/page16_img10.jpeg', 'extracted_images/page16_img11.jpeg', 'extracted_images/page16_img12.jpeg', 'extracted_images/page16_img13.jpeg', 'extracted_images/page16_img14.jpeg', 'extracted_images/page16_img15.png', 'extracted_images/page16_img16.jpeg', 'extracted_images/page16_img17.jpeg', 'extracted_images/page16_img18.jpeg', 'extracted_images/page16_img19.jpeg', 'extracted_images/page16_img20.jpeg', 'extracted_images/page16_img21.jpeg', 'extracted_images/page16_img22.jpeg', 'extracted_images/page16_img23.jpeg', 'extracted_images/page16_img24.jpeg', 'extracted_images/page16_img25.jpeg', 'extracted_images/page16_img26.jpeg', 'extracted_images/page18_img1.png', 'extracted_images/page18_img2.jpeg', 'extracted_images/page18_img3.jpeg', 'extracted_images/page18_img4.jpeg', 'extracted_images/page18_img5.jpeg', 'extracted_images/page18_img6.jpeg', 'extracted_images/page18_img7.jpeg', 'extracted_images/page18_img8.png', 'extracted_images/page18_img9.jpeg', 'extracted_images/page18_img10.jpeg', 'extracted_images/page18_img11.jpeg', 'extracted_images/page18_img12.jpeg', 'extracted_images/page18_img13.jpeg', 'extracted_images/page18_img14.jpeg', 'extracted_images/page18_img15.jpeg', 'extracted_images/page18_img16.jpeg', 'extracted_images/page18_img17.jpeg', 'extracted_images/page18_img18.jpeg', 'extracted_images/page18_img19.jpeg', 'extracted_images/page18_img20.jpeg', 'extracted_images/page18_img21.jpeg', 'extracted_images/page18_img22.jpeg', 'extracted_images/page18_img23.jpeg', 'extracted_images/page18_img24.jpeg', 'extracted_images/page18_img25.jpeg', 'extracted_images/page18_img26.jpeg', 'extracted_images/page18_img27.jpeg', 'extracted_images/page18_img28.jpeg', 'extracted_images/page18_img29.jpeg', 'extracted_images/page18_img30.jpeg', 'extracted_images/page18_img31.jpeg', 'extracted_images/page20_img1.png', 'extracted_images/page20_img2.png', 'extracted_images/page20_img3.jpeg', 'extracted_images/page20_img4.jpeg', 'extracted_images/page20_img5.jpeg', 'extracted_images/page20_img6.jpeg', 'extracted_images/page20_img7.jpeg', 'extracted_images/page20_img8.jpeg', 'extracted_images/page20_img9.png', 'extracted_images/page20_img10.png', 'extracted_images/page20_img11.jpeg', 'extracted_images/page20_img12.jpeg', 'extracted_images/page20_img13.jpeg', 'extracted_images/page20_img14.jpeg', 'extracted_images/page20_img15.jpeg', 'extracted_images/page20_img16.jpeg', 'extracted_images/page20_img17.jpeg', 'extracted_images/page20_img18.jpeg', 'extracted_images/page20_img19.jpeg', 'extracted_images/page20_img20.jpeg', 'extracted_images/page20_img21.jpeg', 'extracted_images/page20_img22.jpeg', 'extracted_images/page20_img23.jpeg', 'extracted_images/page20_img24.jpeg', 'extracted_images/page20_img25.jpeg', 'extracted_images/page20_img26.jpeg', 'extracted_images/page20_img27.jpeg', 'extracted_images/page22_img1.png', 'extracted_images/page22_img2.jpeg', 'extracted_images/page22_img3.jpeg', 'extracted_images/page22_img4.jpeg', 'extracted_images/page22_img5.jpeg', 'extracted_images/page22_img6.jpeg', 'extracted_images/page22_img7.jpeg', 'extracted_images/page22_img8.png', 'extracted_images/page22_img9.jpeg', 'extracted_images/page22_img10.jpeg', 'extracted_images/page22_img11.jpeg', 'extracted_images/page22_img12.jpeg', 'extracted_images/page22_img13.jpeg', 'extracted_images/page22_img14.jpeg', 'extracted_images/page22_img15.jpeg', 'extracted_images/page22_img16.jpeg', 'extracted_images/page23_img1.jpeg', 'extracted_images/page23_img2.jpeg', 'extracted_images/page23_img3.jpeg', 'extracted_images/page23_img4.jpeg', 'extracted_images/page23_img5.jpeg', 'extracted_images/page23_img6.jpeg', 'extracted_images/page23_img7.jpeg', 'extracted_images/page23_img8.png', 'extracted_images/page23_img9.jpeg', 'extracted_images/page23_img10.jpeg', 'extracted_images/page23_img11.png', 'extracted_images/page23_img12.jpeg', 'extracted_images/page23_img13.jpeg', 'extracted_images/page23_img14.jpeg', 'extracted_images/page23_img15.jpeg', 'extracted_images/page23_img16.jpeg', 'extracted_images/page23_img17.png', 'extracted_images/page23_img18.jpeg', 'extracted_images/page23_img19.jpeg', 'extracted_images/page23_img20.jpeg', 'extracted_images/page23_img21.jpeg', 'extracted_images/page23_img22.jpeg', 'extracted_images/page24_img1.jpeg', 'extracted_images/page24_img2.jpeg', 'extracted_images/page24_img3.jpeg', 'extracted_images/page24_img4.jpeg', 'extracted_images/page24_img5.jpeg', 'extracted_images/page24_img6.jpeg', 'extracted_images/page24_img7.jpeg', 'extracted_images/page24_img8.jpeg', 'extracted_images/page24_img9.jpeg', 'extracted_images/page24_img10.jpeg', 'extracted_images/page24_img11.jpeg', 'extracted_images/page24_img12.jpeg', 'extracted_images/page24_img13.jpeg', 'extracted_images/page24_img14.jpeg', 'extracted_images/page24_img15.jpeg', 'extracted_images/page24_img16.jpeg', 'extracted_images/page24_img17.jpeg', 'extracted_images/page24_img18.jpeg', 'extracted_images/page24_img19.jpeg', 'extracted_images/page25_img1.jpeg', 'extracted_images/page25_img2.png', 'extracted_images/page25_img3.jpeg', 'extracted_images/page25_img4.png', 'extracted_images/page25_img5.jpeg', 'extracted_images/page25_img6.jpeg', 'extracted_images/page25_img7.jpeg', 'extracted_images/page25_img8.jpeg', 'extracted_images/page25_img9.jpeg', 'extracted_images/page25_img10.jpeg', 'extracted_images/page25_img11.jpeg', 'extracted_images/page25_img12.jpeg', 'extracted_images/page25_img13.jpeg', 'extracted_images/page25_img14.jpeg', 'extracted_images/page25_img15.jpeg', 'extracted_images/page25_img16.jpeg', 'extracted_images/page25_img17.jpeg', 'extracted_images/page25_img18.jpeg', 'extracted_images/page25_img19.jpeg', 'extracted_images/page27_img1.jpeg', 'extracted_images/page27_img2.jpeg', 'extracted_images/page27_img3.jpeg', 'extracted_images/page27_img4.jpeg', 'extracted_images/page27_img5.jpeg', 'extracted_images/page27_img6.jpeg', 'extracted_images/page27_img7.png', 'extracted_images/page27_img8.jpeg', 'extracted_images/page27_img9.jpeg', 'extracted_images/page27_img10.jpeg', 'extracted_images/page27_img11.jpeg', 'extracted_images/page27_img12.jpeg', 'extracted_images/page27_img13.jpeg', 'extracted_images/page27_img14.jpeg', 'extracted_images/page27_img15.jpeg', 'extracted_images/page27_img16.jpeg', 'extracted_images/page27_img17.jpeg', 'extracted_images/page27_img18.jpeg', 'extracted_images/page27_img19.jpeg']\n"
     ]
    }
   ],
   "source": [
    "import pymupdf  # PyMuPDF\n",
    "import os\n",
    "\n",
    "pdf_path = temp_file_path #\"example.pdf\"\n",
    "output_dir = \"extracted_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "doc = pymupdf.open(pdf_path)\n",
    "image_files = []\n",
    "\n",
    "for page_number in range(len(doc)):\n",
    "    page = doc[page_number]\n",
    "    images = page.get_images(full=True)\n",
    "    for img_index, img in enumerate(images, start=1):\n",
    "        xref = img[0]\n",
    "        base_image = doc.extract_image(xref)\n",
    "        image_bytes = base_image[\"image\"]\n",
    "        image_ext = base_image[\"ext\"]\n",
    "        image_filename = f\"{output_dir}/page{page_number+1}_img{img_index}.{image_ext}\"\n",
    "        with open(image_filename, \"wb\") as f:\n",
    "            f.write(image_bytes)\n",
    "        image_files.append(image_filename)\n",
    "\n",
    "print(f\"Extracted {len(image_files)} images:\", image_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d605bc3-e72a-4739-8c3c-20ebf194f8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "67829e34-e7fd-4649-b77b-10bea08113e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete. Check extracted_images\n"
     ]
    }
   ],
   "source": [
    "import pymupdf\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "pdf_path = temp_file_path\n",
    "output_folder = \"extracted_images\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "doc = pymupdf.open(pdf_path)\n",
    "\n",
    "def is_blank(image_bytes):\n",
    "    \"\"\"Return True if image is mostly blank/white.\"\"\"\n",
    "    img = Image.open(io.BytesIO(image_bytes)).convert(\"L\")  # grayscale\n",
    "    arr = np.array(img)\n",
    "    if np.mean(arr) > 250:  # mostly white\n",
    "        return True\n",
    "    elif np.mean(arr) < 10:  # mostly white\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "for page_index, page in enumerate(doc):\n",
    "    images = page.get_images(full=True)\n",
    "    found = False\n",
    "    for img_index, img in enumerate(images):\n",
    "        xref = img[0]\n",
    "        base_image = doc.extract_image(xref)\n",
    "        image_bytes = base_image[\"image\"]\n",
    "        ext = base_image[\"ext\"]\n",
    "\n",
    "        # Skip blank images\n",
    "        if is_blank(image_bytes):\n",
    "            continue\n",
    "\n",
    "        found = True\n",
    "        image_path = os.path.join(output_folder, f\"page{page_index}_img{img_index}.{ext}\")\n",
    "        with open(image_path, \"wb\") as f:\n",
    "            f.write(image_bytes)\n",
    "\n",
    "    # # If no visible images found, optionally render the page\n",
    "    # if not found:\n",
    "    #     pix = page.get_pixmap(dpi=300)\n",
    "    #     img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    #     page_image_path = os.path.join(output_folder, f\"page{page_index}_rendered.png\")\n",
    "    #     img.save(page_image_path)\n",
    "\n",
    "print(f\"Extraction complete. Check {output_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426cd29a-f406-4acf-9eb0-db019a94a158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec12b3b4-bcbb-4c7e-b82e-76fc31cb2660",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "  {\n",
      "    \"title\": \"Why did the scarecrow win an award?\",\n",
      "    \"summary\": \"A farmer recognized the scarecrow's outstanding performance in protecting the crops.\",\n",
      "    \"overview\": \"This classic joke relies on wordplay and a humorous misunderstanding of a scarecrow's purpose.\"\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from agents.ollama_agent import OllamaAgent\n",
    "\n",
    "llm = OllamaAgent(model=\"gemma3:4b\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    llm_response = llm.chat(\"Tell me a joke\")\n",
    "    print(llm_response)\n",
    "    # print(llm.chat(\"What is few-shot learning?\", [\"LLMs can learn from few examples.\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "471f8848-abe1-445e-a38c-36de0f0356f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjsonify_llm_response\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m jsonify_llm_response\n\u001b[1;32m----> 2\u001b[0m jsonify_llm_response(\u001b[43mllm_response\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'llm_response' is not defined"
     ]
    }
   ],
   "source": [
    "from utils.jsonify_llm_response import jsonify_llm_response\n",
    "jsonify_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35703cf9-53da-4e62-9b41-381e361c6d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab08e975-133b-4751-a9a0-ecaf2aecc4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import json\n",
    "\n",
    "# # MCP server URL\n",
    "# BASE_URL = \"http://127.0.0.1:8080\"\n",
    "\n",
    "# def analyze_with_llm(query: str, context_query: str = None):\n",
    "#     \"\"\"\n",
    "#     Calls the MCP server's /analyze_with_llm endpoint.\n",
    "#     \"\"\"\n",
    "#     context_query = context_query or query\n",
    "#     url = f\"{BASE_URL}/analyze_with_llm\"\n",
    "#     params = {\"query\": query, \"context_query\": context_query}\n",
    "\n",
    "#     try:\n",
    "#         response = requests.post(url, params=params)\n",
    "#         response.raise_for_status()\n",
    "#         data = response.json()\n",
    "\n",
    "#         print(\"\\nðŸ§  LLM Research Summary\")\n",
    "#         print(\"=\" * 60)\n",
    "#         print(f\"Query: {data.get('query')}\")\n",
    "#         print(f\"Context used: {data.get('context_used', 0)} papers\")\n",
    "#         print(\"\\n--- LLM Response ---\\n\")\n",
    "#         print(data.get(\"llm_response\", \"No response generated.\"))\n",
    "#         print(\"=\" * 60)\n",
    "\n",
    "#         return data\n",
    "\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print(\"âŒ Request failed:\", e)\n",
    "#         if e.response is not None:\n",
    "#             print(\"Response text:\", e.response.text)\n",
    "#         return None\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # url = f\"{BASE_URL}/analyze_with_llm\"    \n",
    "#     # ðŸ” Example query\n",
    "#     query = \"Summarize recent advances in vision transformers\"\n",
    "#     context_query = query or None\n",
    "#     params = {\"query\": query, \"context_query\": context_query}\n",
    "#     url = f\"{BASE_URL}/fetch_all_sources\"\n",
    "#     response = requests.get(url, params=params)\n",
    "#     context_query = response.json()\n",
    "#     process_with_llm(query, context_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6297b18e-6e58-4170-a013-765d84bee2c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Summarize recent advances in vision transformers', 'length_of_context_used': 48, 'llm_response': '```json\\n[\\n  {\\n    \"title\": \"Increased Model Size & Training Data\",\\n    \"summary\": \"Recent advancements in ViTs heavily rely on scaling up model size and training datasets significantly. Larger models like Swin Transformer and PaViT demonstrate improved performance.\",\\n    \"overview\": \"This scaling trend, driven by compute advancements, has been a primary factor in boosting ViT performance, allowing them to rival and even surpass convolutional networks in certain tasks.\"\\n  },\\n  {\\n    \"title\": \"Swin Transformer\",\\n    \"summary\": \"The Swin Transformer utilizes a hierarchical structure with shifted windows, improving computational efficiency and performance, especially for high-resolution images.\",\\n    \"overview\": \"Swin Transformer\\'s hierarchical design addresses the computational bottleneck of standard ViTs, enabling applications in dense vision tasks like object detection and semantic segmentation.\"\\n  },\\n  {\\n    \"title\": \"PaViT\",\\n    \"summary\": \"PaViT introduces a Path Aggregation Network (PANet) which facilitates information flow between different scales within the transformer architecture, improving its effectiveness.\",\\n    \"overview\": \"PaViTâ€™s architecture significantly enhances ViTâ€™s ability to capture long-range dependencies, leading to better performance in various visual tasks.\"\\n  },\\n  {\\n    \"title\": \"Efficient ViT Variants\",\\n    \"summary\": \"Research continues on creating more efficient ViT variants, such as MobileViT and DistilViT, focusing on model compression and speed optimization.\",\\n    \"overview\": \"These developments make ViTs more practical for deployment on resource-constrained devices and accelerate inference times.\"\\n  }\\n]\\n```'}\n",
      "{'query': 'Summarize recent advances in vision transformers', 'length_of_context_used': 48, 'llm_response': '```json\\n[\\n  {\\n    \"title\": \"Efficient Transformers\",\\n    \"summary\": \"Research focuses on reducing the computational cost of Transformers through techniques like Swin Transformers and ConvNeXt, which incorporate convolutional layers to improve efficiency and performance.\",\\n    \"overview\": \"These advancements drastically improve the scalability of Vision Transformers, making them viable for high-resolution image tasks and enabling deployment on less powerful hardware.\"\\n  },\\n  {\\n    \"title\": \"Hybrid Architectures\",\\n    \"summary\": \"Combining Vision Transformers with convolutional neural networks (CNNs) is a prevalent trend. This allows leveraging the strengths of both architectures â€“ CNNs for local feature extraction and Transformers for global context understanding.\",\\n    \"overview\": \"Hybrid models demonstrate superior performance on many benchmarks, showcasing a balanced approach to image processing.\"\\n  },\\n  {\\n    \"title\": \"Attention Mechanisms\",\\n    \"summary\": \"Innovations are being made to attention mechanisms, including alternatives to standard self-attention, such as linear attention and sparse attention, to reduce computational demands while maintaining accuracy.\",\\n    \"overview\": \"These modifications address the quadratic complexity of self-attention, allowing Transformers to handle longer sequences and more complex image features.\"\\n  },\\n  {\\n    \"title\": \"Pre-training Strategies\",\\n    \"summary\": \"New pre-training methods, like masked image modeling (MIM) applied to Vision Transformers, are improving their representation learning capabilities.\",\\n    \"overview\": \"Pre-training allows Vision Transformers to learn more effective image features from large datasets, leading to better performance on downstream tasks.\"\\n  }\\n]\\n```'}\n",
      "{'query': 'Summarize recent advances in vision transformers', 'length_of_context_used': 48, 'llm_response': '```json\\n[\\n  {\\n    \"title\": \"Sparse Transformers\",\\n    \"summary\": \"Sparse Transformers address the computational cost of standard Transformers by only attending to a subset of input tokens, improving efficiency.\",\\n    \"overview\": \"This significantly reduces memory requirements and speeds up training, making Transformers more viable for high-resolution images.\"\\n  },\\n  {\\n    \"title\": \"Longformer\",\\n    \"summary\": \"Longformer utilizes a combination of sliding window, global, and dilated attention mechanisms to handle longer sequences effectively.\",\\n    \"overview\": \"This allows for processing of entire images, capturing long-range dependencies and improving performance on tasks like visual question answering.\"\\n  },\\n  {\\n    \"title\": \"PVT (Pyramid Vision Transformer)\",\\n    \"summary\": \"PVT employs a hierarchical structure with multi-scale self-attention to capture both local and global image features.\",\\n    \"overview\": \"This approach achieves state-of-the-art performance on image classification benchmarks, demonstrating the effectiveness of multi-scale vision Transformers.\"\\n  },\\n  {\\n    \"title\": \"DeiT (Data-efficient Image Transformers)\",\\n    \"summary\": \"DeiT introduces stochastic depth and knowledge distillation to train Vision Transformers with comparable performance to convolutional networks, but with significantly less data.\",\\n    \"overview\": \"This highlights the potential of Transformers to learn effectively with smaller datasets, potentially reducing the need for massive labeled datasets.\"\\n  }\\n]\\n```'}\n",
      "{'query': 'Summarize recent advances in vision transformers', 'length_of_context_used': 48, 'llm_response': '```json\\n[\\n  {\\n    \"title\": \"Efficient Transformers\",\\n    \"summary\": \"Research focuses on reducing the computational demands of ViTs through techniques like sparse attention, low-rank approximations, and knowledge distillation.\",\\n    \"overview\": \"These advancements enable ViTs to be deployed on resource-constrained devices and scale to higher resolutions, improving their practicality.\"\\n  },\\n  {\\n    \"title\": \"Hybrid Architectures\",\\n    \"summary\": \"Combining ViTs with convolutional neural networks (CNNs) is gaining traction, leveraging the strengths of both architectures.\",\\n    \"overview\": \"Hybrid approaches often improve accuracy and efficiency, particularly in tasks requiring both local and global context understanding.\"\\n  },\\n  {\\n    \"title\": \"Multi-Stage Transformers\",\\n    \"summary\": \"Architectures dividing the image into multiple stages, processing it hierarchically, mimicking human visual processing.\",\\n    \"overview\": \"This approach enhances the ability of ViTs to capture long-range dependencies and effectively represent complex visual scenes.\"\\n  },\\n  {\\n    \"title\": \"Self-Supervised Learning\",\\n    \"summary\": \"ViTs are increasingly pre-trained effectively with self-supervised learning techniques like masked image modeling and contrastive learning.\",\\n    \"overview\": \"Pre-training significantly improves ViT performance on downstream tasks, reducing the reliance on large labeled datasets.\"\\n  }\\n]\\n```'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # MCP server URL\n",
    "    BASE_URL = \"http://127.0.0.1:8080\"\n",
    "    \n",
    "    # url = f\"{BASE_URL}/analyze_with_llm\"    \n",
    "    # ðŸ” Example query\n",
    "    query = \"Summarize recent advances in vision transformers\"\n",
    "    context_query = query or None\n",
    "    params = {\"query\": query, \"context_query\": context_query}\n",
    "    url = f\"{BASE_URL}/fetch_all_sources\"\n",
    "    response = requests.get(url, params=params)\n",
    "    contexts = response.json()\n",
    "\n",
    "    for source in contexts:\n",
    "        context_query = contexts[source]\n",
    "        url = f\"{BASE_URL}/process_with_llm\"\n",
    "        response = requests.post(url, params=params)\n",
    "        print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b9f4cf4-b94a-47c0-9d37-02e5c31fe535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Summarize recent advances in vision transformers',\n",
       " 'length_of_context_used': 48,\n",
       " 'llm_response': '```json\\n[\\n  {\\n    \"title\": \"Efficient Transformers\",\\n    \"summary\": \"Research focuses on reducing the computational demands of ViTs through techniques like sparse attention, low-rank approximations, and knowledge distillation.\",\\n    \"overview\": \"These advancements enable ViTs to be deployed on resource-constrained devices and scale to higher resolutions, improving their practicality.\"\\n  },\\n  {\\n    \"title\": \"Hybrid Architectures\",\\n    \"summary\": \"Combining ViTs with convolutional neural networks (CNNs) is gaining traction, leveraging the strengths of both architectures.\",\\n    \"overview\": \"Hybrid approaches often improve accuracy and efficiency, particularly in tasks requiring both local and global context understanding.\"\\n  },\\n  {\\n    \"title\": \"Multi-Stage Transformers\",\\n    \"summary\": \"Architectures dividing the image into multiple stages, processing it hierarchically, mimicking human visual processing.\",\\n    \"overview\": \"This approach enhances the ability of ViTs to capture long-range dependencies and effectively represent complex visual scenes.\"\\n  },\\n  {\\n    \"title\": \"Self-Supervised Learning\",\\n    \"summary\": \"ViTs are increasingly pre-trained effectively with self-supervised learning techniques like masked image modeling and contrastive learning.\",\\n    \"overview\": \"Pre-training significantly improves ViT performance on downstream tasks, reducing the reliance on large labeled datasets.\"\\n  }\\n]\\n```'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9447daca-8166-42c4-b06b-a6e2fa479726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Efficient Transformers',\n",
       "  'summary': 'Research focuses on reducing the computational demands of ViTs through techniques like sparse attention, low-rank approximations, and knowledge distillation.',\n",
       "  'overview': 'These advancements enable ViTs to be deployed on resource-constrained devices and scale to higher resolutions, improving their practicality.'},\n",
       " {'title': 'Hybrid Architectures',\n",
       "  'summary': 'Combining ViTs with convolutional neural networks (CNNs) is gaining traction, leveraging the strengths of both architectures.',\n",
       "  'overview': 'Hybrid approaches often improve accuracy and efficiency, particularly in tasks requiring both local and global context understanding.'},\n",
       " {'title': 'Multi-Stage Transformers',\n",
       "  'summary': 'Architectures dividing the image into multiple stages, processing it hierarchically, mimicking human visual processing.',\n",
       "  'overview': 'This approach enhances the ability of ViTs to capture long-range dependencies and effectively represent complex visual scenes.'},\n",
       " {'title': 'Self-Supervised Learning',\n",
       "  'summary': 'ViTs are increasingly pre-trained effectively with self-supervised learning techniques like masked image modeling and contrastive learning.',\n",
       "  'overview': 'Pre-training significantly improves ViT performance on downstream tasks, reducing the reliance on large labeled datasets.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonify_llm_response(response.json()['llm_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae5b26c-86f6-4f8a-a82d-743e62ac1e63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbfb3589-5e64-4ced-8a94-09fda18981df",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = ''\n",
    "context = context_query['arxiv']\n",
    "prompt = f\"Use the following context to answer concisely. \\n Include the follwing in answer as a json for each item in list: title: title, summary: brief summary, overview: brief overview on its impact. \\n Question: {user_query}\\nContext: {context}\"\n",
    "llm_response = llm.invoke(prompt)\n",
    "jsonify_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3db830-61ac-4b19-94f9-e9981acbc9b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ce0ffde-899c-420e-85aa-2185255b493f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arxiv\n",
      "openreview\n",
      "google_research\n",
      "meta_fair\n"
     ]
    }
   ],
   "source": [
    "from utils.send_feed import send_discord\n",
    "\n",
    "discord_web_hook = \"https://discord.com/api/webhooks/1435121885187670088/-QO-bAi_EsWGo8usBFWnO-QBGjp8Qx6ICZp5OvtiUcMGGUC7r_iYn6RtPTt94yHGtkPi\"\n",
    "for source in context_query:\n",
    "    print(source)\n",
    "    send_discord(context_query[source], source, discord_web_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bf2594-75a0-4095-87eb-a881cbdf0824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c962fa0e-1f1a-4232-8b89-a8b83d4c7103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from utils.render_templates import render_discord, render_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce07fa0c-9e68-4aa2-9583-ae517ab74899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [400]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload = render_discord(context_query[source], source)\n",
    "requests.post(discord_web_hook, json=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6eb645-8ddc-4c5d-a391-9f203731c1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1136889-c007-41dd-9cd7-0d28e908fcb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46dcc78b-b54b-43bc-acb5-f472e0b7b980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [204]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "data = {\n",
    "    \"content\": \"ðŸš€ Hello from your AI Research Bot!\\nNew updates are available ðŸŽ‰\"\n",
    "}\n",
    "requests.post(discord_web_hook, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94e45452-37df-4625-aee2-9cf462747300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'LifWavNet: Lifting Wavelet-based Network for Non-contact ECG\\n  Reconstruction from Radar',\n",
       "  'summary': 'Non-contact electrocardiogram (ECG) reconstruction from radar signals offers\\na promising approach for unobtrusive cardiac monitoring. We present LifWavNet,\\na lifting wavelet network based on a multi-resolution analysis and synthesis\\n(MRAS) model for radar-to-ECG reconstruction. Unlike prior models that use\\nfixed wavelet approaches, LifWavNet employs learnable lifting wavelets with\\nlifting and inverse lifting units to adaptively capture radar signal features\\nand synthesize physiologically meaningful ECG waveforms. To improve\\nreconstruction fidelity, we introduce a multi-resolution short-time Fourier\\ntransform (STFT) loss, that enforces consistency with the ground-truth ECG in\\nboth temporal and spectral domains. Evaluations on two public datasets\\ndemonstrate that LifWavNet outperforms state-of-the-art methods in ECG\\nreconstruction and downstream vital sign estimation (heart rate and heart rate\\nvariability). Furthermore, intermediate feature visualization highlights the\\ninterpretability of multi-resolution decomposition and synthesis in\\nradar-to-ECG reconstruction. These results establish LifWavNet as a robust\\nframework for radar-based non-contact ECG measurement.',\n",
       "  'link': 'http://arxiv.org/abs/2510.27692v1',\n",
       "  'published': '2025-10-31T17:59:58+00:00'},\n",
       " {'title': 'Vision Transformer for Robust Occluded Person Reidentification in\\n  Complex Surveillance Scenes',\n",
       "  'summary': 'Person re-identification (ReID) in surveillance is challenged by occlusion,\\nviewpoint distortion, and poor image quality. Most existing methods rely on\\ncomplex modules or perform well only on clear frontal images. We propose Sh-ViT\\n(Shuffling Vision Transformer), a lightweight and robust model for occluded\\nperson ReID. Built on ViT-Base, Sh-ViT introduces three components: First, a\\nShuffle module in the final Transformer layer to break spatial correlations and\\nenhance robustness to occlusion and blur; Second, scenario-adapted augmentation\\n(geometric transforms, erasing, blur, and color adjustment) to simulate\\nsurveillance conditions; Third, DeiT-based knowledge distillation to improve\\nlearning with limited labels.To support real-world evaluation, we construct the\\nMyTT dataset, containing over 10,000 pedestrians and 30,000+ images from base\\nstation inspections, with frequent equipment occlusion and camera variations.\\nExperiments show that Sh-ViT achieves 83.2% Rank-1 and 80.1% mAP on MyTT,\\noutperforming CNN and ViT baselines, and 94.6% Rank-1 and 87.5% mAP on\\nMarket1501, surpassing state-of-the-art methods.In summary, Sh-ViT improves\\nrobustness to occlusion and blur without external modules, offering a practical\\nsolution for surveillance-based personnel monitoring.',\n",
       "  'link': 'http://arxiv.org/abs/2510.27677v1',\n",
       "  'published': '2025-10-31T17:43:50+00:00'},\n",
       " {'title': 'NegoCollab: A Common Representation Negotiation Approach for\\n  Heterogeneous Collaborative Perception',\n",
       "  'summary': \"Collaborative perception improves task performance by expanding the\\nperception range through information sharing among agents. . Immutable\\nheterogeneity poses a significant challenge in collaborative perception, as\\nparticipating agents may employ different and fixed perception models. This\\nleads to domain gaps in the intermediate features shared among agents,\\nconsequently degrading collaborative performance. Aligning the features of all\\nagents to a common representation can eliminate domain gaps with low training\\ncost. However, in existing methods, the common representation is designated as\\nthe representation of a specific agent, making it difficult for agents with\\nsignificant domain discrepancies from this specific agent to achieve proper\\nalignment. This paper proposes NegoCollab, a heterogeneous collaboration method\\nbased on the negotiated common representation. It introduces a negotiator\\nduring training to derive the common representation from the local\\nrepresentations of each modality's agent, effectively reducing the inherent\\ndomain gap with the various local representations. In NegoCollab, the mutual\\ntransformation of features between the local representation space and the\\ncommon representation space is achieved by a pair of sender and receiver. To\\nbetter align local representations to the common representation containing\\nmultimodal information, we introduce structural alignment loss and pragmatic\\nalignment loss in addition to the distribution alignment loss to supervise the\\ntraining. This enables the knowledge in the common representation to be fully\\ndistilled into the sender.\",\n",
       "  'link': 'http://arxiv.org/abs/2510.27647v1',\n",
       "  'published': '2025-10-31T17:20:54+00:00'},\n",
       " {'title': 'SpecAttn: Speculating Sparse Attention',\n",
       "  'summary': 'Large Language Models (LLMs) face significant computational bottlenecks\\nduring inference due to the quadratic complexity of self-attention mechanisms,\\nparticularly as context lengths increase. We introduce SpecAttn, a novel\\ntraining-free approach that seamlessly integrates with existing speculative\\ndecoding techniques to enable efficient sparse attention in pre-trained\\ntransformers. Our key insight is to exploit the attention weights already\\ncomputed by the draft model during speculative decoding to identify important\\ntokens for the target model, eliminating redundant computation while\\nmaintaining output quality. SpecAttn employs three core techniques: KL\\ndivergence-based layer alignment between draft and target models, a\\nGPU-optimized sorting-free algorithm for top-p token selection from draft\\nattention patterns, and dynamic key-value cache pruning guided by these\\npredictions. By leveraging the computational work already performed in standard\\nspeculative decoding pipelines, SpecAttn achieves over 75% reduction in\\nkey-value cache accesses with a mere 15.29% increase in perplexity on the PG-19\\ndataset, significantly outperforming existing sparse attention methods. Our\\napproach demonstrates that speculative execution can be enhanced to provide\\napproximate verification without significant performance degradation.',\n",
       "  'link': 'http://arxiv.org/abs/2510.27641v1',\n",
       "  'published': '2025-10-31T17:12:34+00:00'},\n",
       " {'title': 'Sketch-to-Layout: Sketch-Guided Multimodal Layout Generation',\n",
       "  'summary': 'Graphic layout generation is a growing research area focusing on generating\\naesthetically pleasing layouts ranging from poster designs to documents. While\\nrecent research has explored ways to incorporate user constraints to guide the\\nlayout generation, these constraints often require complex specifications which\\nreduce usability. We introduce an innovative approach exploiting user-provided\\nsketches as intuitive constraints and we demonstrate empirically the\\neffectiveness of this new guidance method, establishing the sketch-to-layout\\nproblem as a promising research direction, which is currently under-explored.\\nTo tackle the sketch-to-layout problem, we propose a multimodal\\ntransformer-based solution using the sketch and the content assets as inputs to\\nproduce high quality layouts. Since collecting sketch training data from human\\nannotators to train our model is very costly, we introduce a novel and\\nefficient method to synthetically generate training sketches at scale. We train\\nand evaluate our model on three publicly available datasets: PubLayNet,\\nDocLayNet and SlidesVQA, demonstrating that it outperforms state-of-the-art\\nconstraint-based methods, while offering a more intuitive design experience. In\\norder to facilitate future sketch-to-layout research, we release O(200k)\\nsynthetically-generated sketches for the public datasets above. The datasets\\nare available at https://github.com/google-deepmind/sketch_to_layout.',\n",
       "  'link': 'http://arxiv.org/abs/2510.27632v1',\n",
       "  'published': '2025-10-31T17:05:10+00:00'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = \"arxiv\"\n",
    "context_query[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4222e1d3-7333-4b36-abd2-4e460082adff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af7f05-d744-403b-a20f-ba3fb181e349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8151730-b05c-427a-a697-3bbe12e9d8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b07d8b2-4690-4830-b449-171e349013e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyMuPDFLoader(path_to_pdf, mode='single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75bde63e-ecfd-43ef-ab6c-b2c3ea35559d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark', 'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': ''}, page_content='Are Video Models Ready as Zero-Shot Reasoners?\\nAn Empirical Study with the MME-COF Benchmark\\nZiyu Guoâˆ—â€ 1, Xinyan Chenâˆ—2, Renrui Zhangâˆ—â€¡2, Ruichuan Anâˆ—3, Yu Qiâˆ—4, Dongzhi Jiang2\\nXiangtai Li3, Manyuan Zhang2, Hongsheng Li2, Pheng-Ann Heng1\\nCUHK 1IMIXR & 2MMLab\\n3Peking University\\n4Northeastern University\\nâˆ—Equal Contribution\\nâ€ Project Lead\\nâ€¡Corresponding Author\\nProject Page: https://video-cof.github.io\\nAbstract\\nRecent video generation models can produce high-fidelity, temporally coherent\\nvideos, indicating that they may encode substantial world knowledge. Beyond real-\\nistic synthesis, they also exhibit emerging behaviors indicative of visual perception,\\nmodeling, and manipulation [70]. Yet, an important question still remains: Are\\nvideo models ready to serve as zero-shot reasoners in challenging visual reasoning\\nscenarios? In this work, we conduct an empirical study to comprehensively\\ninvestigate this question, focusing on the leading and popular Veo-3 [21]. We\\nevaluate its reasoning behavior across 12 dimensions, including spatial, geometric,\\nphysical, temporal, and embodied logic, systematically characterizing both its\\nstrengths and failure modes. To standardize this study, we curate the evaluation\\ndata into MME-COF, a compact benchmark that enables in-depth and thorough\\nassessment of Chain-of-Frame (CoF) reasoning. Our findings reveal that while\\ncurrent video models demonstrate promising reasoning patterns on short-horizon\\nspatial coherence, fine-grained grounding, and locally consistent dynamics, they\\nremain limited in long-horizon causal reasoning, strict geometric constraints, and\\nabstract logic. Overall, they are not yet reliable as standalone zero-shot reasoners,\\nbut exhibit encouraging signs as complementary visual engines alongside dedicated\\nreasoning models.\\n1\\nIntroduction\\nVideo models [21, 63, 55, 81, 11], including text-to-video and video-to-text generation models, have\\nmade rapid progress in recent years. Thanks to advances in diffusion [75, 7, 84] and autoregressive [36,\\n76, 16] architectures, current video models can produce high-fidelity videos maintaining consistent\\nobject relations and realistic motion dynamics across frames. This suggests that the models may\\nhave internalized substantial visual and structural knowledge about the world. Recent research from\\nGoogle [70] further hints that, such models are evolving beyond pure content generation: Veo-3 [21]\\nhas been shown to perform dozens of distinct vision tasks across perception, modeling, manipulation,\\nand reasoning, without any task-specific training. These emergent capabilities have led researchers to\\nposit that video models could serve as unified, generalist vision models, much like large language\\nmodels (LLMs) [1, 13, 3, 30] have become foundation models for natural language.\\nCrucially, the sequential nature of video generation provides a new perspective on how such models\\nmight reason. Each generated frame builds upon the last, creating a temporal chain of information\\npropagation. This has been dubbed â€œChain-of-Frameâ€ (CoF) reasoning [70], an analogy to the chain-\\nof-thought (CoT) process in LLMs [69, 35, 82, 23, 79] and their multi-modal variants (MLLMs) [12,\\n4, 40, 31, 10]. In essence, as a video model generates a sequence of frames, it can iteratively refine\\nand update the scene, thereby working through a problem step-by-step in time and space. This CoF\\nconcept suggests that, beyond surface-level pattern generation, general-purpose visual reasoning may\\nemerge from video generative models.\\narXiv:2510.26802v1  [cs.CV]  30 Oct 2025\\n\\x0cEmbodied\\nReasoning\\n3D Geometry\\nReasoning\\nReal-world Spatial\\nReasoning\\nPhysics-based\\nReasoning\\n2D Geometry\\nReasoning\\nTable and Chart\\nReasoning\\nObject Counting\\nReasoning\\nRotation\\nReasoning\\nVisual Detail\\nReasoning\\nVisual Trace\\nReasoning\\nGUI\\nReasoning\\nMedical\\nReasoning\\nVeo\\nSora\\nSeedance\\nVideo Models\\n...\\nZero-shot Reasoning?\\nKling\\nFigure 1: Overview of Our Study on the Reasoning Potential of Video Models. We investigate\\nwhether state-of-the-art video models exhibit emergent reasoning potentials beyond content synthesis.\\nThe analysis spans 12 reasoning dimensions under a unified perspective, exploring whether large-scale\\nvideo models can serve as zero-shot visual reasoners via CoF reasoning.\\nHowever, it remains unclear to what extent current video models truly exhibit reasoning about the\\ncontent they create. Strong generative performance does not automatically imply robust reasoning\\npotential. Emerging evidence [22, 47, 5, 78] shows that a model may produce coherent videos by\\nlearning surface-level patterns in the training data, rather than by internalizing general principles. For\\ninstance, a video model can maintain object continuity yet fail to grasp physical plausibility across\\na long sequence, or it may mimic observed visual sequences without understanding the underlying\\ncause-and-effect relationships. This motivates our central question: Are video models, purely through\\nlarge-scale visual learning, obtain the zero-shot reasoning potential?\\nTo this end, we present the first empirical study to systematically probe the CoF reasoning capabili-\\nties of modern video models, spanning 12 dimensions such as spatial, geometric, physical, temporal,\\nand embodied logic, as detailed in 1. We carry out our analysis on Veo-3, which has been system-\\natically examined as a zero-shot learner in prior work [70]. Our preliminary observations suggest\\nthat current leading video models exhibit comparable reasoning patterns, making Veo-3 a represen-\\ntative choice. Our analysis builds on reasoning scenarios distilled from diverse reasoning-oriented\\nbenchmarks [25, 67, 45, 71, 29, 34], as well as those we design ourselves, providing a compact yet\\nexpressive foundation. The prompts for video models are meticulously crafted by transforming the\\nunderlying, textual reasoning process of problem-solving into a clear, video-presentation format.\\nEach case receives a qualitative assessment across three performance levels, i.e., good, moderate, and\\nbad, complemented by a quantitative success rate to measure robustness.\\nTo standardize evaluation, we curate these tasks into the MME-COF benchmark, as illustrated in\\nFigure 2 and Section 3.2. Leveraging this benchmark, we measure several state-of-the-art video\\nmodels, i.e., Veo-3 [21], Sora-2 [56], Kling [38], and Seedance [19], to obtain directly comparable\\nscores and qualitative behaviors across categories. Our investigation reveals that the models exhibit\\npromising reasoning patterns in short-horizon spatial coherence, fine-grained grounding, and con-\\n2\\n\\x0cMedical\\nReasoning\\nEmbodied\\nReasoning\\nObject Counting\\nReasoning\\nGUI\\nReasoning\\nTable & Chart\\nReasoning\\nRotation\\nReasoning\\n3D Geometry\\nReasoning\\nReal-world Spatial\\nReasoning\\nVisual Trace\\nReasoning \\nVisual Detail\\nReasoning \\n2D Geometry\\nReasoning\\nPhysics-based\\nReasoning\\nKling-v1\\nSeedance-1.0-pro\\nSora-2-pro\\nSora-2\\nVeo-3-fast\\nVeo-3-preview\\n(a) Evaluation Radar Map.\\n(b) Word Cloud.\\nFigure 2: Illustration of the MME-COF Benchmark. It showcases that different models specialize\\nin distinct reasoning aspects, but most models exhibit limited reasoning capability across all tasks.\\nsistent local dynamics; however, they struggle with complex reasoning conditions, particularly in\\nlong-horizon causal consistency, geometric constraint adherence, and abstract logic. Overall, current\\nvideo models are not yet ready as standalone zero-shot reasoners. Still, they show encouraging signs\\nof emergent reasoning, suggesting strong potential as complementary reasoning agents alongside\\nspecialized models.\\nOur main contributions are summarized as follows:\\nâ€¢ A Comprehensive Empirical Study. We provide the first investigation of video models\\n(Veo-3) to analyze their visual reasoning potential, detailing representative successes, char-\\nacteristic errors, and the conditions under which CoF reasoning emerges, holds, or breaks.\\nâ€¢ The MME-COF Benchmark. We curate MME-COF, a compact benchmark providing a\\nstandardized taxonomy and an evaluation protocol aligned with CoF reasoning, enabling\\nconsistent and category-wise assessment beyond surface-level visual fidelity.\\nâ€¢ Insights and Directions. We summarize common success patterns (e.g., short-horizon\\ncoherence and stable spatial layout) and failure patterns (e.g., long-horizon degradation,\\nviolations of basic geometry/physics, and temporal logic), making clear when the behavior\\nreflects genuine reasoning versus pattern replay.\\n2\\nDeep-Dive Analysis on Veo-3\\n2.1\\nOverview\\nTo ensure a rigorous empirical study, we detail our core methodology in this section, including the\\ntaxonomy of reasoning tasks, test case curation process, the standardized style for prompt design,\\nand the analysis setup.\\nTask Taxonomy.\\nTo capture different dimensions of reasoning, our study starts from dozens of\\nreasoning-oriented tasks, which can be organized into the following 12 categories:\\n3\\n\\x0c1) Visual Detail Reasoning\\n2) Visual Trace Reasoning\\n3) Real-world Spatial Reasoning\\n4) 3D Geometry Reasoning\\n5) 2D Geometry Reasoning\\n6) Physics-based Reasoning\\n7) Rotation Reasoning\\n8) Table and Chart Reasoning\\n9) Object Counting Reasoning\\n10) GUI Reasoning\\n11) Embodied Reasoning\\n12) Medical Reasoning\\nEach category comprises several representative cases selected to test specific aspects of reasoning.\\nTest Case Curation.\\nWe recruit five PhD-level experts with deep expertise in text-image reasoning,\\nwho are tasked with selecting representative cases from benchmarks [25, 67, 45, 52, 74] corresponding\\nto each task category. For each reasoning case, the experts manually constructed text prompts that\\nexplicitly or unambiguously define the target reasoning objective, aiming to evaluate the potential of\\nvideo models for multi-modal reasoning.\\nPrompt Design Style.\\nTo ensure consistency and fairness, all prompts follow a unified style\\nemphasizing explicit visual constraints, controlled motion, and minimal linguistic ambiguity. Prompts\\nare encouraged to be written in imperative form and designed to reduce variance from language\\ninterpretation, focusing the modelâ€™s behavior on the intended visual reasoning objective. The overall\\ndesign principles are as follows:\\n1) Static camera and fixed viewpoint, unless motion is explicitly required by the task.\\n2) Stable spatial composition, consistent framing, and unchanging scene layout across\\nframes.\\n3) Clear specification of allowed and disallowed changes (e.g., â€œno zoom, no pan, no dollyâ€)\\nto constrain camera dynamics.\\n4) Explicit temporal phrasing to control the pace of motion, using cues such as â€œinstantlyâ€,\\nâ€œsmoothlyâ€, or â€œstep-by-stepâ€.\\n5) Avoidance of direct textual hints toward the answer; instructions are purely visual and\\ntask-oriented.\\n6) Inclusion of realistic phrasing and scene context to align with the modelâ€™s natural video\\npriors while minimizing artifacts.\\nThe standardized prompt style ensures that differences in output primarily reflect the modelâ€™s internal\\nreasoning potential rather than prompt variability.\\nAnalysis Setup.\\nFor every reasoning case, we construct a text prompt that explicitly or implicitly\\nspecifies the target reasoning objective. Each prompt produces six video samples at a resolution of\\n1280Ã—720, 24 FPS, and a duration of 8 seconds. All experiments are conducted in a unified zero-shot\\nsetup without fine-tuning, additional supervision, or auxiliary tools.\\nWe evaluate model outputs through qualitative judgments along three levels of performance, i.e.,\\nGood , Moderate , and Bad, based on the clarity, correctness, and temporal stability of the visual\\nreasoning process. Detailed definitions and examples of these evaluation criteria are provided in the\\ncorresponding task subsections. Note that, since we observe that most video models struggle to follow\\nthe requirement of â€˜static shotâ€™ reliably, we apply more permissive qualitative criteria for static-shot\\nevaluations. We further define a success rate to measure robustness across generations for each case,\\ncomputed as the proportion of successful samples among the six generated. For cases categorized as\\nBad, the success rate is always 0. Non-zero success rates only appear in cases evaluated as Good or\\nModerate , indicating that Veo-3 exhibits some potential to perform as a visual reasoner. A higher\\nsuccess rate reflects a more stable reasoning capability of the model.\\n4\\n\\x0cInput Image:\\nReasoning Video:\\nZoom in on the black bag with the Apple logo to focus on the \\nlogo\\'s color. Static shot.\\nI. Question:\\nText-to-Video Prompt:\\nQ: What is the color of the Apple logo?\\n1st frame\\nInput Image:\\nReasoning Video:\\nII. Question:\\nText-to-Video Prompt:\\n1st frame\\nIII. Question:\\nText-to-Video Prompt:\\nA: The color of the Apple logo is polychromatic.\\nIV. Question:\\n~ Moderate\\nSuccess Rate: 17%\\nGradually zoom in on the group of people walking along the path, centering \\non the person carrying the handbag. Keep the surrounding park and benches \\nsoftly blurred to emphasize the handbagâ€™s color. Static shot.\\nQ: What is the color of the handbag? \\nA: The color of the handbag is white.\\nâœ“ Good\\nSuccess Rate: 33%\\nInput Image:\\nReasoning Video:\\n1st frame\\nSmoothly zoom in on the dog near the lower right corner of the \\nscene, then highlight the motorcycle parked near it. Keep the \\nsurrounding jeeps and people slightly blurred to emphasize \\nspatial relation. Static shot. \\nQ: Is the motorcycle on the left or right side of \\nthe dog?\\nA: The motorcycle is on the left side of the dog.\\nâœ“ Good\\nSuccess Rate: 83%\\nText-to-Video Prompt:\\nInput Image:\\nReasoning Video:\\n1st frame\\nGradually zoom in on the area near the cone along the pathway, \\ncentering both the cone and the baby carriage in the frame. Keep \\nthe surrounding trees and grass softly blurred to emphasize these \\ntwo objects. Static shot.\\nQ: Is the baby carriage on the left or right side \\nof the cone?\\nA: The baby carriage is on the right side of the \\ncone.\\nâœ— Bad\\nFigure 3: Showcase of Visual Detail Reasoning by Veo-3. It illustrates Veo-3â€™s ability to localize\\ntargets and maintain fine-grained visual attributes across frames, together with common failure modes\\nwhen targets are small, occluded, or embedded in clutter.\\n5\\n\\x0c2.2\\nVisual Detail Reasoning\\nTask Description and Evaluated Aspects.\\nIn the visual detail reasoning category, the objective is\\nto assess a modelâ€™s ability to discern and maintain fine-grained visual attributes and spatial relations\\nwithin generated video sequences. It covers attribute recognition, e.g., identifying color, texture or\\nmaterial of an object, and spatial relation identification, e.g., recognizing that one object is on the\\nleft of or behind another object. The model is evaluated on the capacity both to attend to the correct\\ntarget region and to maintain visual consistency, across frames, of the attribute or relation in question.\\nDefinition of Good / Moderate / Bad.\\nWe define the three-level evaluation criteria as follows:\\nâœ“Good: The reasoning video accurately centers on the correct target region, clearly resolves\\nthe relevant attribute, such as color, texture or position, and maintains sharp, stable and natural\\nrendering throughout the sequence. There are no visible frame drops, artifacts or unintended\\nmotion.\\n~ Moderate: The region of interest is approximately correct, and the attribute remains\\ninferable, but the sequence suffers from minor blur, incomplete framing, slight instability\\nmild unnatural motion, or sometimes deviates from the textual instruction and produces a\\nplausible but unaligned or self-directed visual interpretation, limiting confident interpretation.\\nâœ—Bad: The target region is incorrect or ambiguous, the attribute cannot be reliably inferred,\\nor the video exhibits severe artifacts: abrupt frame jumps, major jitter, unintended zoom or\\ncrop, extraneous objects interfering, or conspicuous quality degradation that obstructs the\\nreasoning task altogether.\\nData Source.\\nWe sample data from the Vâˆ—Bench [71], which provides a comprehensive set of\\nevaluation dimensions including spatial relationship and color/attribute consistency tasks.\\nExample and Analysis.\\nWe illustrate typical behaviors of Veo-3 in visual detail reasoning through\\nfour representative cases in Figure 3. In case I, the model performs well in localizing the target:\\nalthough it does not strictly execute the â€œzoom inâ€ instruction, it instead achieves an equivalent\\nvisual outcome through a semantically consistent motion with a personâ€™s hand. This slight deviation\\nsuggests that the model may exhibit certain generation preferences in how it interprets and realizes\\nspatial instructions, possibly reflecting stylistic tendencies learned from training data. In cases II\\nand III, the model achieves better success rates when the targets are visually salient and contextually\\ndistinct. For the handbag and dog-motorcycle scenes, Veo-3 attends to the correct regions and\\nmaintains smooth temporal coherence. However, when the object (e.g., the motorcycle) is small or\\nsurrounded by distracting elements, the model occasionally fails to locate it accurately, indicating\\nlimited fine-grained spatial discrimination in cluttered scenes. In case IV, when the target object is\\ntiny and visually indistinct, Veo-3 cannot identify it even with explicit positional hints, highlighting\\nthat the modelâ€™s perceptual grounding and reasoning weaken sharply when object size and salience\\nare too low for reliable attention.\\nTakeaway 1\\nVeo-3 performs well in fine-grained attribute and spatial reasoning for salient, well-grounded\\ntargets, but fails when objects are small, occluded, or cluttered. It sometimes exhibits stylistic\\ngeneration biases that lead to plausible yet instruction-divergent outcomes.\\n2.3\\nVisual Trace Reasoning\\nTask Description and Evaluated Aspects.\\nThe visual trace reasoning category evaluates a modelâ€™s\\nability to represent and maintain causal continuity across sequential actions. Typical tasks include\\nmaze navigation, path following, and multi-step object manipulation, where the video must visually\\nencode a coherent sequence of intermediate decisions that lead to the correct goal. Performance is\\nassessed based on two major aspects: (i) temporal coherence, which is the smoothness and logical\\n6\\n\\x0cInput Image:\\nReasoning Video:\\nStarting at the red dot in the middle-right cell, animate step-by-\\nstep moves: go down 1 cell, left 1, left 1, up 1, and up 1,\\ndrawing arrows for each step and finishing with a glow around\\nthe final cell. Static shot.\\nI. Question:\\nText-to-Video Prompt:\\nQ: Starting from the red dot, follow the given\\nmovement instructions and determine the final\\nposition. Down 1, left 1, left 1, up 1, up 1.\\nA: A\\n1st frame\\nInput Image:\\nReasoning Video:\\nII. Questionâ€ :\\nText-to-Video Prompt:\\n1st frame\\nIII. Questionâ€ :\\nText-to-Video Prompt:\\nIV. Question:\\nAnimate the elf moving step by step toward the gift while carefully\\navoiding the icy frozen lake. Highlight the successful path and end\\nwith the elf standing beside the gift. Static shot.\\nQ: The character must avoid falling into the\\nfrozen lake and reach the gift pack safely.\\nInput Image:\\nReasoning Video:\\n1st frame\\nAnimate the red triangle moving step by step toward the white printer,\\npicking it up once it reaches it. Then have the triangle carry the printer\\nupward and place it on the brown area representing the table. End with a\\nsubtle highlight around the printer to show it is toggled on. Static shot.\\nQ: Move the character (red triangle)\\nto pick up the white printer and\\nplace it anywhere on the desk.\\nText-to-Video Prompt:\\nInput Image:\\nReasoning Video:\\n1st frame\\nAnimate a bright path tracing from the blue point at the\\ntop through the mazeâ€™s open corridors toward the red\\npoint at the bottom, highlighting each green numbered\\nmark it passes. Keep the maze and all walls fixed while\\nthe glowing path moves smoothly through the correct\\nroute. Static shot.\\nQ: The given picture is a maze, and the black lines\\nrepresent walls that cannot be walked. Now you want to\\nwalk from the blue point to the red point. Is there a\\nfeasible path? If so, which of the green marks numbered\\n1-5 In the picture must be passed in the path?\\nA: Yes, 3.\\nâœ—Bad\\nâœ“Good\\nSuccess Rate: 17%\\nâœ—Bad\\nâœ—Bad\\nFigure 4: Showcase of Visual Trace Reasoning by Veo-3 (Part I). It shows short-horizon path-\\nfollowing successes, object-grounding failures, and a certain bias that causes step omissions/mistakes\\nin multi-step traces. â€  The ground-truth answers of cases II and III are intuitive and non-unique,\\nwhich are omitted to highlight the key reasoning behaviors.\\n7\\n\\x0cInput Image:\\nReasoning Video:\\nCreate a 2D animation based on the provided diagram. The red arrow is\\nthe initial arrow, and the green arrow is the final arrow. The arrow can\\nmove in four directions (forward, backward, left, right), where \\'forward\\'\\nalways refers to the current direction the arrow is pointing. After each\\nmovement, the arrow\\'s direction is updated to the direction of movement.\\nMovement commands:\\n- The red arrow moves forward for 1 unit.\\n- The red arrow moves left for 1 unit (relative to its new current direction\\nafter step 1). Then turns green.\\nScene:\\n- No change in scene composition.\\n- No change in the layout of the diagram.\\nCamera: Static camera. No zoom. No pan. No glitches, noise, or artifacts.\\nV. Question:\\nText-to-Video Prompt:\\nQ: In the diagram, the red arrow is\\nthe initial arrow, and the green arrow\\nis the final arrow. The arrow can\\nmove in four directions (forward,\\nbackward,\\nleft,\\nright),\\nwhere\\n\\'forward\\' always refers to the current\\ndirection the arrow is pointing. After\\neach\\nmovement,\\nthe\\narrow\\'s\\ndirection is updated to the direction\\nof\\nmovement.\\nWhich\\nof\\nthe\\nfollowing paths can make the arrow\\nmove from the starting position to\\nthe ending position?\\nA: (Forward, 1 unit) - (Left, 1 unit)\\n1st frame\\nInput Image:\\nReasoning Video:\\nVI. Question:\\nText-to-Video Prompt:\\n1st frame\\nTwo small characters start from the same purple origin at the same\\ntime, and move along the red and green paths toward another purple\\ndestination at the same speed. Static camera, no zoom, no pan.\\nQ: What are the advantages of the green\\nroute and the red route respectively?\\nâœ—Bad\\nâœ—Bad\\nFigure 5: Showcase of Visual Trace Reasoning (Part II) by Veo-3. The examples highlight\\nlong-horizon planning breakdowns, inconsistent arrow/trajectory rendering, and failures to preserve\\ncomparative or sequential information across frames.\\nprogression between consecutive steps; and (ii) goal consistency, which means whether the full\\nsequence visually completes the intended reasoning trajectory without deviation or contradiction.\\nDefinition of Good / Moderate / Bad.\\nWe rate the performance according to the following criteria:\\nâœ“Good: Each movement step is depicted continuously and logically toward the correct goal.\\nThe motion is smooth, temporally consistent, and follows causal order with no skipping,\\nstuttering, or direction reversal.\\n~ Moderate: The overall trajectory roughly aligns with the intended sequence, but small\\ndiscontinuities, timing irregularities, or partial missteps occur. The reasoning path remains\\ninterpretable, and the goal can still be inferred.\\nâœ—Bad: Key steps are missing, reversed, or illogical. The sequence shows abrupt jumps,\\ninconsistent object trajectories, or goal confusion, breaking the temporal and causal coherence\\nof the reasoning process.\\nData Source.\\nWe select samples from MVoT [41], FrozenLake [8, 72], MiniBehavior [32], RBench-\\nV [25], SpatialViz-Bench [66], and OmniSpatial [29], which provide controlled multi-step envi-\\nronments for evaluating temporal reasoning, sequential planning, and causal continuity in visual\\nsimulations.\\n8\\n\\x0cExample and Analysis.\\nIn Figure 4 and Figure 5, we showcase six representative visual-trace\\nexamples. In case I, the model repeatedly fails to execute the exact step sequence and instead drifts\\ntoward a visually salient central cell. However, case II is one of the few successes: the model can\\nproduce a coherent step-by-step path in simple, low-branching settings, but this behavior is not\\nrobust across trials. Case III largely fails, where the model often does not ground the specified object\\n(printer), sometimes hallucinating its appearance or placement rather than performing a consistent\\npickup-and-place. Case IV shows near-uniform failure on long-horizon, highly branched navigation:\\noutputs contain wrong turns, discontinuities, and no faithful global plan. Case V reveals difficulty\\ngrounding abstract movement rules, producing inconsistent arrow trajectories. Case VI produces\\nvisually plausible motions along individual paths but fails to preserve or present the comparative\\ninformation required for contrastive reasoning. Taken together, these examples indicate that the\\nmodel can simulate locally coherent short traces but systematically fails at long-horizon planning,\\nrule-grounded execution, and object-persistent manipulations.\\nTakeaway 2\\nVeo-3 can produce locally coherent, short-horizon trace animations in simple, low-branching\\nscenarios, but it does not reliably execute long-horizon plans or rule-grounded sequences.\\n2.4\\nReal-World Spatial Reasoning\\nTask Description and Evaluated Aspects.\\nThis task investigates Veo-3 [21]â€™s ability to perceive\\nand maintain spatial relations within natural scenes, with a focus on reasoning about viewpoint change,\\norientation consistency, and reference-frame alignment. We assess whether the model preserves a\\nstable global coordinate frame and coherent scene orientation under varying viewpoints, and whether\\nobjects retain correct relative positions and orientations with respect to each other across different\\nviews.\\nDefinition of Good / Moderate / Bad.\\nWe define the evaluation criteria in three levels:\\nâœ“Good: Scene orientation, reference frame, and viewpoint are consistent and correctly\\nrepresent spatial relations. The camera remains steady and the motion is natural.\\n~ Moderate: Scene roughly matches the instruction but contains small perspective errors,\\nunnatural transitions, or partial mirroring. Motion remains interpretable but not physically\\ncoherent.\\nâœ—Bad: Reference frame or direction is wrong; viewpoint shifts abruptly or inconsistently.\\nVideo suffers from strong camera drift, disorienting motion, or spatial chaos.\\nData Source.\\nTo evaluate on orientation and layout reasoning, we specifically sample data from\\nMMSI-Bench [74]. Also, the tasks of perspective taking and spatial interaction are selected from the\\nOmniSpatial dataset [29].\\nExample and Analysis.\\nAs shown in Figure 6, Veo-3 can correctly handle basic spatial layouts\\nin case I, but struggles with complex viewpoints or orientation changes in case II. The perspective\\ntransformations are sometimes inaccurate or even incorrect, suggesting that the model tends to\\nprioritize visual plausibility over precise spatial reasoning, which hinders further reasoning in case IV.\\nMoreover, case III demonstrates that Veo-3 has difficulty understanding depth, further limiting its\\nspatial reasoning capability.\\nTakeaway 3\\nWhile Veo-3 exhibits an emerging ability for simple real-world spatial reasoning, its capability\\nremains insufficient for handling more complex spatial understanding tasks.\\n9\\n\\x0cInput Image:\\nReasoning Video:\\nâœ“Good\\nSuccess Rate: 33%\\nI. Question:\\nText-to-Video Prompt:\\n1st frame\\nInput Image:\\nReasoning Video:\\nAn arrow points from the player wearing jersey number 10 in purpleto the\\nbasketball. Static camera view, no zoom or pan.\\nII. Question:\\nText-to-Video Prompt:\\nQuestion: From the perspective of\\nthe player wearing jersey number 10\\nin purple, where is the basketball?\\nA: Left front.\\n1st frame\\nInput Image:\\nReasoning Video:\\nThe camera slowly and smoothly elevates from its current isometric\\nview, gradually rising upwards while maintaining focus on the\\napartment layout. It continues to ascend until it reaches a complete\\noverhead, bird\\'s-eye perspective, providing a full top-down view of the\\nentire floor plan, displaying all rooms and furniture clearly from above.\\nThe movement is fluid and controlled, ending with a static shot from\\nthe high vantage point.\\nIV. Question:\\nText-to-Video Prompt:\\nQ: If you are facing the washing machine,\\nhow should you walk to the stove and face\\nthe stove?\\nA: Turn around and go straight, then turn\\nleft and go straight, then turn right and go\\nstraight, finally turn left to face the stove.\\n1st frame\\nA red arrow point from the green chair toward the balcony. Another red arrow point\\nfrom the door to the balcony. Static camera view, no zoom or pan.\\nQuestion: The balcony is\\nnorth relative to the door,\\nin which direction\\non\\nthe balcony is the chair?\\nA: Southwest.\\nInput Image:\\nReasoning Video:\\nIII. Question:\\nText-to-Video Prompt:\\n1st frame\\nâœ—Bad\\nThe image transitions to a depth-map of the scene: Darker colors represent pixels further\\nfrom the camera, lighter colors represent pixels closer to the camera. The exact color map\\nto use is provided on the right side of the image. Static scene, no pan, no zoom, no dolly.\\nQ:\\nFrom\\nthe\\ndunker\\'s\\nviewpoint, which white-\\nuniformed player is the\\nfarthest from them?\\nA: Five.\\n~ Moderate\\nSuccess Rate: 17%\\n~ Moderate\\nSuccess Rate: 20%\\nFigure 6: Showcase of Real-World Spatial Reasoning by Veo-3. Although Veo-3 can reason\\nabout simple spatial layouts, it still struggles to maintain consistency under complex perspective or\\norientation changes.\\n10\\n\\x0cInput Image:\\nReasoning Video:\\nâœ“Good\\nSuccess Rate: 83%\\nI. Question:\\nText-to-Video Prompt:\\n1st frame\\nInput Image:\\nReasoning Video:\\nA hand moves the object to the left along the y-axis and then moves it up. Static camera \\nview, no zoom or pan, and the perspective of the object remains unchanged throughout.\\nII. Question:\\nText-to-Video Prompt:\\nQ: Move the object to the \\nleft along the y-axis and up.\\n1st frame\\nInput Image:\\nReasoning Video:\\nâœ—Bad\\nSmoothly zoom in to the \"Initial State\" figure. The yellow block, starting at (1,0,0), moves one \\nunit in the positive Y direction to position (1,1,0). Then, move back to (1,0,0). The cyan block, \\nstarting at (2,0,0), moves one unit in the positive Y direction to position (2,1,0), exchange the \\nposition with the purple block. Static shot. No pan. No glitches, noise, or artifacts.\\nIV. Question:\\nText-to-Video Prompt:\\nQ: The sequence of moves that \\nturns the first cube stack into \\nthe final one is _______.\\nA: (1,0,0)y+ (1,1,0)y- (2,1,0)y+\\n1st frame\\nA: D.\\nâœ—Bad\\nMove the object up. Static camera view, no zoom or pan, and the perspective of the \\nobject remains unchanged throughout.\\nQ: Move the object up.\\nA: D.\\nInput Image:\\nReasoning Video:\\nIII. Question:\\nText-to-Video Prompt:\\n1st frame\\nâœ—Bad\\nThe net is folded to form a single cube, with folding edges clearly shown. Static camera \\nperspective, no zoom or pan.\\nQ: Check out a net with 6 \\nfaces below: Can the net \\nbe folded to form a cube, \\nyes or no? \\nA: Yes.\\nFigure 7: Showcase of 3D Geometry Reasoning by Veo-3 (Part I). While Veo-3 shows certain\\npotential in basic 3D geometry reasoning, its performance remains unstable for complex geometry\\ntransformations.\\n11\\n\\x0ccropped_0.jpg\\nInput Image:\\nReasoning Video:\\n \\nRotate only the left 3D shape in place with a small, constant-speed yaw at fixed scale; keep \\neverything else static.\\nV. Question:\\nText-to-Video Prompt:\\nQ: From any angle, which one \\non the right is not a view of \\nthe three dimensional shape \\ngiven on the left?\\nA: C\\n1st frame\\n \\n~ Moderate\\nSuccess Rate: 17%\\nFigure 8: Showcase of 3D Geometry Reasoning by Veo-3 (Part II). The model often generates\\nmisaligned or self-intersecting structures, compromising geometric consistency.\\n2.5\\n3D Geometry Reasoning\\nTask Description and Evaluated Aspects.\\nWe also evaluate Veo-3â€™s potential on 3D geometry\\nreasoning tasks, such as geometric object motion and three-dimensional structural transformations\\nlike reconstructing a cube net. The assessment focuses on three key dimensions: geometric accuracy,\\nstructural completeness throughout the transformation, and visual continuity across frames.\\nDefinition of Good / Moderate / Bad.\\nWe categorize the modelâ€™s performance into three levels:\\nâœ“Good: Transformations like folding, rotation and assembly are geometrically correct,\\nvisually smooth, and continuous, maintaining structural integrity and realistic motion. No\\nbroken edges, jumps, or spatial artifacts.\\n~ Moderate: Transformations are partially correct but show local misalignment, unrealistic\\ndeformation, or discontinuous motion; geometry is roughly interpretable but imperfect.\\nâœ—Bad: Transformation fails. For example, wrong fold, structure collapse, or impossible\\ngeometry. Motion is erratic, discontinuous, or visually implausible, breaking the sense of\\nphysical realism.\\nData Source.\\nTo construct diverse and representative evaluation data, we adapt tasks from estab-\\nlished geometric spatial reasoning datasets, including the 3D-Text-Instruct and Folding Nets subsets\\nof the STARE benchmark [43], the BlockMoving subset from the SpatialViz-Bench [66], as well as\\nVisuLogic [73] benchmark.\\nExample and Analysis.\\nWe showcase the results of Veo-3 on 3D geometry reasoning tasks\\nin Figure 7 and Figure 8. Veo-3 demonstrates a degree of potential on 3D geometry reasoning,\\nperforming reasonably well on simple, single-step geometric transformations, as shown in case I.\\nHowever, its performance degrades noticeably when facing multi-step or compositionally complex\\ntransformations in case II. As presented in cases III and V, the model frequently produces misaligned\\nor self-intersecting structures, leading to a loss of geometric consistency. Further observations in case\\nIV, show that while the model can partially understand the geometric shape of individual objects, it\\nlacks a coherent understanding of coordinate systems and the spatial relationships among multiple\\nobjects.\\nTakeaway 4\\nVeo-3 exhibits emerging reasoning potential on basic 3D transformations but breaks down on\\ncomplex or multi-step geometry, often yielding misaligned or self-intersecting structures. Its\\n12\\n\\x0cTakeaway 5\\n3D geometric reasoning remains fragile, revealing substantial gaps in its ability to function as a\\nreliable 3D geometry reasoner.\\n2.6\\n2D Geometry Reasoning\\nTask Description and Evaluated Aspects.\\nTo assess a modelâ€™s competence in 2D geometric\\nreasoning, we evaluate its zero-shot performance on planar geometric construction tasks. These\\ntasks involve drawing geometric relations by connecting points, adding auxiliary lines, and moving\\ngeometric shapes. The evaluation focuses on whether the generated constructions or movements\\naccurately reflect the described geometric relationships and adhere to the given instructions, while\\nmaintaining smooth, stable operations that ensure visual clarity and coherence throughout the process.\\nDefinition of Good / Moderate / Bad.\\nWe rate the performance according to the following criteria:\\nâœ“Good: Constructions and movements are geometrically accurate and visually smooth.\\nEndpoints, intersections, angles, and motion trajectories align correctly with the instructions.\\nBoth drawing and movement processes are stable, fluid, and natural, resembling human\\nsketching or manipulation.\\n~ Moderate: Constructions and movements roughly follow the intended geometry but exhibit\\nminor inaccuracies in line placement, shape alignment, trajectory, or smoothness. Some local\\njitter or abrupt motion may appear, but the overall structure and motion remain interpretable.\\nâœ—Bad: Constructions or movements deviate substantially from geometric correctness. Lines\\nor shapes may be misplaced, disconnected, or moved in a chaotic or discontinuous manner\\n(e.g., jittering, overlapping, or distorted paths), leading to visual instability and loss of\\ninterpretability.\\nData Source.\\nThe evaluation data are drawn from multiple established sources, including the\\nGeo170k dataset [18], the VarsityTutors subset of Math-PUMA [85] dataset, the line-connection\\nsubset of RBench-V [25], the MAVIS-Gen [80], Tangram Puzzle subsets of the STARE [43] benchmark,\\nand data from VAT [46].\\nExample and Analysis.\\nThe representative examples of the 2D geometry reasoning task are\\npresented in Figures 9 and 10. Veo-3 demonstrates a foundational capability for simple geometric\\nconnection tasks, correctly identifying and linking elements in straightforward scenarios like in case\\nIII. However, this basic competence is inconsistent. The model often prioritizes producing visually\\nsymmetric or semantically meaningful patterns rather than strictly adhering to geometric instructions\\n(cases I and II). Furthermore, case II reveals instances where the model unintentionally modifies the\\noriginal figures, indicating a limited awareness of geometric constraints and poor spatial consistency.\\nWhen tackling more complex connection tasks, the model frequently fails to interpret the intended\\ndrawing order or point indices, resulting in incorrect connection sequences, as demonstrated in cases\\nV, VI, and VII. This is often coupled with an inability to control task termination, as the model tends\\nto continue drawing beyond the required constructions. Finally, for tasks involving the movement\\nof geometric shapes in cases IV and VIII, the model struggles to maintain geometric structural\\nconsistency throughout the motion.\\nTakeaway 6\\nVeo-3 shows initial 2D geometric reasoning ability but still falls short of consistent, constraint-\\naware geometric understanding, remaining far from a robust geometric reasoner.\\n13\\n\\x0cInput Image:\\nReasoning Video:\\nA line connecting point A and point C. The\\nvideo ends once the connection process is\\ncomplete. Static view, no zoom or pan.\\nI. Question:\\nText-to-Video Prompt:\\nQ: In the figure shown, let \\'n\\' represent the length of side AB of the inscribed\\nrectangle ABCD, where n is an undetermined value. With BC equal to 6.0\\nand the diameter of circle O equal to 10.0, what is the value of \\'nâ€™?\\nA: 8\\n1st frame\\nInput Image:\\nReasoning Video:\\nII. Question:\\nText-to-Video Prompt:\\n1st frame\\nIII. Question:\\nText-to-Video Prompt:\\nIV. Question:\\n~ Moderate\\nSuccess Rate: 83%\\nSmoothly\\nconnecting\\npoint\\nM and point N. The video\\nends\\nonce\\nthe\\nconnection\\nprocess is complete. Static\\nview, no zoom or pan.\\nQ: The figure presented depicts a square designated as ABCD. Within this square, point\\nM is identified as the midpoint of the side AB, while point N is the midpoint of the\\nopposing side CD. Additionally, point O is located at the midpoint of segment CN. Your\\ntask is to draw the segment MO. It is given that the length of segment AM is represented\\nby t. The objective is to determine which of the following expressions accurately\\nrepresents the length of the segment MO in terms of t.\\nA: 17\\n4 ð‘¡\\nInput Image:\\nReasoning Video:\\n1st frame\\nSmoothly connecting point C and point D with a line. The video\\nends once the connection process is complete. Static view, no\\nzoom or pan.\\nQ: AB equals to 8.0. What would the area of \\nthe entire shape ABCD be?\\nA: 62.87\\nText-to-Video Prompt:\\nInput Image:\\nReasoning Video:\\n1st frame\\nPlace piece A with its upper-\\nleft corner at (x, y) = (0, 3).\\nQ: Check out an Tangram puzzle below. The left panel is an empty Tangram puzzle,\\nwhile the right panel shows available pieces to complete the puzzle. Keep in mind\\nthat you can rotate or flip the pieces. Can the Tangram puzzle be completed with\\nthe available pieces, yes or no?\\nA: Yes.\\nâœ—Bad\\nâœ—Bad\\n~ Moderate\\nSuccess Rate: 33%\\nFigure 9: Showcase of 2D Geometry Reasoning by Veo-3 (Part I). While Veo-3 shows potential in\\nrecognizing simple patterns, it lacks the robust constraint awareness essential for accurate geometric\\nmanipulation.\\n14\\n\\x0cInput Image:\\nReasoning Video:\\nAnimate the dots connecting sequentially from 1 to\\n25, each straight line appearing smoothly until the\\nfull outline emerges. Keep the background with the\\nsmiling sun and plants unchanged. Static shot.\\nV. Question:\\nText-to-Video Prompt:\\nQ: Connect the black dots in the image sequentially with straight\\nlines according to the edge numbers (i.e., connect dot 1 to dot 2,\\ndot 2 to dot 3, and so on). The final result will form a simple line\\ndrawing. What does this drawing represent?\\nA: Duck.\\n1st frame\\nInput Image:\\nReasoning Video:\\nVI. Question:\\nText-to-Video Prompt:\\n1st frame\\nVII. Question:\\nText-to-Video Prompt:\\n~ Moderate\\nSuccess Rate: 83%\\nInput Image:\\nReasoning Video:\\n1st frame\\nâœ—Bad\\nAnimate\\nthe\\nnumbered\\ndots\\nconnecting\\nsequentially from 1 to 118, each straight line\\nappearing\\nsmoothly\\nas\\nthe\\noutline\\ngradually\\nemerges. Keep all numbers and dots visible while\\nthe connecting lines form step by step. Static shot.\\nQ: Connect the black dots in the image sequentially with straight\\nlines according to the edge numbers (i.e., connect dot 1 to dot 2,\\ndot 2 to dot 3, and so on). The final result will form a simple line\\ndrawing. What does this drawing represent?\\nA: Lion.\\nAnimate\\nthe\\nnumbered\\ndots\\nconnecting\\nsequentially from 1 to 118, each straight line\\nappearing\\nsmoothly\\nas\\nthe\\noutline\\ngradually\\nemerges. Keep all numbers and dots visible while\\nthe connecting lines form step by step. Static shot.\\nQ: Connect the black dots in the image sequentially with straight\\nlines according to the edge numbers (i.e., connect dot 1 to dot 2,\\ndot 2 to dot 3, and so on). The final result will form a simple line\\ndrawing. What does this drawing represent?\\nA: Bird.\\nâœ—Bad\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nPan vertically at a steady speed to center the top yellow line, then the bottom one, keeping \\nboth visible with identical scale and exposure. Ensure all elements stay unchanged and finish \\nwith a full view showing both lines together.\\nVIII. Question:\\nText-to-Video Prompt:\\nQ: Is the top yellow line \\nshorter than the bottom \\nyellow line?\\nA: No.\\n1st frame\\nâœ—Bad\\nFigure 10: Showcase of 2D Geometry Reasoning by Veo-3 (Part II). Veo-3â€™s reasoning abilities are\\nfurther challenged by complex sequential instructions and the need to preserve structural integrity.\\n15\\n\\x0cInput Image:\\nReasoning Video:\\nShow the rough semicircular track with\\nheight label h and a block at P; release it,\\nadd faint friction streaks as it slides down\\nand up the right side, stopping below the\\nrim.\\nShow\\nthe\\nmove\\nquickly\\nand\\ncompletely. Static shot.\\nI. Question:\\nText-to-Video Prompt:\\nQ: The figure shows a rough semicircular track whose ends are at a\\nvertical height h. A block placed at point P at one end of the track is\\nreleased from rest and slides past the bottom of the track. Which of the\\nfollowing is true of the height to which the block rises on the other side\\nof the track?\\nA: It is between zero and h; the exact height depends on how much\\nenergy is lost to friction.\\n1st frame\\nInput Image:\\nReasoning Video:\\nII. Question:\\nText-to-Video Prompt:\\n1st frame\\nIII. Question:\\nText-to-Video Prompt:\\nIV. Question:\\nAnimate the red ball moving along the blue\\narrow\\'s direction, bouncing off\\nthe black\\nwalls according to reflection rules, keeping\\nspeed consistent. Continue its path upward\\nuntil it reaches and collides with one of the\\nnumbered top bricks. Static shot.\\nQ: The red ball moves in the direction indicated by the blue arrow\\nand bounces off the black side walls upon collision; the component\\nof its velocity perpendicular to the wall reverses in direction but\\nmaintains its magnitude, while the component parallel to the wall\\nremains unchanged. Based on this behavior, please estimate which\\nnumbered brick (from 1 to 10) at the top the red ball will hit first.\\nA: 1.\\nInput Image:\\nReasoning Video:\\n1st frame\\nDynamically\\ndepict\\nthe\\nattraction\\nbetween\\nmagnets,\\npaying\\nattention to speed and intensity. Static shot.\\nQ: Think about the magnetic force between\\nthe magnets in each pair.\\nA: The magnetic force is stronger in Pair 2.\\nText-to-Video Prompt:\\nInput Image:\\nReasoning Video:\\n1st frame\\nThe orange gear rotates counterclockwise in the given view.\\nAnimate the provided planetary gear system. The orange gear is\\nfixed on the green gear. The central orange sun gear rotates\\ncounterclockwise, driving the yellow planet gear. All components\\nmust maintain their relative axial positions and proper gear\\nmeshing. The camera is static, with no zoom or pan.\\nQ: The orange gear is fixed on the\\nstationary green gear. If the orange\\ngear rotates counterclockwise in the\\ngiven view, what is the motion of the\\nyellow gear relative to the orange gear?\\nA: Clockwise rotation.\\nCounterclockwise revolution.\\nâœ—Bad\\nâœ—Bad\\nâœ—Bad\\n~ Moderate\\nSuccess Rate: 83%\\nFigure 11: Showcase of Physics-based Reasoning by Veo-3. The physics scenarios demonstrate lo-\\ncally plausible dynamics and reflections, alongside systematic quantitative and causal inconsistencies\\nunder frictional, force-driven, or constrained interactions.\\n16\\n\\x0c2.7\\nPhysics-based Reasoning\\nTask Description and Evaluated Aspects.\\nThe physics-based reasoning category assesses a\\nmodelâ€™s capacity to depict and reason about motion dynamics, physical causality, and rule-based\\ninteractions between objects. Tasks in this group involve gravity, collisions, reflection, momentum, or\\nenergy conservation, requiring the model to generate physically plausible and temporally coherent\\nmotion. Evaluation focuses on two complementary aspects: (i) physical plausibility, which means\\nwhether the simulated motion obeys common physical principles; and (ii) causal correctness, which is\\nwhether object interactions are consistent with the underlying cause-and-effect relationships described\\nin the prompt.\\nDefinition of Good / Moderate / Bad.\\nWe rate the performance according to the following criteria:\\nâœ“Good: The motion sequence adheres to physical laws such as gravity, momentum, and\\nenergy conservation. Object interactions are realistic and temporally smooth, and the visual\\noutcome remains coherent and credible throughout.\\n~ Moderate: The physical relations are approximately correct but include minor inconsisten-\\ncies, such as irregular acceleration, timing mismatch, or slight violation of conservation. The\\noverall motion remains interpretable and visually plausible.\\nâœ—Bad: The motion is physically implausible or visually chaoticâ€”objects float, stop abruptly,\\nor behave contrary to basic causal principles. Severe artifacts or temporal discontinuities\\ndisrupt the perception of a coherent physical process.\\nData Source.\\nWe draw samples from MMMU [77], ScienceQA [49], and related physical reasoning\\nsubsets of RBench-V [26] and SpatialViz-Bench [66], covering scenarios such as object collisions,\\npendulum motion, frictional sliding, and optical or magnetic interactions.\\nExample and Analysis.\\nFigure 11 presents four representative physics tasks and their outputs.\\nCase I shows that the model can produce a visually coherent slide, but the behavior violates basic\\nphysical laws. Case II is the most reliable, where reflections and general trajectory shape are rendered\\nplausibly and the task attains a high success rate, although small angular or timing offsets are common.\\nIn case III, the model conveys attraction through motion, yet the depicted dynamics do not reliably\\ntrack the intended force magnitudes or causal ordering. Finally, case IV exposes structural failures,\\nincorrect meshing, inconsistent relative rotations, and nonphysical contact behavior occur frequently,\\nso the mechanical constraints are not respected. Overall, the model can synthesize locally plausible\\ndynamics and handle simple reflection rules, but it fails to maintain quantitative physical constraints\\nand causal fidelity in frictional, force-driven, or mechanically constrained scenarios.\\nTakeaway 7\\nVeo-3 often generates visually plausible short-term dynamics, but it systematically fails to\\npreserve quantitative physical constraints (energy, momentum), causal ordering, and contact\\nmechanics in frictional, force-driven, or mechanically constrained scenarios. Thus, its outputs\\nare somewhat useful for qualitative illustration but are not reliable for quantitative physics\\ninference or causal prediction.\\n2.8\\nRotation Reasoning\\nTask Description and Evaluated Aspects.\\nThe rotation reasoning task assesses the ability to\\nreason about planar object rotation and maintain consistent spatial grounding under rotational\\ntransformations, thereby supporting subsequent reasoning processes. In each instance, the model is\\nrequired to accurately rotate target objects within a fixed 2D plane while preserving the overall scene\\nstructure and structural consistency, followed by performing reasoning tasks like grounding and OCR.\\nThe evaluation focuses on both the accuracy of the rotation in terms of angle and direction, and the\\nprecision of the resulting reasoning tasks.\\n17\\n\\x0cInput Image:\\nReasoning Video:\\nI. Question:\\nText-to-Video Prompt:\\n1st frame\\nInput Image:\\nReasoning Video:\\nRotate the scene 180 degrees clockwise. Then draw a bounding \\nbox around the leftmost vending machine.\\nII. Question:\\nText-to-Video Prompt:\\nQ: Looking up from the floor, how many rows\\nof drinks are in the leftmost vending machine?\\nA: 2\\n1st frame\\nInput Image:\\nReasoning Video:\\nâœ—Bad\\nThe entire \\'Original\\' grid figure performs one smooth, continuous 360-degree \\nrotation clockwise within its own 2D plane. The camera stays static, with no pan.\\nIV. Question:\\nText-to-Video Prompt:\\nQï¼šWhich grid can be obtained \\nby rotating the grid only?\\n1st frame\\nâœ—Bad\\nRotate the scene 45 degrees clockwise. Then draw bounding boxes around the\\nfrontmost skiing character.\\nQ: Is the frontmost skier\\nwearing a scarf?\\nA: No.\\nInput Image:\\nReasoning Video:\\nIII. Question:\\nText-to-Video Prompt:\\n1st frame\\nâœ—Bad\\nRotate the video frame 90 degrees counterclockwise in the 2D\\nplane, then draw bounding boxes around each \\'IKEA\\' label.\\nQ: On which floors are the \\'IKEA\\' labels located?\\nA: One on the top floor, one on the middle floor, \\nand one on the bottom floor.\\n~ Moderate\\nSuccess Rate: 83%\\nA: A\\nFigure 12: Showcase of Rotation Reasoning by Veo-3. Veo-3 struggles in complex scenes. However,\\nits foundational grasp of simple rotations signals its potential to support rotation-based reasoning\\ntasks.\\n18\\n\\x0cDefinition of Good / Moderate / Bad.\\nModel outputs are categorized into three quality levels:\\nâœ“Good: The rotation is accurate, complete, and strictly confined to the 2D plane, with no\\nextraneous scene motion. The following reasoning tasks are completed correctly. Target\\nobjects remain precisely grounded after rotation.\\n~ Moderate: The rotation is largely correct but may be incomplete or slightly off-angle,\\nthough still confined to the 2D plane. The following reasoning tasks are mostly completed.\\nMinor temporal or visual inconsistencies may appear, but do not alter the core 2D structure or\\nobject grounding.\\nâœ—Bad: The model fails to perform the correct rotation, extends the transformation into 3D\\nspace, or introduces substantial scene distortion. Cannot complete the following reasoning\\ntask. The original 2D structure is altered, leading to inaccurate grounding of the target objects.\\nData Source.\\nTo specifically assess the rotation reasoning task, we recruit some PhD-level experts\\nwith deep expertise in text-image reasoning to design the evaluation data manually, followed by the\\nnecessary review process, as mentioned in Section 3.2. Each question is designed following the\\nprinciple that it must involve a 2D rotation to reach the correct solution, ensuring the task genuinely\\nprobes rotational understanding rather than simple visual matching. Moreover, we sample data from\\nthe 2DRotation subset from the SpatialViz-Bench [66], and reformulate the question into instructions\\nfor the video models.\\nExample and Analysis.\\nThe results are shown in Figure 12. In case I, we find that Veo-3 handles\\nsmall-angle rotations and simple planar scenes reasonably well, demonstrating a basic grasp of\\nrotational motion. However, in more complex scenarios like cases II, III, and IV, the model often\\nignores the 2D rotation constraint and inadvertently alters the 3D structure, resulting in incorrect\\nrotations and degraded spatial grounding. Such errors frequently propagate to downstream tasks, such\\nas OCR in case III, or object localization in case II, due to inconsistencies in post-rotation alignment.\\nThese observations suggest that the reasoning behavior of Veo-3 remains more pattern-driven rather\\nthan principle-driven. However, as it demonstrates a partial understanding of planar rotation, this can\\nto some extent facilitate subsequent reasoning tasks.\\nTakeaway 8\\nVeo-3 exhibits only a superficial understanding of rotation reasoning. While it can approximate\\nsmall planar rotations, it fails to preserve geometric consistency under larger or compound\\ntransformations.\\n2.9\\nTable and Chart Reasoning\\nTask Description and Evaluated Aspects.\\nThe table and chart reasoning task requires the model\\nto identify and focus on the key elements within visualizations or tabular data. For evaluation, we\\nfurther consider how effectively the model identifies the regions relevant to the query and whether\\nit can transition smoothly and visually coherently to these areas, preserving clarity, continuity, and\\nproper scaling.\\nDefinition of Good / Moderate / Bad.\\nWe rate the performance according to the following criteria:\\nâœ“Good: Camera precisely focuses on the correct chart or table segment, smoothly high-\\nlighting or zooming into the queried data (e.g., correct year, category, or value). Motion is\\ncontinuous, the chart and table remain clear, and no distortion or overexposure occurs.\\n~ Moderate: Camera approximately focuses on the right region but partially misses boundaries,\\nintroduces slight blur, or transitions abruptly. Data can still be inferred.\\nâœ—Bad: Video fails to locate the correct region or changes the chart or table geometry\\nunnaturally. Motion jitter, scaling errors, or artifacts make data unreadable or misleading.\\n19\\n\\x0cInput Image:\\nReasoning Video:\\nStart with smoothly zooming in to focus on the \\'Nova Scotia\\' row. Then,\\nsmoothly zoom out to the full view of the chart. End with smoothly zooming\\nin to focus on the \\'Manitoba\\' row. The chart itself, including all its data, lines,\\nand labels, must remain completely static and unchanged throughout the video.\\nI. Question:\\nText-to-Video Prompt:\\nQ: What is the sum of footwear\\nmanufacturing establishments in\\nNova Scotia and Mantioba as of\\nDecember 2020?\\nA: 3\\n1st frame\\nInput Image:\\nReasoning Video:\\nII. Question:\\nText-to-Video Prompt:\\n1st frame\\nIII. Question:\\nText-to-Video Prompt:\\nIV. Question:\\nStart with a static, full view of the chart. Then, smoothly zoom the camera in to focus on\\nthe vertical area corresponding to the year 2014. The chart itself, including all its data,\\nlines, and labels, must remain completely static and unchanged throughout the video.\\nQ: In the year 2014, which\\nopinion is dominant?\\nA: Unfavorable.\\nInput Image:\\nReasoning Video:\\n1st frame\\nZoom in to focus on the smallest section in the chart. The chart itself, including all its data,\\nlines, and labels, must remain completely static and unchanged throughout the video.\\nQ: What\\' the color of \\nsmallest section in the chart?\\nA: Gray.\\nText-to-Video Prompt:\\nInput Image:\\nReasoning Video:\\n1st frame\\nDraw a bounding box around the end market for the Engineered\\nSystems segment. The table itself, including all its text, lines, and labels,\\nmust remain completely static and unchanged throughout the video.\\nQ: What is the end market for the \\nEngineered Systems segment?\\nA: Printing & Identification, Industrials.\\nâœ—Bad\\n~ Moderate\\nSuccess Rate: 83%\\nâœ—Bad\\nâœ—Bad\\nFigure 13: Showcase of Table and Chart Reasoning by Veo-3. Veo-3 demonstrates an initial ability\\nto focus on relevant data regions but lacks the precision and consistency required for reliable visual\\nanalysis.\\n20\\n\\x0cData Source.\\nWe use samples from the ChartQA [52] dataset and TableVQA-Bench [34].\\nExample and Analysis.\\nFor charts, as presented in cases I, II and III in Figure 13, Veo-3 can often\\nzoom into an approximately correct region but lacks the precision needed to accurately locate the\\nqueried data. For tables, as shown in case IV, Veo-3 fails to correctly identify the required element\\nand tends to select entries randomly. The model also frequently adds, modifies, or distorts existing\\nchart and table elements, resulting in visual inconsistencies that undermine the accuracy of chart\\ninterpretation.\\nTakeaway 9\\nVeo-3 demonstrates emerging competence and potential in structured visual understanding, but\\nstill falls short of functioning as a precise and reliable chart-table reasoner.\\n2.10\\nObject Counting Reasoning\\nTask Description and Evaluated Aspects.\\nIn this category, we focus on the ability to accurately\\nenumerate objects within a 2D or 3D scene. In each instance, the model is required to identify, ground,\\nand count target objects, typically by highlighting, drawing bounding boxes, applying numerical\\nlabels, or panning. The evaluation focuses on the accuracy of the count and the precision of the\\nspatial grounding, performed within a scene that remains static or experiences only minimal motion,\\nensuring the counting process is not influenced.\\nDefinition of Good / Moderate / Bad.\\nModel outputs are categorized into three quality levels:\\nâœ“Good: The model precisely highlights, draws bounding boxes around, or labels the objects\\nwith correct numbers, and performs smooth and controlled panning when necessary to cover\\nall targets. Motion is continuous, and the scene remains static or experiences only slight\\nchanges that do not influence the counting process.\\n~ Moderate: The model approximately highlights or draws bounding boxes around the objects,\\nor performs panning with minor instability or incomplete coverage. Objects or the scene may\\nmove or change slightly, but this does not strongly affect the counting process.\\nâœ—Bad: The model fails to correctly highlight, label, or draw bounding boxes around the\\nobjects, or pans erratically such that parts of the scene are missed or revisited unnecessarily.\\nObjects or the scene move or change substantially, severely affecting the counting process.\\nData Source.\\nThe 2D object counting data are sampled from the counting subset of RBench-V [25].\\nThe 3D object counting data are from the Super-CLEVER dataset [45] and VAT [46].\\nExample and Analysis.\\nThe results are shown in Figures 14 and 15. In the 2D counting tasks from\\ncases I to III, objects frequently move or change during the process, negatively impacting counting\\nstability and accuracy. In the 3D counting tasks, Veo-3 successfully handles simple grounding and\\ncounting scenarios, as demonstrated in case V, but struggles with scenes involving complex materials\\nor geometric variations in cases VI and VII, leading to inaccurate counts. Additionally, in the panning\\nprocess of case VII, the camera fails to precisely move to the regions containing all target objects,\\nfurther hindering the counting process.\\nTakeaway 10\\nVeo-3 demonstrates basic counting capability but lacks the spatial control and robustness\\nrequired for reliable object enumeration in dynamic or complex scenes.\\n21\\n\\x0ccropped_0.jpg\\nInput Image:\\nReasoning Video:\\nA scanner dot moves along the black line from bottom-left to top-right. As soon as this\\ndot enters a new grid square, that entire square is instantly filled with yellow color and\\nstays yellow. A square only turns yellow if the scanner dot on the line has entered it.\\nStatic camera, no zoom.\\nI. Question:\\nText-to-Video Prompt:\\nQ:\\nHow\\nmany\\nunit\\nsquares\\ndoes\\nthe\\nline\\nsegment pass through in\\nthe given grid diagram?\\nA: 16\\n1st frame\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nHighlight only the rectangles in the figure with a bright yellow color. Not highlight \\nany other shapes like squares, triangles, circles, or irregular polygons. Static camera, \\nno zoom, no pan.\\nII. Question:\\nText-to-Video Prompt:\\nQ: How many rectangles \\nare there in the figure?\\nA: 8\\n1st frame\\nâœ—Bad\\nâœ—Bad\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nLabel all the fish with increasing numbers (1, 2, 3, ...). The fish keep static. Static \\ncamera, no zoom, no pan.\\nIII. Question:\\nText-to-Video Prompt:\\nQ: How many rectangles \\nare there in the figure?\\nA: 18\\n1st frame\\nâœ—Bad\\nFigure 14: Showcase of 2D Object Counting Reasoning by Veo-3. Veo-3â€™s lack of spatial control\\noften introduces object motion, undermining the stability and accuracy of the counting process.\\n2.11\\nGUI Reasoning\\nTask Description and Evaluated Aspects.\\nIn the Graphical User Interface (GUI) reasoning task,\\nwe focus on the capability to understand and interact with graphical user interfaces across different\\noperating systems, including Android, Linux, and Web environments. In each instance, the model is\\nrequired to perform actions, such as clicking on specific UI elements. The evaluation focuses on the\\naccuracy of the click and the temporal coherence of the interaction, ensuring the scene and irrelevant\\nUI elements remain consistent.\\nDefinition of Good / Moderate / Bad.\\nWe define the evaluation criteria in three levels:\\nâœ“Good: The click is precise, with no extraneous actions. No superfluous icons appear, and\\nthe original data and icons remain unchanged.\\n~ Moderate: The click is precise but may be accompanied by minor extraneous actions.\\nSuperfluous icons might appear but do not obscure the click target, and original data or icons\\nshow only slight alterations.\\n22\\n\\x0ccropped_0.jpg\\nInput Image:\\nReasoning Video:\\nDraw bounding boxes around the brown metal mountain bikes to the right of \\nthe origami crane. Static shot.\\nV. Question:\\nText-to-Video Prompt:\\nQ: There is a small yellow object that is \\nto the left of the tiny metal motorbike; \\nhow many brown metal mountain bikes \\nare to the right of it?\\nA: 1\\n1st frame\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nDraw bounding boxes around any matte tandem bikes and metal cruisers present in the \\nscene. Static shot.\\nVI. Question:\\nText-to-Video Prompt:\\nQ: How many cyan things \\nare matte tandem bikes or \\nmetal cruisers?\\nA: 1\\n1st frame\\nâœ“Good\\nSuccess Rate: 100%\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nPan smoothly to include both the lidâ€“body interface and the spout or cap in view at a fixed \\nscale, keeping exposure steady and avoiding any visual or geometric changes.\\nVII. Question:\\nText-to-Video Prompt:\\nQ: How many burners are \\non the stove?\\nA: 4\\n1st frame\\n~ Moderate\\nSuccess Rate: 17%\\nâœ“Good\\nSuccess Rate: 33%\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nDraw bounding boxes around the tiny things that have the same material as the green \\nmotorbike. Static shot.\\nIV. Question:\\nText-to-Video Prompt:\\nQ: How many tiny things \\nhave the same material as \\nthe green motorbike?\\nA: 1\\n1st frame\\nâœ—Bad\\nFigure 15: Showcase of 3D Object Counting Reasoning by Veo-3. Veo-3â€™s basic 3D counting\\nabilities are challenged by complex materials, geometric variations, and imprecise camera control.\\n23\\n\\x0ccropped_0.jpg\\nInput Image:\\nReasoning Video:\\n \\n \\nClick the pkgs folder to collapse it. Static shot.\\nI. Question:\\nText-to-Video Prompt:\\nQ: Collapse the pkgs folder.\\nA: \\n1st frame\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\n \\nClick the calendar icon located to the right of the flight date options, \\nnext to the price display for June 6. Static shot.\\nII. Question:\\nText-to-Video Prompt:\\nQ: A calendar icon \\nlocated to the right of \\nthe flight date options, \\nnext to the price \\ndisplay for June 6.\\n1st frame\\nâœ— Bad\\n  \\nâœ— Bad\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\n \\nClick the navigation arrow located at the right edge of the browse by category \\ncarousel. Static shot.\\nIII. Question:\\nText-to-Video Prompt:\\nQ: A navigation \\narrow located at the \\nright edge of the \\nbrowse by category \\ncarousel.\\n1st frame\\n  \\nâœ— Bad\\nA:\\nA:\\nFigure 16: Showcase of GUI Reasoning by Veo-3. Veo-3â€™s attempts at graphical interface interaction\\nexhibit visual inconsistencies and logical inaccuracies, indicating only a shallow grasp of underlying\\nGUI logic. Note that the answer to each question is a bounding box. For visual clarity, screenshots\\nwith the ground-truth bounding boxes are shown.\\nâœ—Bad: The click is imprecise or erratic. Original data and icons are significantly altered,\\nhindering judgment and assessment.\\nData Source.\\nThe Linux data are selected from the Common Linux Screenshot subset of ScreenSpot-\\nPro [42], while the Android and Web data are drawn from the OS Android and OS Web subsections\\nof MMBench-GUI [67], respectively.\\nExample and Analysis.\\nAcross the three cases in Figure 16, Veo-3 fails to accurately capture the\\ncorrect click position and often exhibits inconsistencies between the click location and the resulting\\non-screen effect. In addition, it occasionally alters or generates new icons and text, which can\\ninterfere with judgment. In the Web system in case III, however, the model demonstrates partial GUI\\nresponsiveness and provides some degree of visual feedback.\\n24\\n\\x0ccropped_0.jpg\\nInput Image:\\nReasoning Video:\\nPan to the banana while keeping tray edges in view. Fix scale (banana ~two-thirds of \\nthe frame, axis horizontal). Sweep once along the inner concave edge from stem to tip \\nat constant speed, then stop and hold at its midpoint.\\nI. Questionâ€ :\\nText-to-Video Prompt:\\nQ: Which point \\ncorresponds to \\nthe affordance for \\nmanipulating the \\nbanana?\\n1st frame\\n~ Moderate\\nSuccess Rate: 33%\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nKeep the cucumberâ€™s start and the pot opening in view. Sweep once from start to pot \\nat fixed scale and speed, briefly dwelling at four evenly spaced waypoints (p1â†’p4) \\nalong the path, then hold on both endpoints.\\nII. Questionâ€ :\\nText-to-Video Prompt:\\nQ: Which set of 4 \\npoints is a right \\ntrajectory when \\ndoing place a \\ncucumber into a pot?\\n1st frame\\nâœ—Bad\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nPan smoothly to include both the lidâ€“body interface and the spout or cap in view at a fixed \\nscale, keeping exposure steady and avoiding any visual or geometric changes.\\nIII. Questionâ€ :\\nText-to-Video Prompt:\\nQ: Is the container sealed?\\nA: No.\\n1st frame\\n~ Moderate\\nSuccess Rate: 17%\\nA:\\nA:\\nFigure 17: Showcase of Embodied Reasoning by Veo-3. It illustrates plausible static affordance\\ndetection in simple settings, common workaround/hallucination behaviors for dynamic manipulations,\\nand failures to reliably localize or preserve manipulation-relevant context. â€  Green points in the\\nanswer image denote ground-truth points or trajectories.\\nTakeaway 11\\nVeo-3 demonstrates a limited awareness of GUI click actions, imitating interaction behaviors\\nwithout fully grasping the underlying functional logic.\\n2.12\\nEmbodied Reasoning\\nTask Description and Evaluated Aspects.\\nThis category evaluates the modelâ€™s potential to perceive\\nand reason about object affordances and manipulation dynamics. It involves recognizing both static\\nand dynamic affordances, as well as identifying manipulation-relevant object and scene attributes.\\nEvaluation focuses on two aspects: (i) the generation of stable and contextually relevant visual\\nsequences, and (ii) the maintenance of reasoning fidelity without resorting to implausible planning\\nshortcuts or hallucinated interactions.\\n25\\n\\x0cDefinition of Good / Moderate / Bad.\\nWe define the evaluation criteria in three levels:\\nâœ“Good: The sweep/framing covers all candidates fairly (equal or near-equal dwell), centers\\nthe manipulation-relevant geometry (e.g., handle + frame/gap, lid-body interface, hinge side)\\nwith crisp focus and stable scale; no cropping of key context; no content alterations.\\n~ Moderate: The view roughly includes the right region(s) but with minor bias or coverage\\nissues: slight off-center, brief under-exposure of one candidate, small motion jitter, or shallow\\ncontext (still enough to infer).\\nâœ—Bad: The camera misses or biases the evidence (e.g., lingers only on one point, crops\\naway the hinge/rail, over-zooms a non-relevant patch), introduces distortion/content edits, or\\nproduces footage from which a fair decision cannot be made.\\nData Source.\\nWe select samples from Robobench [51] for the analysis. In addition to a general\\nunderstanding of static attributes, we also sample data to assess whether Veo-3 can perform direct\\nreasoning on tasks involving the generation of static and dynamic affordances.\\nExample and Analysis.\\nAs shown in Figure 17, Veo-3 demonstrates the ability to comprehend\\nobjects within real-world scenes. However, its capacity for assisting visual reasoning in embodied\\nscenarios remains constrained by insufficient stability. As illustrated in case I, when provided with a\\nclearly defined object for manipulation, Veo-3 is capable of generating plausible manipulation affor-\\ndances. When it comes to dynamic affordances, Veo-3 tends to employ workarounds to compensate\\nfor its planning deficiencies, as evidenced in case II, where it generated a new cucumber instead of\\nthe intended object. With respect to static attributes, Veo-3 struggles to accurately differentiate visual\\nprompts and misidentifies the position of containers. As shown in case III, the green box, intended to\\nspecify the location of the container, inadvertently led Veo-3 to produce hallucinations.\\nTakeaway 12\\nVeo-3â€™s capabilities are currently limited to basic object recognition rather than true embodied\\nreasoning. It lacks the necessary planning and stability to reliably interpret and act upon\\ndynamic or spatially constrained instructions, indicating its limitations in understanding and\\nreasoning of real-world interactions.\\n2.13\\nMedical Reasoning\\nTask Description and Evaluated Aspects.\\nThis category assesses the modelâ€™s ability to localize\\nlesions or structures, identify relevant attributes (e.g., side, lobe), recognize pathological patterns\\n(e.g., â€œjump distributionâ€), and make binary decisions (e.g., presence or absence). The evaluation\\nfocuses on both the correctness of object manipulation and the visual stability of the surrounding\\nregions.\\nDefinition of Good / Moderate / Bad.\\nWe define the evaluation criteria in three levels:\\nâœ“Good: The camera cleanly settles on the correct anatomical level/lesion, with clear margins\\nand readable context; motion is reasonable; no geometric distortion or content alteration.\\n~ Moderate: The view roughly covers the right area but is slightly off (partial coverage, mild\\nblur, small framing mistakes). The general shape of the tissue or organ can still be observed.\\nâœ—Bad: The video misses the target region or introduces distortions/crops that hide key cues.\\nTissues or organs begin to distort. Misleading results due to confusion of medical terminology.\\nData Source.\\nWe select samples representing different body parts from the ViTAR [9] dataset.\\nExample and Analysis.\\nWe showcase the evaluation results in Figure 18. Veo-3 retains the\\nability to manipulate images when dealing with medical images. However, due to its lack of medical\\n26\\n\\x0ccropped_0.jpg\\nInput Image:\\nReasoning Video:\\nShow the full sagittal lumbar view, then sweep smoothly from L1 to S1 at constant speed \\nwithout pausing. End on a view showing adjacent disc spaces, including narrow and normal \\nlevels. Keep image content and geometry unchanged.\\nI. Question:\\nText-to-Video Prompt:\\nQ: What is the distribution \\npattern of stenotic segments?\\nA: Jump distribution.\\n1st frame\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nShow the full PA chest view, then adjust framing to include both the heart silhouette and\\nthe widest inner thoracic diameter at a fixed scale. Keep contrast and geometry\\nunchanged, holding steady for visual CTR estimation.\\nII. Question:\\nText-to-Video Prompt:\\nQ: Is cardiomegaly presentï¼Ÿ\\nA: No.\\n1st frame\\nâœ—Bad\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nShow the full axial CT, then pan and zoom smoothly to the right lung so the nodule and \\nnearby fissure appear together. Keep windowing standard and geometry unchanged.\\nIII. Question:\\nText-to-Video Prompt:\\nQ: Which lobe contains \\nthe pulmonary nodule?\\nA: Left lobe.\\n1st frame\\nâœ—Bad\\nâœ—Bad\\nFigure 18: Showcase of Medical Reasoning by Veo-3. As shown in cases I and III, Veo-3 fails to\\nmaintain the shape of the rest of medical organization. Veo-3 also can not understand and precisely\\nlocate the mentioned medical terminology in the prompt, as demonstrated in case II.\\nknowledge, Veo-3 struggles to accurately manipulate the correct objects when instructions include\\nmedical terminology. This phenomenon is evident across all cases. Furthermore, Veo-3 cannot model\\nmedical organs effectively. When performing operations such as zooming in, the medical images\\nsuffer from significant distortion, resulting in a substantial loss of detail.\\nTakeaway 13\\nVeo-3â€™s failure to handle the reasoning in the medical domain, causing distortion even on simple\\nzoom-ins, highlights its limited grasp of specialized, non-general knowledge.\\n27\\n\\x0cMME-COF\\nFigure 19: Category Distribution.\\nTable 1: Key Statistics of MME-COF.\\nStatistic\\nNumber\\nTotal entries\\n59\\nTotal categories\\n12\\nMax prompt length\\n124\\nAvg prompt length\\n36.7\\nMax entries per category\\n7\\nAvg entries per category\\n4.9\\n3\\nMME-COF\\n3.1\\nBenchmark Overview\\nTo standardize the empirical study and systematically evaluate the reasoning potential of state-of-the-\\nart generative video models [21, 55, 56], we introduce MME-COF, which, to our knowledge, is the\\nfirst benchmark specifically designed to reveal and quantify the reasoning potential of video models.\\n3.2\\nBenchmark Composition\\nData Curation and Distribution.\\nAligning with the task taxonomy in Section 2.1, the MME-COF\\nbenchmark is curated from the cases used in our empirical study. It comprises 59 curated entries and\\ninstruction prompts spanning 12 diverse reasoning categories. The key statistics of MME-COF and\\nits overall composition are summarized in Table 1, Figure 2b and Figure 19.\\nReview Process.\\nFollowing the prompt design protocol in Section 2.1, all prompts undergo a\\ntwo-stage review process. In the cross-validation phase, each prompt was independently reviewed by\\nanother expert to ensure semantic clarity, alignment with the intended reasoning task, and the absence\\nof linguistic bias. In the final adjudication phase, discrepancies were jointly discussed and resolved\\nthrough consensus. This multi-step procedure ensured that every prompt was conceptually precise,\\nvisually grounded, and fully aligned with the evaluation objectives of MME-COF.\\n3.3\\nEvaluation Protocol\\nModels and Generation Settings.\\nWe evaluate the leading video models in a zero-shot setting,\\nincluding Kling-v1 [38], Seedance-1.0-pro [19], Veo-3.0-preview [70], Veo-3.0-fast [70], Sora-2 [56],\\nSora-2-pro [56]. Each model generates six video samples per prompt, and final scores were computed\\nas the mean across all samples. All videos are generated at a 16:9 aspect ratio. We adopt the\\ndefault 8-second duration for the Sora and Veo series, while retaining the default 5-second length for\\nKling and Seedance. Note that, since most video models apply automated safety filters and content\\nmoderation, which may block sensitive content, we exclude videos that are suppressed by such filters\\nfrom our evaluation.\\nEvaluation Metrics.\\nWe employ Gemini-2.5-Pro [12] as an automatic verifier to evaluate each\\ngenerated video. Gemini is prompted with the following evaluation criteria and returns structured\\nscores between 0 and 4, where higher values indicate better performance:\\n1) Instruction Alignment (0-4): Measures how well the video follows the described structure\\nand sequence in the prompt. A high score indicates that the visual steps faithfully reflect\\nthe textual instructions.\\n2) Temporal Consistency (0-4): Evaluates the smoothness and continuity between frames.\\nDisjointed or abrupt transitions will lead to a lower score.\\n28\\n\\x0cTable 2: Model-level Overall and Per-dimension Performance on MME-COF. Mean scores and\\nstandard deviations are reported on a 0â€“4 scale, as graded by Gemini-2.5-Pro.\\nModel\\nOverall\\nInstruction\\nAlignment\\nTemporal\\nConsistency\\nVisual\\nStability\\nContent\\nFidelity\\nFocus\\nRelevance\\nKling-v1 [38]\\n0.64 Â± 0.91\\n0.01 Â± 0.09\\n0.15 Â± 0.75\\n2.43 Â± 1.86\\n0.21 Â± 0.79\\n0.43 Â± 1.07\\nSeedance-1.0-pro [19]\\n1.41 Â± 1.51\\n0.30 Â± 0.86\\n1.65 Â± 1.57\\n2.00 Â± 1.72\\n1.13 Â± 1.65\\n1.98 Â± 1.75\\nVeo-3.0-fast [21]\\n1.44 Â± 1.51\\n0.56 Â± 1.09\\n1.37 Â± 1.51\\n1.88 Â± 1.73\\n1.10 Â± 1.52\\n2.27 Â± 1.69\\nVeo-3.0-preview [21]\\n1.45 Â± 1.50\\n0.54 Â± 1.06\\n1.43 Â± 1.53\\n1.89 Â± 1.71\\n1.12 Â± 1.49\\n2.26 Â± 1.73\\nSora-2-pro [56]\\n1.66 Â± 1.53\\n0.48 Â± 0.96\\n1.36 Â± 1.59\\n2.39 Â± 1.65\\n1.64 Â± 1.72\\n2.44 Â± 1.73\\nSora-2 [56]\\n1.72 Â± 1.59\\n0.59 Â± 1.12\\n1.52 Â± 1.69\\n2.32 Â± 1.68\\n1.62 Â± 1.75\\n2.52 Â± 1.71\\nTable 3: Per-category Scores on MME-COF. Mean scores and standard deviations are reported on\\na 0â€“4 scale, as graded by Gemini-2.5-Pro.\\nCategory\\nKling-v1 [38]\\nSeedance-1.0\\nPro [19]\\nVeo-3.0\\nFast [21]\\nVeo-3.0\\nPreview [21]\\nSora-2 [56]\\nSora-2\\nPro [56]\\nVisual Detail\\n0.72 Â± 0.69\\n1.37 Â± 1.39\\n1.10 Â± 1.24\\n1.59 Â± 1.68\\n1.14 Â± 1.32\\n1.08 Â± 1.89\\nVisual Trace\\n0.49 Â± 0.65\\n1.23 Â± 1.13\\n1.43 Â± 1.26\\n1.48 Â± 1.24\\n1.51 Â± 1.37\\n1.75 Â± 1.31\\nReal-world Spatial\\n0.77 Â± 0.76\\n1.79 Â± 1.53\\n2.07 Â± 1.54\\n2.10 Â± 1.46\\n1.84 Â± 1.43\\n1.77 Â± 1.35\\n3D Geometry\\n0.61 Â± 0.58\\n1.95 Â± 1.64\\n1.71 Â± 1.54\\n1.54 Â± 1.43\\n1.37 Â± 1.49\\n1.42 Â± 1.45\\n2D Geometry\\n0.49 Â± 0.67\\n0.96 Â± 1.11\\n1.18 Â± 1.15\\n1.27 Â± 1.20\\n1.77 Â± 1.45\\n1.77 Â± 1.21\\nPhysics-based\\n0.60 Â± 0.62\\n1.27 Â± 1.25\\n1.44 Â± 1.39\\n1.44 Â± 1.35\\n2.13 Â± 1.32\\n2.10 Â± 1.33\\nRotation\\n0.22 Â± 0.34\\n2.30 Â± 1.46\\n1.83 Â± 1.44\\n1.60 Â± 1.29\\n1.62 Â± 1.37\\n1.44 Â± 1.28\\nTable & Chart\\n0.87 Â± 0.72\\n0.71 Â± 1.18\\n0.82 Â± 1.30\\n0.96 Â± 1.44\\n1.84 Â± 1.61\\n1.48 Â± 1.59\\nGUI\\n1.09 Â± 0.51\\n0.70 Â± 0.76\\n1.11 Â± 1.09\\n1.18 Â± 0.89\\n1.88 Â± 1.64\\n1.52 Â± 1.48\\nObject Counting\\n0.64 Â± 0.58\\n1.15 Â± 0.97\\n2.03 Â± 1.42\\n1.84 Â± 1.42\\n2.06 Â± 1.48\\n1.86 Â± 1.41\\nEmbodied\\n0.80 Â± 0.00\\n1.82 Â± 1.67\\n1.33 Â± 1.57\\n1.18 Â± 1.46\\n1.30 Â± 1.51\\n1.40 Â± 1.42\\nMedical\\n1.15 Â± 1.17\\n1.56 Â± 1.41\\n0.27 Â± 0.39\\n0.30 Â± 0.58\\n2.08 Â± 1.56\\n1.81 Â± 1.42\\n3) Visual Stability (0-4): Assesses the stability of the video in terms of camera motion, object\\nappearance, and scene composition. Shaky or glitchy outputs are penalized.\\n4) Content Fidelity (0-4): Determines how accurately the key elements described in the\\nprompt are preserved. Hallucinated or missing objects/events will reduce the score.\\n5) Focus Relevance (0-4): Examines whether the videoâ€™s visual attention remains focused on\\nthe correct objects or regions throughout. Irrelevant distractions or poorly framed targets\\nare penalized.\\nWe adopt a direct prompting strategy, instructing Gemini with the prompt, videos, and evaluation\\ncriteria to produce numerical scores in JSON format directly.\\n3.4\\nQuantitative Results and Analysis\\nWe report the quantitative scores of the five evaluated models across the five reasoning dimensions in\\nTable 2, and provide detailed per-category results in Table 3 and Figure 2a.\\nOverall, most models exhibit limited reasoning capability across all tasks in MME-COF, reflected\\nby generally low scores. Among the five dimensions, Visual Stability achieves the highest average,\\nindicating that current video models can generate smooth and coherent sequences. Yet, their behavior\\nremains largely at the level of pattern replay rather than genuine reasoning.\\nThe Sora-2 series [56] shows relative advantages in physics-based, embodied, and medical reason-\\ning, while the Veo-3.0 series [21] performs comparatively better in real-world spatial reasoning.\\nSeedance-1.0-pro [19] demonstrates relative strength in rotation and 3D geometry reasoning. These\\ntrends suggest that different models specialize in distinct reasoning aspects. However, their mean\\nscores remain below 2.0 out of 4, highlighting substantial room for improvement and pointing to\\nopportunities for more targeted enhancement in future development.\\n29\\n\\x0c4\\nRelated Work\\nVideo Models.\\nVideo models have been progressively evolving both in the fields of video under-\\nstanding and generation. For video understanding methods, earlier approaches, such as MViT [14],\\nVideo Swin Transformer [48], and VideoMAE [62], aim to learn a robust representation that fosters\\ndownstream tasks. With the rise of LLMs, recent approaches encode videos as tokens and exploit the\\nlanguage backbone for captioning [61], event localization [59], and high-level reasoning [28, 83].\\nVideo generation models have also attracted much attention. Closed system, including OpenAIâ€™s\\nSora [55, 56], Runwayâ€™s Gen-3 [58], Pika Labs [57], Luma AI [50], and Google DeepMindâ€™s\\nVeo series [20, 21], have exhibited impressive results. However, they remain inaccessible due to\\ntheir closed-source nature. Open-source alternatives have recently become available: Stable Video\\nDiffusion [6] introduces efficient training strategies, Hunyan-Video [37] proposes systematic scaling,\\nand Wan-2.1 [64] presents an efficient 3D VAE with expanded pipelines.\\nReasoning with Video.\\nThe advent of large reasoning models [24, 60, 27, 69], such as OpenAI\\no1 [54] and DeepSeek-R1 [23], has spurred the development of video reasoning benchmarks. Most\\ncurrent methods [15, 44, 53] employ MLLMs specialized in video reasoning understanding. For\\nexample, Video-R1 [15] specifically targets temporal reasoning capabilities by introducing a temporal\\ngroup relative policy optimization (GRPO) loss. VideoChat-R1 [44] focuses on spatio-temporal\\nreasoning abilities by training with GRPO and rule-based rewards. A two-stage training strategy,\\ncombining SFT and RL, is used by VideoRFT [65]. When trained on vast collections of images\\nand videos, this strategy boosts the modelâ€™s ability to handle QA tasks, whether in general con-\\ntexts or reasoning-focused ones. These methods primarily focus on enhancing specific types of\\nquestion-answering or captioning tasks. Concurrently, [70] demonstrates the large potential of video\\ngenerative models in video reasoning. These models have implicitly acquired world knowledge\\nthroughdemonstrates impressive performance on various tasks, includinging and reasoning capability.\\nYet, this direction has rarely been explored and only experimented with in zero-shot settings.\\nEvaluation of Video Models as Zero Shot Learner.\\nRecently, several works have been exploring\\nthe zero-shot capability of video generation models in various domains, including general-purpose\\nvision understanding [70, 17], medical imaging [39], and world models [68]. [70] conducts experi-\\nments on Veo 3 with a variety of vision tasks that have not been explicitly included during training.\\nThe video model showcases surprising performance on multiple tasks like object segmentation, image\\nediting, and even maze solving. [39] later adopts a similar paradigm to medical images understanding\\ntasks and finds video generation models also show powerful capabilities, e.g., delineation of anatomi-\\ncal structures in CT scans, medical image segmentation, and even forecasting of future 3D CT phases.\\nBesides, [68] shows that video generation models could also understand complex temporal causality\\nand world knowledge in the real world, thereby serving as a world model [2, 33].\\n5\\nConclusions and Insights\\nVideo models demonstrate an intuitive understanding of the simple visual world.\\nRecent\\nvideo models can generate high-fidelity videos with realistic motion dynamics, suggesting that they\\nhave internalized substantial visual and structural knowledge about the world. Through qualitative\\nresults from our empirical study and quantitative results from the MME-COF benchmark, our work\\nconfirms that these models do exhibit intuitive yet local reasoning potential. This emergent behavior,\\nwhich aligns with the â€œChain-of-Frameâ€ (CoF) mechanism, is revealed across several common\\nsuccess patterns. (i) Fine-grained Grounding. Models demonstrate a capability for fine-grained\\nattribute and spatial grounding, especially when targets are visually distinct, as presented in visual\\ndetail reasoning tasks. (ii) Short-horizon Trace Consistency. In Visual Trace Reasoning tasks,\\nmodels can maintain short-term consistency in visual traces. (iii) Emergent Tool-Use Simulation.\\nAn emergent ability to follow CoF instructions that mimic tool-use is presented, such as drawing\\nlines in 2D geometry, highlighting targets in object counting, or controlling the camera in table\\nand chart reasoning. (iv) Foundational Spatial and Geometric Grasp. This includes single-step 3D\\ngeometry transformations, understanding basic real-world spatial layouts, finding coherent sequential\\npaths, and handling small-angle Rotations. (v) Preliminary Real-world Interaction. Models display\\na preliminary comprehension of real-world interaction, generating coherent manipulation paths in\\nembodied reasoning.\\n30\\n\\x0cComplex visual reasoning reveals fundamental limitations.\\nHowever, visual reasoning demands\\nmore than these foundational skills. It tests a modelâ€™s ability to maintain long-horizon logical\\nconsistency, adhere to abstract constraints, and understand functional principles. In these complex\\nareas, our study reveals fundamental limitations and several common failure patterns. (i) Causal and\\nPhysical Logic. This is evident in physics-based reasoning, where the model generates implausible\\nmotion that violates basic causal principles, and in visual trace reasoning, where the generated\\nsequences break causal order with illogical steps. (ii) Long-horizon and Rule-grounded Reasoning.\\nIn visual trace reasoning, models fail to maintain state and adhere to task-specific rules over extended\\nsequences. (iii) Geometric and Spatial Logic. Models fail at multi-step or complex transformations\\nin 3D/2D geometry and real-world spatial tasks, often breaking constraints or prioritizing visual\\nplausibility over correctness. (iv) Functional and Interaction Logic. They merely imitate GUI actions\\nwithout grasping their purpose and lack the necessary planning and stability for reliable Embodied\\ntasks, often resorting to workarounds. (v) Perceptual Precision and Specialized Knowledge. This\\nweakness appears when models fail to identify small or indistinct targets in visual detail reasoning,\\ndistort data in table and chart tasks, and fail to process specialized medical imagery due to a lack of\\ndomain understanding.\\nCurrent video models are not yet ready as standalone zero-shot reasoners.\\nOverall, our findings\\nshow that current video models are not yet reliable as standalone zero-shot reasoners. Strong\\ngenerative performance does not automatically imply robust reasoning during inference. The modelâ€™s\\nbehavior appears to be driven more by learning surface-level patterns and correlations rather than by\\ninternalizing general principles. It excels at short-term coherence rather than long-horizon causality.\\nThis is evident when the model prioritizes visual plausibility over precise spatial reasoning, or favors\\nvisually symmetric patterns over strictly adhering to geometric instructions. This tendency to produce\\nplausible but instructionally flawed outputs reveals a reasoning process that is pattern-driven, not\\nprinciple-driven, thereby undermining its ability to function as a standalone zero-shot reasoner.\\nThe potential in advancing next-generation collaborative visual reasoning.\\nDespite these\\nlimitations, the emergent behaviors observed in video models signal strong potential. The CoF\\nconcept suggests a novel modality for reasoning through visual problems step by step. While these\\nmodels are not yet robust standalone reasoners, their foundational capabilities demonstrate that they\\ncan be guided through carefully designed prompts. This suggests a path where video models exhibit\\nencouraging signs as complementary visual engines alongside dedicated reasoning models.\\nReferences\\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4\\ntechnical report. arXiv preprint arXiv:2303.08774, 2023.\\n[2] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit\\nChattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model\\nplatform for physical ai. arXiv preprint arXiv:2501.03575, 2025.\\n[3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge,\\nYu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.\\n[4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang\\nZhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding,\\nlocalization, text reading, and beyond, 2023.\\n[5] Zechen Bai, Hai Ci, and Mike Zheng Shou. Impossible videos. arXiv preprint arXiv:2503.14378,\\n2025.\\n[6] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Do-\\nminik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion:\\nScaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023.\\n[7] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja\\nFidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent\\n31\\n\\x0cdiffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition, pages 22563â€“22575, 2023.\\n[8] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\\nand Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\\n[9] Kaitao Chen, Shaohao Rui, Yankai Jiang, Jiamin Wu, Qihao Zheng, Chunfeng Song, Xiaosong\\nWang, Mu Zhou, and Mianxin Liu. Think twice to see more: Iterative visual reasoning in\\nmedical vlms. arXiv preprint arXiv:2510.10052, 2025.\\n[10] Xinyan Chen, Renrui Zhang, Dongzhi Jiang, Aojun Zhou, Shilin Yan, Weifeng Lin, and\\nHongsheng Li. Mint-cot: Enabling interleaved visual tokens in mathematical chain-of-thought\\nreasoning. arXiv preprint arXiv:2506.05331, 2025.\\n[11] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu,\\nWenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling\\nand audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024.\\n[12] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit\\nDhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the\\nfrontier with advanced reasoning, multimodality, long context, and next generation agentic\\ncapabilities. arXiv preprint arXiv:2507.06261, 2025.\\n[13] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,\\nAiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd\\nof models. arXiv e-prints, pages arXivâ€“2407, 2024.\\n[14] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and\\nChristoph Feichtenhofer. Multiscale vision transformers. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision (ICCV), pages 6824â€“6835, 2021.\\n[15] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei\\nWu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning\\nin mllms. arXiv preprint arXiv:2503.21776, 2025.\\n[16] Zhanzhou Feng, Qingpei Guo, Xinyu Xiao, Ruihan Xu, Ming Yang, and Shiliang Zhang. Unified\\nvideo generation via next-set prediction in continuous domain. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision, pages 19427â€“19438, 2025.\\n[17] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang,\\nChenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive\\nevaluation benchmark of multi-modal llms in video analysis. CVPR 2025 Highlight, 2024.\\n[18] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong,\\nJianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal\\nlarge language model. arXiv preprint arXiv:2312.11370, 2023.\\n[19] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li,\\nJiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation\\nmodels. arXiv preprint arXiv:2506.09113, 2025.\\n[20] Google DeepMind. Veo 2, 12 2024. Accessed: 2024.\\n[21] Google DeepMind. Veo-3 technical report. Technical report, Google DeepMind, May 2025.\\n[22] Kaisi Guan, Zhengfeng Lai, Yuchong Sun, Peng Zhang, Wei Liu, Kieran Liu, Meng Cao, and\\nRuihua Song. Etva: Evaluation of text-to-video alignment via fine-grained question generation\\nand answering. arXiv preprint arXiv:2503.16867, 2025.\\n[23] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\\nShirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in\\nllms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.\\n32\\n\\x0c[24] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan,\\nJian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint\\narXiv:2505.07062, 2025.\\n[25] Meng-Hao Guo, Xuanyu Chu, Qianrui Yang, Zhe-Han Mo, Yiqing Shen, Pei-lin Li, Xinjie Lin,\\nJinnian Zhang, Xin-Sheng Chen, Yi Zhang, et al. Rbench-v: A primary assessment for visual\\nreasoning models with multi-modal outputs. arXiv preprint arXiv:2505.16770, 2025.\\n[26] Meng-Hao Guo, Xuanyu Chu, Qianrui Yang, Zhe-Han Mo, Yiqing Shen, Pei-Lin Li, Xinjie Lin,\\nJinnian Zhang, Xin-Sheng Chen, Yi Zhang, Kiyohiro Nakayama, Zhengyang Geng, Houwen\\nPeng, Han Hu, and Shi-Min Hu. Rbench-v: A primary assessment for visual reasoning models\\nwith multi-modal outputs. 2025.\\n[27] Ziyu Guo*, Renrui Zhang*, Chengzhuo Tong*, Zhizheng Zhao*, Peng Gao, Hongsheng Li, and\\nPheng-Ann Heng. Can we generate images with cot? letâ€™s verify and reinforce image generation\\nstep by step. CVPR 2025, 2025.\\n[28] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei\\nLiu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos.\\narXiv preprint arXiv:2501.13826, 2025.\\n[29] Mengdi Jia, Zekun Qi, Shaochen Zhang, Wenyao Zhang, Xinqiang Yu, Jiawei He, He Wang, and\\nLi Yi. Omnispatial: Towards comprehensive spatial reasoning benchmark for vision language\\nmodels. arXiv preprint arXiv:2506.03135, 2025.\\n[30] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\\nSaulnier, LÃ©lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\\nLavril, Thomas Wang, TimothÃ©e Lacroix, and William El Sayed. Mistral 7b, 2023.\\n[31] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan\\nJin, Claire Guo, Shen Yan, et al. Mme-cot: Benchmarking chain-of-thought in large multimodal\\nmodels for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621,\\n2025.\\n[32] Emily Jin, Jiaheng Hu, Zhuoyi Huang, Ruohan Zhang, Jiajun Wu, Li Fei-Fei, and Roberto\\nMartÃ­n-MartÃ­n. Mini-behavior: A procedurally generated benchmark for long-horizon decision-\\nmaking in embodied ai. arXiv preprint arXiv:2310.01824, 2023.\\n[33] Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Huang Gao, and Jiashi\\nFeng. How far is video generation from world model? â€“ a physical law perspective. arXiv\\npreprint arXiv:2406.16860, 2024.\\n[34] Yoonsik Kim, Moonbin Yim, and Ka Yeon Song. Tablevqa-bench: A visual question answering\\nbenchmark on multiple table domains. arXiv preprint arXiv:2404.19205, 2024.\\n[35] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\\nlanguage models are zero-shot reasoners. Advances in neural information processing systems,\\n35:22199â€“22213, 2022.\\n[36] Dan Kondratyuk, Lijun Yu, Xiuye Gu, JosÃ© Lezama, Jonathan Huang, Grant Schindler, Rachel\\nHornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: A large language\\nmodel for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023.\\n[37] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin\\nLi, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: A systematic framework for large video\\ngenerative models. arXiv preprint arXiv:2412.03603, 2024.\\n[38] Kuaishou Technology. Kling ai: Next-generation ai creative studio. https://klingai.com/,\\nJune 2024.\\n[39] Yuxiang Lai, Jike Zhong, Ming Li, Yuheng Li, and Xiaofeng Yang. Are video models emerging\\nas zero-shot learners and reasoners in medical imaging? arXiv preprint arXiv:2510.10254,\\n2025.\\n33\\n\\x0c[40] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan\\nZhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint\\narXiv:2408.03326, 2024.\\n[41] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan VuliÂ´c,\\nand Furu Wei. Imagine while reasoning in space: Multimodal visualization-of-thought. arXiv\\npreprint arXiv:2501.07542, 2025.\\n[42] Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang,\\nand Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer\\nuse. arXiv preprint arXiv:2504.07981, 2025.\\n[43] Linjie Li, Mahtab Bigverdi, Jiawei Gu, Zixian Ma, Yinuo Yang, Ziang Li, Yejin Choi, and Ranjay\\nKrishna. Unfolding spatial cognition: Evaluating multimodal models on visual simulations.\\narXiv preprint arXiv:2506.04633, 2025.\\n[44] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao,\\nYi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforce-\\nment fine-tuning. arXiv preprint arXiv:2504.06958, 2025.\\n[45] Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin\\nVan Durme, and Alan L Yuille. Super-clevr: A virtual benchmark to diagnose domain robustness\\nin visual reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition, pages 14963â€“14973, 2023.\\n[46] Dairu Liu, Ziyue Wang, Minyuan Ruan, Fuwen Luo, Chi Chen, Peng Li, and Yang Liu. Visual\\nabstract thinking empowers multimodal reasoning. arXiv preprint arXiv:2505.20164, 2025.\\n[47] Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, and\\nLu Hou. Fetv: A benchmark for fine-grained evaluation of open-domain text-to-video generation.\\nAdvances in Neural Information Processing Systems, 36:62352â€“62387, 2023.\\n[48] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin\\ntransformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 3202â€“3211, 2022.\\n[49] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind\\nTafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought\\nchains for science question answering. Advances in Neural Information Processing Systems,\\n35:2507â€“2521, 2022.\\n[50] LumaLabs. Dream machine, 06 2024. Accessed: 2024.\\n[51] Yulin Luo, Chun-Kai Fan, Menghang Dong, Jiayu Shi, Mengdi Zhao, Bo-Wen Zhang, Cheng\\nChi, Jiaming Liu, Gaole Dai, Rongyu Zhang, Ruichuan An, Kun Wu, Zhengping Che, Shaoxuan\\nXie, Guocai Yao, Zhongxia Zhao, Pengwei Wang, Guang Liu, Zhongyuan Wang, Tiejun Huang,\\nand Shanghang Zhang. Robobench: A comprehensive evaluation benchmark for multimodal\\nlarge language models as embodied brain, 2025.\\n[52] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: A benchmark\\nfor question answering about charts with visual and logical reasoning. In Findings of the\\nAssociation for Computational Linguistics: ACL 2022, pages 2263â€“2279, Dublin, Ireland, May\\n2022. Association for Computational Linguistics.\\n[53] Jiahao Meng, Xiangtai Li, Haochen Wang, Yue Tan, Tao Zhang, Lingdong Kong, Yunhai Tong,\\nAnran Wang, Zhiyang Teng, Yujing Wang, and Zhuochen Wang. Open-o3 video: Grounded\\nvideo reasoning with explicit spatio-temporal evidence. arXiv preprint arXiv:2510.20579, 2025.\\n[54] OpenAI.\\nOpenai\\no1\\nsystem\\ncard.\\nhttps://openai.com/index/\\nopenai-o1-system-card/, December 2024. Accessed: 2024-12-05.\\n[55] OpenAI. Video generation models as world simulators. Technical report, OpenAI, 2024.\\n[56] OpenAI. Sora 2 system card. Technical report, OpenAI, September 2025.\\n34\\n\\x0c[57] PikaLabs. Pika 1.5, 10 2024. Accessed: 2024.\\n[58] Runway. Introducing gen-3 alpha: A new frontier for video generation. https://runwayml.\\ncom/research/introducing-gen-3-alpha/, June 2024.\\n[59] Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu. Audio-visual event\\nlocalization in unconstrained videos. In Proceedings of the European conference on computer\\nvision (ECCV), pages 247â€“263, 2018.\\n[60] Chengzhuo Tong*, Ziyu Guo*, Renrui Zhang*, Wenyu Shan*, Xinyu Wei, Zhenghao Xing,\\nHongsheng Li, and Pheng-Ann Heng. Delving into rl for image generation with cot: A study on\\ndpo vs. grpo. arXiv preprint arXiv:2505.17017, 2025.\\n[61] Tony Cheng Tong, Sirui He, Zhiwen Shao, and Dit-Yan Yeung. G-veval: A versatile metric for\\nevaluating image and video captions using gpt-4o. In Proceedings of the AAAI Conference on\\nArtificial Intelligence, volume 39, pages 7419â€“7427, 2025.\\n[62] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are\\ndata-efficient learners for self-supervised video pre-training. In Advances in Neural Information\\nProcessing Systems (NeurIPS), 2022.\\n[63] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu,\\nHaiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative\\nmodels. arXiv preprint arXiv:2503.20314, 2025.\\n[64] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao,\\nJianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative\\nmodels. arXiv preprint arXiv:2503.20314, 2025.\\n[65] Qi Wang, Yanrui Yu, Ye Yuan, Rui Mao, and Tianfei Zhou. Videorft: Incentivizing video\\nreasoning capability in mllms via reinforced fine-tuning. arXiv preprint arXiv:2505.12434,\\n2025.\\n[66] Siting Wang, Minnan Pei, Luoyang Sun, Cheng Deng, Kun Shao, Zheng Tian, Haifeng Zhang,\\nand Jun Wang. Spatialviz-bench: An mllm benchmark for spatial visualization. arXiv preprint\\narXiv:2507.07610, 2025.\\n[67] Xuehui Wang, Zhenyu Wu, JingJing Xie, Zichen Ding, Bowen Yang, Zehao Li, Zhaoyang\\nLiu, Qingyun Li, Xuan Dong, Zhe Chen, et al. Mmbench-gui: Hierarchical multi-platform\\nevaluation framework for gui agents. arXiv preprint arXiv:2507.19478, 2025.\\n[68] Zeqing Wang, Xinyu Wei, Bairui Li, Zhen Guo, Jinrui Zhang, Hongyang Wei, Keze Wang, and\\nLei Zhang. Videoverse: How far is your t2v generator from a world model? arXiv preprint\\narXiv:2510.08398, 2025.\\n[69] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\\nAdvances in neural information processing systems, 35:24824â€“24837, 2022.\\n[70] ThaddÃ¤us Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky,\\nBeen Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners.\\narXiv preprint arXiv:2509.20328, 2025.\\n[71] Penghao Wu and Saining Xie. V?: Guided visual search as a core mechanism in multimodal\\nllms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\\npages 13084â€“13094, 2024.\\n[72] Qiucheng Wu, Handong Zhao, Michael Saxon, Trung Bui, William Yang Wang, Yang Zhang,\\nand Shiyu Chang. Vsp: Assessing the dual challenges of perception and reasoning in spatial\\nplanning tasks for vlms. arXiv preprint arXiv:2407.01863, 2024.\\n[73] Weiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu,\\nHouqiang Li, Xiaohua Wang, Xizhou Zhu, Wenhai Wang, Jifeng Dai, and Jinguo Zhu. Visulogic:\\nA benchmark for evaluating visual reasoning in multi-modal large language models. arXiv\\npreprint arXiv:2504.15279, 2025.\\n35\\n\\x0c[74] Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen\\nChen, Haodong Duan, Xiangyu Yue, et al. Mmsi-bench: A benchmark for multi-image spatial\\nintelligence. arXiv preprint arXiv:2505.23764, 2025.\\n[75] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming\\nYang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion\\nmodels with an expert transformer. arXiv preprint arXiv:2408.06072, 2024.\\n[76] Lijun Yu, Yong Cheng, Kihyuk Sohn, JosÃ© Lezama, Han Zhang, Huiwen Chang, Alexander G\\nHauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video\\ntransformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 10459â€“10469, 2023.\\n[77] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens,\\nDongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal\\nunderstanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition, pages 9556â€“9567, 2024.\\n[78] Chenyu Zhang, Daniil Cherniavskii, Andrii Zadaianchuk, Antonios Tragoudaras, Antonios\\nVozikis, Thijmen Nijdam, Derck WE Prinzhorn, Mark Bodracska, Nicu Sebe, and Efstratios\\nGavves. Morpheus: Benchmarking physical reasoning of video generative models with real\\nphysical experiments. arXiv preprint arXiv:2504.02918, 2025.\\n[79] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun\\nZhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly\\nsee the diagrams in visual math problems? ECCV 2024, 2024.\\n[80] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming\\nLiu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual instruction\\ntuning. arXiv e-prints, pages arXivâ€“2407, 2024.\\n[81] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu,\\nand Chunyuan Li. Llava-next: A strong zero-shot video understanding model, April 2024.\\n[82] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting\\nin large language models. arXiv preprint arXiv:2210.03493, 2022.\\n[83] Yilun Zhao, Haowei Zhang, Lujing Xie, Tongyan Hu, Guo Gan, Yitao Long, Zhiyuan Hu,\\nWeiyuan Chen, Chuhan Li, Zhijian Xu, et al. Mmvu: Measuring expert-level multi-discipline\\nvideo understanding. In Proceedings of the Computer Vision and Pattern Recognition Confer-\\nence, pages 8475â€“8489, 2025.\\n[84] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun\\nZhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all.\\narXiv preprint arXiv:2412.20404, 2024.\\n[85] Wenwen Zhuang, Xin Huang, Xiantao Zhang, and Jin Zeng. Math-puma: Progressive up-\\nward multimodal alignment to enhance mathematical reasoning. In Proceedings of the AAAI\\nConference on Artificial Intelligence, volume 39, pages 26183â€“26191, 2025.\\n36')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = pdf_loader.load()\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aabecb4a-1c72-4c15-8428-388180bbbb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tejas\\AppData\\Local\\Temp\\ipykernel_8684\\1630999078.py:1: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  doc_context = jsonify_llm_response(doc[0].json())['page_content']\n"
     ]
    }
   ],
   "source": [
    "doc_context = jsonify_llm_response(doc[0].json())['page_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67e145fb-a2b7-42e4-997f-f1ea100861ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, hereâ€™s a breakdown of the provided text, organized for clarity and highlighting key aspects:\n",
      "\n",
      "**1. Overview & Overall Assessment of Veo-3**\n",
      "\n",
      "*   **Capabilities:** Veo-3 demonstrates potential in recognizing simple patterns (2D geometry) and generating visually plausible short-term dynamics (physics).\n",
      "*   **Limitations:** It consistently struggles with maintaining quantitative physical constraints (energy, momentum), causal ordering, and contact mechanics in more complex scenarios (physics).  Its outputs are good for qualitative illustration but unreliable for precise reasoning.\n",
      "\n",
      "**2. Detailed Task Descriptions & Evaluation Categories**\n",
      "\n",
      "*   **2.1 Geometry Reasoning (2D)**\n",
      "    *   **Focus:** Assessing the ability to reason about planar object rotation and maintain consistent spatial grounding under rotation.\n",
      "    *   **Evaluation:** Based on physical plausibility and causal correctness.\n",
      "    *   **Rating System:**  Good, Moderate, or Bad (based on adherence to physical laws and causal relationships).\n",
      "\n",
      "*   **2.2 Physics-based Reasoning**\n",
      "    *   **Focus:**  Modeling and predicting motion dynamics, including:\n",
      "        *   Gravity\n",
      "        *   Collisions\n",
      "        *   Reflection\n",
      "        *   Momentum & Energy Conservation\n",
      "    *   **Evaluation:** Two key aspects:\n",
      "        *   **Physical Plausibility:**  Does the motion *look* physically reasonable?\n",
      "        *   **Causal Correctness:**  Do the object interactions follow consistent cause-and-effect relationships?\n",
      "    *   **Data Sources:** MMMU, ScienceQA, RBench-V\n",
      "    *   **Example & Analysis:** Illustrative cases highlighting strength/weakness (e.g., simple slide vs. complex interactions).\n",
      "\n",
      "*   **2.7 Rotation Reasoning** - (Not much detail provided in the extracted text. Likely this section would describe tasks specifically focused on understanding and correctly applying rotational transformations.)\n",
      "\n",
      "**3.  Key Observations & Implications**\n",
      "\n",
      "*   **Strengths:** Veo-3 excels in generating *visually* plausible short-term motion.\n",
      "*   **Weaknesses:** The modelâ€™s failure to maintain quantitative physical constraints is a critical limitation, preventing it from being reliably used for physics-based reasoning. Small inconsistencies or artifacts can quickly snowball into unrealistic behavior.\n",
      "\n",
      "**4.  Structure of the Evaluation Data**\n",
      "\n",
      "*   **Rating Levels:** Good, Moderate, Bad - provides a simple scale for assessing task performance.\n",
      "*   **Data Sources:**  Identifies the datasets used for training/testing, giving context for the model's capabilities.\n",
      "\n",
      "---\n",
      "\n",
      "**In essence, Veo-3 is a promising visual reasoning model but is not a robust physics engine.  It needs significant improvements to reliably handle quantitative and causal aspects of physical systems.**\n",
      "\n",
      "Do you want me to elaborate on any specific aspect of this breakdown, or perhaps explore potential reasons for the modelâ€™s limitations?\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # MCP server URL\n",
    "    BASE_URL = \"http://127.0.0.1:8080\"\n",
    "    \n",
    "    # url = f\"{BASE_URL}/analyze_with_llm\"    \n",
    "    # ðŸ” Example query\n",
    "    query = \"Summarize the documents\"\n",
    "    params = {\"query\": query, \"context_query\": doc_context}\n",
    "    url = f\"{BASE_URL}/process_with_llm\"\n",
    "    response = requests.post(url, params=params)\n",
    "    print(response.json()['llm_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36334d5-f692-440a-a0f0-bef829b3fec5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
