{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "763f987f-ec0a-4433-b934-074cacf92bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import uvicorn\n",
    "import requests\n",
    "import json\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from agents.ollama_agent import OllamaAgent\n",
    "from utils.paper_utils import PaperUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9376f2de-65a0-4a98-9eab-df9916b13875",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_pdf = r\"F:\\AI_ML_Notebooks\\Research_Paper_Agent\\cache\\2025-11-02_00-49-27_0000.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "780d16d4-0308-467d-bdb5-90a33abdb29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tejas\\AppData\\Local\\Temp\\ipykernel_25052\\1097576656.py:3: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  doc_context = json.loads(doc[0].json())['page_content']\n"
     ]
    }
   ],
   "source": [
    "pdf_loader = PyMuPDFLoader(path_to_pdf, mode='single')\n",
    "doc = pdf_loader.load()\n",
    "doc_context = json.loads(doc[0].json())['page_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14dda2ef-d7ca-4358-8a0e-6b3fbb2fea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = OllamaAgent(model=\"gemma3:4b\", host=\"http://127.0.0.1:11434\", temperature=0.1, seed=41, num_ctx=61440, )\n",
    "# llm.get_num_tokens(doc_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77a23487-600d-4270-abcf-81d00237cbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num tokens: 11345\n",
      "llm_response : ```json\n",
      "{\n",
      "  \"summary\": \"Veo-3 demonstrates emerging visual reasoning capabilities, particularly in short-horizon spatial coherence and fine-grained grounding, but struggles with complex reasoning conditions like long-horizon causal consistency, strict geometric constraints, and abstract logic. While it shows encouraging signs as a complementary visual engine alongside specialized reasoning models, it is not yet reliable as a standalone zero-shot reasoner.\",\n",
      "  \"overview\": \"The study comprehensively investigates Veo-3's reasoning potential across 12 dimensions, revealing strengths in short-horizon spatial coherence and local dynamics, but significant limitations in handling complex geometric constraints, causal reasoning, and abstract logic. The research highlights the model's potential as a visual engine alongside specialized reasoning models, emphasizing the need for further advancements in its reasoning capabilities.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# MCP server URL\n",
    "BASE_URL = \"http://127.0.0.1:8080\"\n",
    "\n",
    "# uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8080)  \n",
    "url = f\"{BASE_URL}/get_num_tokens\"\n",
    "params = {\"query_context\": doc_context}\n",
    "response = requests.post(url, params=params)\n",
    "print(f\"num tokens: {response.json()}\")\n",
    "\n",
    "\n",
    "# url = f\"{BASE_URL}/analyze_with_llm\"    \n",
    "# üîç Example query\n",
    "query = \"Summarize the document\"\n",
    "params = {\"query\": query, \"query_context\": doc_context}\n",
    "url = f\"{BASE_URL}/summarize_with_llm\"\n",
    "response = requests.post(url, params=params)\n",
    "llm_response = response.json()['llm_response']\n",
    "print(f\"llm_response : {llm_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8a1af3e-2ed3-48f5-93c5-f4ade11f93c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'Veo-3 demonstrates emerging visual reasoning capabilities, particularly in short-horizon spatial coherence and fine-grained grounding, but struggles with complex reasoning conditions like long-horizon causal consistency, strict geometric constraints, and abstract logic. While it shows encouraging signs as a complementary visual engine alongside specialized reasoning models, it is not yet reliable as a standalone zero-shot reasoner.',\n",
       " 'overview': \"The study comprehensively investigates Veo-3's reasoning potential across 12 dimensions, revealing strengths in short-horizon spatial coherence and local dynamics, but significant limitations in handling complex geometric constraints, causal reasoning, and abstract logic. The research highlights the model's potential as a visual engine alongside specialized reasoning models, emphasizing the need for further advancements in its reasoning capabilities.\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_parser = JsonOutputParser()\n",
    "llm_response_json = json_parser.parse(llm_response)\n",
    "llm_response_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f4e847a-3d12-40d5-80bc-e539e5d2f900",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tejas\\AppData\\Local\\Temp\\ipykernel_25052\\4243559565.py:1: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  doc_json = json.loads(doc[0].json())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': None,\n",
       " 'metadata': {'producer': 'pikepdf 8.15.1',\n",
       "  'creator': 'arXiv GenPDF (tex2pdf:e76afa9)',\n",
       "  'creationdate': '',\n",
       "  'source': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf',\n",
       "  'file_path': 'F:\\\\AI_ML_Notebooks\\\\Research_Paper_Agent\\\\cache\\\\2025-11-02_00-49-27_0000.pdf',\n",
       "  'total_pages': 36,\n",
       "  'format': 'PDF 1.7',\n",
       "  'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark',\n",
       "  'author': 'Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng',\n",
       "  'subject': '',\n",
       "  'keywords': '',\n",
       "  'moddate': '',\n",
       "  'trapped': '',\n",
       "  'modDate': '',\n",
       "  'creationDate': ''},\n",
       " 'page_content': 'Are Video Models Ready as Zero-Shot Reasoners?\\nAn Empirical Study with the MME-COF Benchmark\\nZiyu Guo‚àó‚Ä†1, Xinyan Chen‚àó2, Renrui Zhang‚àó‚Ä°2, Ruichuan An‚àó3, Yu Qi‚àó4, Dongzhi Jiang2\\nXiangtai Li3, Manyuan Zhang2, Hongsheng Li2, Pheng-Ann Heng1\\nCUHK 1IMIXR & 2MMLab\\n3Peking University\\n4Northeastern University\\n‚àóEqual Contribution\\n‚Ä†Project Lead\\n‚Ä°Corresponding Author\\nProject Page: https://video-cof.github.io\\nAbstract\\nRecent video generation models can produce high-fidelity, temporally coherent\\nvideos, indicating that they may encode substantial world knowledge. Beyond real-\\nistic synthesis, they also exhibit emerging behaviors indicative of visual perception,\\nmodeling, and manipulation [70]. Yet, an important question still remains: Are\\nvideo models ready to serve as zero-shot reasoners in challenging visual reasoning\\nscenarios? In this work, we conduct an empirical study to comprehensively\\ninvestigate this question, focusing on the leading and popular Veo-3 [21]. We\\nevaluate its reasoning behavior across 12 dimensions, including spatial, geometric,\\nphysical, temporal, and embodied logic, systematically characterizing both its\\nstrengths and failure modes. To standardize this study, we curate the evaluation\\ndata into MME-COF, a compact benchmark that enables in-depth and thorough\\nassessment of Chain-of-Frame (CoF) reasoning. Our findings reveal that while\\ncurrent video models demonstrate promising reasoning patterns on short-horizon\\nspatial coherence, fine-grained grounding, and locally consistent dynamics, they\\nremain limited in long-horizon causal reasoning, strict geometric constraints, and\\nabstract logic. Overall, they are not yet reliable as standalone zero-shot reasoners,\\nbut exhibit encouraging signs as complementary visual engines alongside dedicated\\nreasoning models.\\n1\\nIntroduction\\nVideo models [21, 63, 55, 81, 11], including text-to-video and video-to-text generation models, have\\nmade rapid progress in recent years. Thanks to advances in diffusion [75, 7, 84] and autoregressive [36,\\n76, 16] architectures, current video models can produce high-fidelity videos maintaining consistent\\nobject relations and realistic motion dynamics across frames. This suggests that the models may\\nhave internalized substantial visual and structural knowledge about the world. Recent research from\\nGoogle [70] further hints that, such models are evolving beyond pure content generation: Veo-3 [21]\\nhas been shown to perform dozens of distinct vision tasks across perception, modeling, manipulation,\\nand reasoning, without any task-specific training. These emergent capabilities have led researchers to\\nposit that video models could serve as unified, generalist vision models, much like large language\\nmodels (LLMs) [1, 13, 3, 30] have become foundation models for natural language.\\nCrucially, the sequential nature of video generation provides a new perspective on how such models\\nmight reason. Each generated frame builds upon the last, creating a temporal chain of information\\npropagation. This has been dubbed ‚ÄúChain-of-Frame‚Äù (CoF) reasoning [70], an analogy to the chain-\\nof-thought (CoT) process in LLMs [69, 35, 82, 23, 79] and their multi-modal variants (MLLMs) [12,\\n4, 40, 31, 10]. In essence, as a video model generates a sequence of frames, it can iteratively refine\\nand update the scene, thereby working through a problem step-by-step in time and space. This CoF\\nconcept suggests that, beyond surface-level pattern generation, general-purpose visual reasoning may\\nemerge from video generative models.\\narXiv:2510.26802v1  [cs.CV]  30 Oct 2025\\n\\x0cEmbodied\\nReasoning\\n3D Geometry\\nReasoning\\nReal-world Spatial\\nReasoning\\nPhysics-based\\nReasoning\\n2D Geometry\\nReasoning\\nTable and Chart\\nReasoning\\nObject Counting\\nReasoning\\nRotation\\nReasoning\\nVisual Detail\\nReasoning\\nVisual Trace\\nReasoning\\nGUI\\nReasoning\\nMedical\\nReasoning\\nVeo\\nSora\\nSeedance\\nVideo Models\\n...\\nZero-shot Reasoning?\\nKling\\nFigure 1: Overview of Our Study on the Reasoning Potential of Video Models. We investigate\\nwhether state-of-the-art video models exhibit emergent reasoning potentials beyond content synthesis.\\nThe analysis spans 12 reasoning dimensions under a unified perspective, exploring whether large-scale\\nvideo models can serve as zero-shot visual reasoners via CoF reasoning.\\nHowever, it remains unclear to what extent current video models truly exhibit reasoning about the\\ncontent they create. Strong generative performance does not automatically imply robust reasoning\\npotential. Emerging evidence [22, 47, 5, 78] shows that a model may produce coherent videos by\\nlearning surface-level patterns in the training data, rather than by internalizing general principles. For\\ninstance, a video model can maintain object continuity yet fail to grasp physical plausibility across\\na long sequence, or it may mimic observed visual sequences without understanding the underlying\\ncause-and-effect relationships. This motivates our central question: Are video models, purely through\\nlarge-scale visual learning, obtain the zero-shot reasoning potential?\\nTo this end, we present the first empirical study to systematically probe the CoF reasoning capabili-\\nties of modern video models, spanning 12 dimensions such as spatial, geometric, physical, temporal,\\nand embodied logic, as detailed in 1. We carry out our analysis on Veo-3, which has been system-\\natically examined as a zero-shot learner in prior work [70]. Our preliminary observations suggest\\nthat current leading video models exhibit comparable reasoning patterns, making Veo-3 a represen-\\ntative choice. Our analysis builds on reasoning scenarios distilled from diverse reasoning-oriented\\nbenchmarks [25, 67, 45, 71, 29, 34], as well as those we design ourselves, providing a compact yet\\nexpressive foundation. The prompts for video models are meticulously crafted by transforming the\\nunderlying, textual reasoning process of problem-solving into a clear, video-presentation format.\\nEach case receives a qualitative assessment across three performance levels, i.e., good, moderate, and\\nbad, complemented by a quantitative success rate to measure robustness.\\nTo standardize evaluation, we curate these tasks into the MME-COF benchmark, as illustrated in\\nFigure 2 and Section 3.2. Leveraging this benchmark, we measure several state-of-the-art video\\nmodels, i.e., Veo-3 [21], Sora-2 [56], Kling [38], and Seedance [19], to obtain directly comparable\\nscores and qualitative behaviors across categories. Our investigation reveals that the models exhibit\\npromising reasoning patterns in short-horizon spatial coherence, fine-grained grounding, and con-\\n2\\n\\x0cMedical\\nReasoning\\nEmbodied\\nReasoning\\nObject Counting\\nReasoning\\nGUI\\nReasoning\\nTable & Chart\\nReasoning\\nRotation\\nReasoning\\n3D Geometry\\nReasoning\\nReal-world Spatial\\nReasoning\\nVisual Trace\\nReasoning \\nVisual Detail\\nReasoning \\n2D Geometry\\nReasoning\\nPhysics-based\\nReasoning\\nKling-v1\\nSeedance-1.0-pro\\nSora-2-pro\\nSora-2\\nVeo-3-fast\\nVeo-3-preview\\n(a) Evaluation Radar Map.\\n(b) Word Cloud.\\nFigure 2: Illustration of the MME-COF Benchmark. It showcases that different models specialize\\nin distinct reasoning aspects, but most models exhibit limited reasoning capability across all tasks.\\nsistent local dynamics; however, they struggle with complex reasoning conditions, particularly in\\nlong-horizon causal consistency, geometric constraint adherence, and abstract logic. Overall, current\\nvideo models are not yet ready as standalone zero-shot reasoners. Still, they show encouraging signs\\nof emergent reasoning, suggesting strong potential as complementary reasoning agents alongside\\nspecialized models.\\nOur main contributions are summarized as follows:\\n‚Ä¢ A Comprehensive Empirical Study. We provide the first investigation of video models\\n(Veo-3) to analyze their visual reasoning potential, detailing representative successes, char-\\nacteristic errors, and the conditions under which CoF reasoning emerges, holds, or breaks.\\n‚Ä¢ The MME-COF Benchmark. We curate MME-COF, a compact benchmark providing a\\nstandardized taxonomy and an evaluation protocol aligned with CoF reasoning, enabling\\nconsistent and category-wise assessment beyond surface-level visual fidelity.\\n‚Ä¢ Insights and Directions. We summarize common success patterns (e.g., short-horizon\\ncoherence and stable spatial layout) and failure patterns (e.g., long-horizon degradation,\\nviolations of basic geometry/physics, and temporal logic), making clear when the behavior\\nreflects genuine reasoning versus pattern replay.\\n2\\nDeep-Dive Analysis on Veo-3\\n2.1\\nOverview\\nTo ensure a rigorous empirical study, we detail our core methodology in this section, including the\\ntaxonomy of reasoning tasks, test case curation process, the standardized style for prompt design,\\nand the analysis setup.\\nTask Taxonomy.\\nTo capture different dimensions of reasoning, our study starts from dozens of\\nreasoning-oriented tasks, which can be organized into the following 12 categories:\\n3\\n\\x0c1) Visual Detail Reasoning\\n2) Visual Trace Reasoning\\n3) Real-world Spatial Reasoning\\n4) 3D Geometry Reasoning\\n5) 2D Geometry Reasoning\\n6) Physics-based Reasoning\\n7) Rotation Reasoning\\n8) Table and Chart Reasoning\\n9) Object Counting Reasoning\\n10) GUI Reasoning\\n11) Embodied Reasoning\\n12) Medical Reasoning\\nEach category comprises several representative cases selected to test specific aspects of reasoning.\\nTest Case Curation.\\nWe recruit five PhD-level experts with deep expertise in text-image reasoning,\\nwho are tasked with selecting representative cases from benchmarks [25, 67, 45, 52, 74] corresponding\\nto each task category. For each reasoning case, the experts manually constructed text prompts that\\nexplicitly or unambiguously define the target reasoning objective, aiming to evaluate the potential of\\nvideo models for multi-modal reasoning.\\nPrompt Design Style.\\nTo ensure consistency and fairness, all prompts follow a unified style\\nemphasizing explicit visual constraints, controlled motion, and minimal linguistic ambiguity. Prompts\\nare encouraged to be written in imperative form and designed to reduce variance from language\\ninterpretation, focusing the model‚Äôs behavior on the intended visual reasoning objective. The overall\\ndesign principles are as follows:\\n1) Static camera and fixed viewpoint, unless motion is explicitly required by the task.\\n2) Stable spatial composition, consistent framing, and unchanging scene layout across\\nframes.\\n3) Clear specification of allowed and disallowed changes (e.g., ‚Äúno zoom, no pan, no dolly‚Äù)\\nto constrain camera dynamics.\\n4) Explicit temporal phrasing to control the pace of motion, using cues such as ‚Äúinstantly‚Äù,\\n‚Äúsmoothly‚Äù, or ‚Äústep-by-step‚Äù.\\n5) Avoidance of direct textual hints toward the answer; instructions are purely visual and\\ntask-oriented.\\n6) Inclusion of realistic phrasing and scene context to align with the model‚Äôs natural video\\npriors while minimizing artifacts.\\nThe standardized prompt style ensures that differences in output primarily reflect the model‚Äôs internal\\nreasoning potential rather than prompt variability.\\nAnalysis Setup.\\nFor every reasoning case, we construct a text prompt that explicitly or implicitly\\nspecifies the target reasoning objective. Each prompt produces six video samples at a resolution of\\n1280√ó720, 24 FPS, and a duration of 8 seconds. All experiments are conducted in a unified zero-shot\\nsetup without fine-tuning, additional supervision, or auxiliary tools.\\nWe evaluate model outputs through qualitative judgments along three levels of performance, i.e.,\\nGood , Moderate , and Bad, based on the clarity, correctness, and temporal stability of the visual\\nreasoning process. Detailed definitions and examples of these evaluation criteria are provided in the\\ncorresponding task subsections. Note that, since we observe that most video models struggle to follow\\nthe requirement of ‚Äòstatic shot‚Äô reliably, we apply more permissive qualitative criteria for static-shot\\nevaluations. We further define a success rate to measure robustness across generations for each case,\\ncomputed as the proportion of successful samples among the six generated. For cases categorized as\\nBad, the success rate is always 0. Non-zero success rates only appear in cases evaluated as Good or\\nModerate , indicating that Veo-3 exhibits some potential to perform as a visual reasoner. A higher\\nsuccess rate reflects a more stable reasoning capability of the model.\\n4\\n\\x0cInput Image:\\nReasoning Video:\\nZoom in on the black bag with the Apple logo to focus on the \\nlogo\\'s color. Static shot.\\nI. Question:\\nText-to-Video Prompt:\\nQ: What is the color of the Apple logo?\\n1st frame\\nInput Image:\\nReasoning Video:\\nII. Question:\\nText-to-Video Prompt:\\n1st frame\\nIII. Question:\\nText-to-Video Prompt:\\nA: The color of the Apple logo is polychromatic.\\nIV. Question:\\n~ Moderate\\nSuccess Rate: 17%\\nGradually zoom in on the group of people walking along the path, centering \\non the person carrying the handbag. Keep the surrounding park and benches \\nsoftly blurred to emphasize the handbag‚Äôs color. Static shot.\\nQ: What is the color of the handbag? \\nA: The color of the handbag is white.\\n‚úì Good\\nSuccess Rate: 33%\\nInput Image:\\nReasoning Video:\\n1st frame\\nSmoothly zoom in on the dog near the lower right corner of the \\nscene, then highlight the motorcycle parked near it. Keep the \\nsurrounding jeeps and people slightly blurred to emphasize \\nspatial relation. Static shot. \\nQ: Is the motorcycle on the left or right side of \\nthe dog?\\nA: The motorcycle is on the left side of the dog.\\n‚úì Good\\nSuccess Rate: 83%\\nText-to-Video Prompt:\\nInput Image:\\nReasoning Video:\\n1st frame\\nGradually zoom in on the area near the cone along the pathway, \\ncentering both the cone and the baby carriage in the frame. Keep \\nthe surrounding trees and grass softly blurred to emphasize these \\ntwo objects. Static shot.\\nQ: Is the baby carriage on the left or right side \\nof the cone?\\nA: The baby carriage is on the right side of the \\ncone.\\n‚úó Bad\\nFigure 3: Showcase of Visual Detail Reasoning by Veo-3. It illustrates Veo-3‚Äôs ability to localize\\ntargets and maintain fine-grained visual attributes across frames, together with common failure modes\\nwhen targets are small, occluded, or embedded in clutter.\\n5\\n\\x0c2.2\\nVisual Detail Reasoning\\nTask Description and Evaluated Aspects.\\nIn the visual detail reasoning category, the objective is\\nto assess a model‚Äôs ability to discern and maintain fine-grained visual attributes and spatial relations\\nwithin generated video sequences. It covers attribute recognition, e.g., identifying color, texture or\\nmaterial of an object, and spatial relation identification, e.g., recognizing that one object is on the\\nleft of or behind another object. The model is evaluated on the capacity both to attend to the correct\\ntarget region and to maintain visual consistency, across frames, of the attribute or relation in question.\\nDefinition of Good / Moderate / Bad.\\nWe define the three-level evaluation criteria as follows:\\n‚úìGood: The reasoning video accurately centers on the correct target region, clearly resolves\\nthe relevant attribute, such as color, texture or position, and maintains sharp, stable and natural\\nrendering throughout the sequence. There are no visible frame drops, artifacts or unintended\\nmotion.\\n~ Moderate: The region of interest is approximately correct, and the attribute remains\\ninferable, but the sequence suffers from minor blur, incomplete framing, slight instability\\nmild unnatural motion, or sometimes deviates from the textual instruction and produces a\\nplausible but unaligned or self-directed visual interpretation, limiting confident interpretation.\\n‚úóBad: The target region is incorrect or ambiguous, the attribute cannot be reliably inferred,\\nor the video exhibits severe artifacts: abrupt frame jumps, major jitter, unintended zoom or\\ncrop, extraneous objects interfering, or conspicuous quality degradation that obstructs the\\nreasoning task altogether.\\nData Source.\\nWe sample data from the V‚àóBench [71], which provides a comprehensive set of\\nevaluation dimensions including spatial relationship and color/attribute consistency tasks.\\nExample and Analysis.\\nWe illustrate typical behaviors of Veo-3 in visual detail reasoning through\\nfour representative cases in Figure 3. In case I, the model performs well in localizing the target:\\nalthough it does not strictly execute the ‚Äúzoom in‚Äù instruction, it instead achieves an equivalent\\nvisual outcome through a semantically consistent motion with a person‚Äôs hand. This slight deviation\\nsuggests that the model may exhibit certain generation preferences in how it interprets and realizes\\nspatial instructions, possibly reflecting stylistic tendencies learned from training data. In cases II\\nand III, the model achieves better success rates when the targets are visually salient and contextually\\ndistinct. For the handbag and dog-motorcycle scenes, Veo-3 attends to the correct regions and\\nmaintains smooth temporal coherence. However, when the object (e.g., the motorcycle) is small or\\nsurrounded by distracting elements, the model occasionally fails to locate it accurately, indicating\\nlimited fine-grained spatial discrimination in cluttered scenes. In case IV, when the target object is\\ntiny and visually indistinct, Veo-3 cannot identify it even with explicit positional hints, highlighting\\nthat the model‚Äôs perceptual grounding and reasoning weaken sharply when object size and salience\\nare too low for reliable attention.\\nTakeaway 1\\nVeo-3 performs well in fine-grained attribute and spatial reasoning for salient, well-grounded\\ntargets, but fails when objects are small, occluded, or cluttered. It sometimes exhibits stylistic\\ngeneration biases that lead to plausible yet instruction-divergent outcomes.\\n2.3\\nVisual Trace Reasoning\\nTask Description and Evaluated Aspects.\\nThe visual trace reasoning category evaluates a model‚Äôs\\nability to represent and maintain causal continuity across sequential actions. Typical tasks include\\nmaze navigation, path following, and multi-step object manipulation, where the video must visually\\nencode a coherent sequence of intermediate decisions that lead to the correct goal. Performance is\\nassessed based on two major aspects: (i) temporal coherence, which is the smoothness and logical\\n6\\n\\x0cInput Image:\\nReasoning Video:\\nStarting at the red dot in the middle-right cell, animate step-by-\\nstep moves: go down 1 cell, left 1, left 1, up 1, and up 1,\\ndrawing arrows for each step and finishing with a glow around\\nthe final cell. Static shot.\\nI. Question:\\nText-to-Video Prompt:\\nQ: Starting from the red dot, follow the given\\nmovement instructions and determine the final\\nposition. Down 1, left 1, left 1, up 1, up 1.\\nA: A\\n1st frame\\nInput Image:\\nReasoning Video:\\nII. Question‚Ä†:\\nText-to-Video Prompt:\\n1st frame\\nIII. Question‚Ä†:\\nText-to-Video Prompt:\\nIV. Question:\\nAnimate the elf moving step by step toward the gift while carefully\\navoiding the icy frozen lake. Highlight the successful path and end\\nwith the elf standing beside the gift. Static shot.\\nQ: The character must avoid falling into the\\nfrozen lake and reach the gift pack safely.\\nInput Image:\\nReasoning Video:\\n1st frame\\nAnimate the red triangle moving step by step toward the white printer,\\npicking it up once it reaches it. Then have the triangle carry the printer\\nupward and place it on the brown area representing the table. End with a\\nsubtle highlight around the printer to show it is toggled on. Static shot.\\nQ: Move the character (red triangle)\\nto pick up the white printer and\\nplace it anywhere on the desk.\\nText-to-Video Prompt:\\nInput Image:\\nReasoning Video:\\n1st frame\\nAnimate a bright path tracing from the blue point at the\\ntop through the maze‚Äôs open corridors toward the red\\npoint at the bottom, highlighting each green numbered\\nmark it passes. Keep the maze and all walls fixed while\\nthe glowing path moves smoothly through the correct\\nroute. Static shot.\\nQ: The given picture is a maze, and the black lines\\nrepresent walls that cannot be walked. Now you want to\\nwalk from the blue point to the red point. Is there a\\nfeasible path? If so, which of the green marks numbered\\n1-5 In the picture must be passed in the path?\\nA: Yes, 3.\\n‚úóBad\\n‚úìGood\\nSuccess Rate: 17%\\n‚úóBad\\n‚úóBad\\nFigure 4: Showcase of Visual Trace Reasoning by Veo-3 (Part I). It shows short-horizon path-\\nfollowing successes, object-grounding failures, and a certain bias that causes step omissions/mistakes\\nin multi-step traces. ‚Ä† The ground-truth answers of cases II and III are intuitive and non-unique,\\nwhich are omitted to highlight the key reasoning behaviors.\\n7\\n\\x0cInput Image:\\nReasoning Video:\\nCreate a 2D animation based on the provided diagram. The red arrow is\\nthe initial arrow, and the green arrow is the final arrow. The arrow can\\nmove in four directions (forward, backward, left, right), where \\'forward\\'\\nalways refers to the current direction the arrow is pointing. After each\\nmovement, the arrow\\'s direction is updated to the direction of movement.\\nMovement commands:\\n- The red arrow moves forward for 1 unit.\\n- The red arrow moves left for 1 unit (relative to its new current direction\\nafter step 1). Then turns green.\\nScene:\\n- No change in scene composition.\\n- No change in the layout of the diagram.\\nCamera: Static camera. No zoom. No pan. No glitches, noise, or artifacts.\\nV. Question:\\nText-to-Video Prompt:\\nQ: In the diagram, the red arrow is\\nthe initial arrow, and the green arrow\\nis the final arrow. The arrow can\\nmove in four directions (forward,\\nbackward,\\nleft,\\nright),\\nwhere\\n\\'forward\\' always refers to the current\\ndirection the arrow is pointing. After\\neach\\nmovement,\\nthe\\narrow\\'s\\ndirection is updated to the direction\\nof\\nmovement.\\nWhich\\nof\\nthe\\nfollowing paths can make the arrow\\nmove from the starting position to\\nthe ending position?\\nA: (Forward, 1 unit) - (Left, 1 unit)\\n1st frame\\nInput Image:\\nReasoning Video:\\nVI. Question:\\nText-to-Video Prompt:\\n1st frame\\nTwo small characters start from the same purple origin at the same\\ntime, and move along the red and green paths toward another purple\\ndestination at the same speed. Static camera, no zoom, no pan.\\nQ: What are the advantages of the green\\nroute and the red route respectively?\\n‚úóBad\\n‚úóBad\\nFigure 5: Showcase of Visual Trace Reasoning (Part II) by Veo-3. The examples highlight\\nlong-horizon planning breakdowns, inconsistent arrow/trajectory rendering, and failures to preserve\\ncomparative or sequential information across frames.\\nprogression between consecutive steps; and (ii) goal consistency, which means whether the full\\nsequence visually completes the intended reasoning trajectory without deviation or contradiction.\\nDefinition of Good / Moderate / Bad.\\nWe rate the performance according to the following criteria:\\n‚úìGood: Each movement step is depicted continuously and logically toward the correct goal.\\nThe motion is smooth, temporally consistent, and follows causal order with no skipping,\\nstuttering, or direction reversal.\\n~ Moderate: The overall trajectory roughly aligns with the intended sequence, but small\\ndiscontinuities, timing irregularities, or partial missteps occur. The reasoning path remains\\ninterpretable, and the goal can still be inferred.\\n‚úóBad: Key steps are missing, reversed, or illogical. The sequence shows abrupt jumps,\\ninconsistent object trajectories, or goal confusion, breaking the temporal and causal coherence\\nof the reasoning process.\\nData Source.\\nWe select samples from MVoT [41], FrozenLake [8, 72], MiniBehavior [32], RBench-\\nV [25], SpatialViz-Bench [66], and OmniSpatial [29], which provide controlled multi-step envi-\\nronments for evaluating temporal reasoning, sequential planning, and causal continuity in visual\\nsimulations.\\n8\\n\\x0cExample and Analysis.\\nIn Figure 4 and Figure 5, we showcase six representative visual-trace\\nexamples. In case I, the model repeatedly fails to execute the exact step sequence and instead drifts\\ntoward a visually salient central cell. However, case II is one of the few successes: the model can\\nproduce a coherent step-by-step path in simple, low-branching settings, but this behavior is not\\nrobust across trials. Case III largely fails, where the model often does not ground the specified object\\n(printer), sometimes hallucinating its appearance or placement rather than performing a consistent\\npickup-and-place. Case IV shows near-uniform failure on long-horizon, highly branched navigation:\\noutputs contain wrong turns, discontinuities, and no faithful global plan. Case V reveals difficulty\\ngrounding abstract movement rules, producing inconsistent arrow trajectories. Case VI produces\\nvisually plausible motions along individual paths but fails to preserve or present the comparative\\ninformation required for contrastive reasoning. Taken together, these examples indicate that the\\nmodel can simulate locally coherent short traces but systematically fails at long-horizon planning,\\nrule-grounded execution, and object-persistent manipulations.\\nTakeaway 2\\nVeo-3 can produce locally coherent, short-horizon trace animations in simple, low-branching\\nscenarios, but it does not reliably execute long-horizon plans or rule-grounded sequences.\\n2.4\\nReal-World Spatial Reasoning\\nTask Description and Evaluated Aspects.\\nThis task investigates Veo-3 [21]‚Äôs ability to perceive\\nand maintain spatial relations within natural scenes, with a focus on reasoning about viewpoint change,\\norientation consistency, and reference-frame alignment. We assess whether the model preserves a\\nstable global coordinate frame and coherent scene orientation under varying viewpoints, and whether\\nobjects retain correct relative positions and orientations with respect to each other across different\\nviews.\\nDefinition of Good / Moderate / Bad.\\nWe define the evaluation criteria in three levels:\\n‚úìGood: Scene orientation, reference frame, and viewpoint are consistent and correctly\\nrepresent spatial relations. The camera remains steady and the motion is natural.\\n~ Moderate: Scene roughly matches the instruction but contains small perspective errors,\\nunnatural transitions, or partial mirroring. Motion remains interpretable but not physically\\ncoherent.\\n‚úóBad: Reference frame or direction is wrong; viewpoint shifts abruptly or inconsistently.\\nVideo suffers from strong camera drift, disorienting motion, or spatial chaos.\\nData Source.\\nTo evaluate on orientation and layout reasoning, we specifically sample data from\\nMMSI-Bench [74]. Also, the tasks of perspective taking and spatial interaction are selected from the\\nOmniSpatial dataset [29].\\nExample and Analysis.\\nAs shown in Figure 6, Veo-3 can correctly handle basic spatial layouts\\nin case I, but struggles with complex viewpoints or orientation changes in case II. The perspective\\ntransformations are sometimes inaccurate or even incorrect, suggesting that the model tends to\\nprioritize visual plausibility over precise spatial reasoning, which hinders further reasoning in case IV.\\nMoreover, case III demonstrates that Veo-3 has difficulty understanding depth, further limiting its\\nspatial reasoning capability.\\nTakeaway 3\\nWhile Veo-3 exhibits an emerging ability for simple real-world spatial reasoning, its capability\\nremains insufficient for handling more complex spatial understanding tasks.\\n9\\n\\x0cInput Image:\\nReasoning Video:\\n‚úìGood\\nSuccess Rate: 33%\\nI. Question:\\nText-to-Video Prompt:\\n1st frame\\nInput Image:\\nReasoning Video:\\nAn arrow points from the player wearing jersey number 10 in purpleto the\\nbasketball. Static camera view, no zoom or pan.\\nII. Question:\\nText-to-Video Prompt:\\nQuestion: From the perspective of\\nthe player wearing jersey number 10\\nin purple, where is the basketball?\\nA: Left front.\\n1st frame\\nInput Image:\\nReasoning Video:\\nThe camera slowly and smoothly elevates from its current isometric\\nview, gradually rising upwards while maintaining focus on the\\napartment layout. It continues to ascend until it reaches a complete\\noverhead, bird\\'s-eye perspective, providing a full top-down view of the\\nentire floor plan, displaying all rooms and furniture clearly from above.\\nThe movement is fluid and controlled, ending with a static shot from\\nthe high vantage point.\\nIV. Question:\\nText-to-Video Prompt:\\nQ: If you are facing the washing machine,\\nhow should you walk to the stove and face\\nthe stove?\\nA: Turn around and go straight, then turn\\nleft and go straight, then turn right and go\\nstraight, finally turn left to face the stove.\\n1st frame\\nA red arrow point from the green chair toward the balcony. Another red arrow point\\nfrom the door to the balcony. Static camera view, no zoom or pan.\\nQuestion: The balcony is\\nnorth relative to the door,\\nin which direction\\non\\nthe balcony is the chair?\\nA: Southwest.\\nInput Image:\\nReasoning Video:\\nIII. Question:\\nText-to-Video Prompt:\\n1st frame\\n‚úóBad\\nThe image transitions to a depth-map of the scene: Darker colors represent pixels further\\nfrom the camera, lighter colors represent pixels closer to the camera. The exact color map\\nto use is provided on the right side of the image. Static scene, no pan, no zoom, no dolly.\\nQ:\\nFrom\\nthe\\ndunker\\'s\\nviewpoint, which white-\\nuniformed player is the\\nfarthest from them?\\nA: Five.\\n~ Moderate\\nSuccess Rate: 17%\\n~ Moderate\\nSuccess Rate: 20%\\nFigure 6: Showcase of Real-World Spatial Reasoning by Veo-3. Although Veo-3 can reason\\nabout simple spatial layouts, it still struggles to maintain consistency under complex perspective or\\norientation changes.\\n10\\n\\x0cInput Image:\\nReasoning Video:\\n‚úìGood\\nSuccess Rate: 83%\\nI. Question:\\nText-to-Video Prompt:\\n1st frame\\nInput Image:\\nReasoning Video:\\nA hand moves the object to the left along the y-axis and then moves it up. Static camera \\nview, no zoom or pan, and the perspective of the object remains unchanged throughout.\\nII. Question:\\nText-to-Video Prompt:\\nQ: Move the object to the \\nleft along the y-axis and up.\\n1st frame\\nInput Image:\\nReasoning Video:\\n‚úóBad\\nSmoothly zoom in to the \"Initial State\" figure. The yellow block, starting at (1,0,0), moves one \\nunit in the positive Y direction to position (1,1,0). Then, move back to (1,0,0). The cyan block, \\nstarting at (2,0,0), moves one unit in the positive Y direction to position (2,1,0), exchange the \\nposition with the purple block. Static shot. No pan. No glitches, noise, or artifacts.\\nIV. Question:\\nText-to-Video Prompt:\\nQ: The sequence of moves that \\nturns the first cube stack into \\nthe final one is _______.\\nA: (1,0,0)y+ (1,1,0)y- (2,1,0)y+\\n1st frame\\nA: D.\\n‚úóBad\\nMove the object up. Static camera view, no zoom or pan, and the perspective of the \\nobject remains unchanged throughout.\\nQ: Move the object up.\\nA: D.\\nInput Image:\\nReasoning Video:\\nIII. Question:\\nText-to-Video Prompt:\\n1st frame\\n‚úóBad\\nThe net is folded to form a single cube, with folding edges clearly shown. Static camera \\nperspective, no zoom or pan.\\nQ: Check out a net with 6 \\nfaces below: Can the net \\nbe folded to form a cube, \\nyes or no? \\nA: Yes.\\nFigure 7: Showcase of 3D Geometry Reasoning by Veo-3 (Part I). While Veo-3 shows certain\\npotential in basic 3D geometry reasoning, its performance remains unstable for complex geometry\\ntransformations.\\n11\\n\\x0ccropped_0.jpg\\nInput Image:\\nReasoning Video:\\n \\nRotate only the left 3D shape in place with a small, constant-speed yaw at fixed scale; keep \\neverything else static.\\nV. Question:\\nText-to-Video Prompt:\\nQ: From any angle, which one \\non the right is not a view of \\nthe three dimensional shape \\ngiven on the left?\\nA: C\\n1st frame\\n \\n~ Moderate\\nSuccess Rate: 17%\\nFigure 8: Showcase of 3D Geometry Reasoning by Veo-3 (Part II). The model often generates\\nmisaligned or self-intersecting structures, compromising geometric consistency.\\n2.5\\n3D Geometry Reasoning\\nTask Description and Evaluated Aspects.\\nWe also evaluate Veo-3‚Äôs potential on 3D geometry\\nreasoning tasks, such as geometric object motion and three-dimensional structural transformations\\nlike reconstructing a cube net. The assessment focuses on three key dimensions: geometric accuracy,\\nstructural completeness throughout the transformation, and visual continuity across frames.\\nDefinition of Good / Moderate / Bad.\\nWe categorize the model‚Äôs performance into three levels:\\n‚úìGood: Transformations like folding, rotation and assembly are geometrically correct,\\nvisually smooth, and continuous, maintaining structural integrity and realistic motion. No\\nbroken edges, jumps, or spatial artifacts.\\n~ Moderate: Transformations are partially correct but show local misalignment, unrealistic\\ndeformation, or discontinuous motion; geometry is roughly interpretable but imperfect.\\n‚úóBad: Transformation fails. For example, wrong fold, structure collapse, or impossible\\ngeometry. Motion is erratic, discontinuous, or visually implausible, breaking the sense of\\nphysical realism.\\nData Source.\\nTo construct diverse and representative evaluation data, we adapt tasks from estab-\\nlished geometric spatial reasoning datasets, including the 3D-Text-Instruct and Folding Nets subsets\\nof the STARE benchmark [43], the BlockMoving subset from the SpatialViz-Bench [66], as well as\\nVisuLogic [73] benchmark.\\nExample and Analysis.\\nWe showcase the results of Veo-3 on 3D geometry reasoning tasks\\nin Figure 7 and Figure 8. Veo-3 demonstrates a degree of potential on 3D geometry reasoning,\\nperforming reasonably well on simple, single-step geometric transformations, as shown in case I.\\nHowever, its performance degrades noticeably when facing multi-step or compositionally complex\\ntransformations in case II. As presented in cases III and V, the model frequently produces misaligned\\nor self-intersecting structures, leading to a loss of geometric consistency. Further observations in case\\nIV, show that while the model can partially understand the geometric shape of individual objects, it\\nlacks a coherent understanding of coordinate systems and the spatial relationships among multiple\\nobjects.\\nTakeaway 4\\nVeo-3 exhibits emerging reasoning potential on basic 3D transformations but breaks down on\\ncomplex or multi-step geometry, often yielding misaligned or self-intersecting structures. Its\\n12\\n\\x0cTakeaway 5\\n3D geometric reasoning remains fragile, revealing substantial gaps in its ability to function as a\\nreliable 3D geometry reasoner.\\n2.6\\n2D Geometry Reasoning\\nTask Description and Evaluated Aspects.\\nTo assess a model‚Äôs competence in 2D geometric\\nreasoning, we evaluate its zero-shot performance on planar geometric construction tasks. These\\ntasks involve drawing geometric relations by connecting points, adding auxiliary lines, and moving\\ngeometric shapes. The evaluation focuses on whether the generated constructions or movements\\naccurately reflect the described geometric relationships and adhere to the given instructions, while\\nmaintaining smooth, stable operations that ensure visual clarity and coherence throughout the process.\\nDefinition of Good / Moderate / Bad.\\nWe rate the performance according to the following criteria:\\n‚úìGood: Constructions and movements are geometrically accurate and visually smooth.\\nEndpoints, intersections, angles, and motion trajectories align correctly with the instructions.\\nBoth drawing and movement processes are stable, fluid, and natural, resembling human\\nsketching or manipulation.\\n~ Moderate: Constructions and movements roughly follow the intended geometry but exhibit\\nminor inaccuracies in line placement, shape alignment, trajectory, or smoothness. Some local\\njitter or abrupt motion may appear, but the overall structure and motion remain interpretable.\\n‚úóBad: Constructions or movements deviate substantially from geometric correctness. Lines\\nor shapes may be misplaced, disconnected, or moved in a chaotic or discontinuous manner\\n(e.g., jittering, overlapping, or distorted paths), leading to visual instability and loss of\\ninterpretability.\\nData Source.\\nThe evaluation data are drawn from multiple established sources, including the\\nGeo170k dataset [18], the VarsityTutors subset of Math-PUMA [85] dataset, the line-connection\\nsubset of RBench-V [25], the MAVIS-Gen [80], Tangram Puzzle subsets of the STARE [43] benchmark,\\nand data from VAT [46].\\nExample and Analysis.\\nThe representative examples of the 2D geometry reasoning task are\\npresented in Figures 9 and 10. Veo-3 demonstrates a foundational capability for simple geometric\\nconnection tasks, correctly identifying and linking elements in straightforward scenarios like in case\\nIII. However, this basic competence is inconsistent. The model often prioritizes producing visually\\nsymmetric or semantically meaningful patterns rather than strictly adhering to geometric instructions\\n(cases I and II). Furthermore, case II reveals instances where the model unintentionally modifies the\\noriginal figures, indicating a limited awareness of geometric constraints and poor spatial consistency.\\nWhen tackling more complex connection tasks, the model frequently fails to interpret the intended\\ndrawing order or point indices, resulting in incorrect connection sequences, as demonstrated in cases\\nV, VI, and VII. This is often coupled with an inability to control task termination, as the model tends\\nto continue drawing beyond the required constructions. Finally, for tasks involving the movement\\nof geometric shapes in cases IV and VIII, the model struggles to maintain geometric structural\\nconsistency throughout the motion.\\nTakeaway 6\\nVeo-3 shows initial 2D geometric reasoning ability but still falls short of consistent, constraint-\\naware geometric understanding, remaining far from a robust geometric reasoner.\\n13\\n\\x0cInput Image:\\nReasoning Video:\\nA line connecting point A and point C. The\\nvideo ends once the connection process is\\ncomplete. Static view, no zoom or pan.\\nI. Question:\\nText-to-Video Prompt:\\nQ: In the figure shown, let \\'n\\' represent the length of side AB of the inscribed\\nrectangle ABCD, where n is an undetermined value. With BC equal to 6.0\\nand the diameter of circle O equal to 10.0, what is the value of \\'n‚Äô?\\nA: 8\\n1st frame\\nInput Image:\\nReasoning Video:\\nII. Question:\\nText-to-Video Prompt:\\n1st frame\\nIII. Question:\\nText-to-Video Prompt:\\nIV. Question:\\n~ Moderate\\nSuccess Rate: 83%\\nSmoothly\\nconnecting\\npoint\\nM and point N. The video\\nends\\nonce\\nthe\\nconnection\\nprocess is complete. Static\\nview, no zoom or pan.\\nQ: The figure presented depicts a square designated as ABCD. Within this square, point\\nM is identified as the midpoint of the side AB, while point N is the midpoint of the\\nopposing side CD. Additionally, point O is located at the midpoint of segment CN. Your\\ntask is to draw the segment MO. It is given that the length of segment AM is represented\\nby t. The objective is to determine which of the following expressions accurately\\nrepresents the length of the segment MO in terms of t.\\nA: 17\\n4 ùë°\\nInput Image:\\nReasoning Video:\\n1st frame\\nSmoothly connecting point C and point D with a line. The video\\nends once the connection process is complete. Static view, no\\nzoom or pan.\\nQ: AB equals to 8.0. What would the area of \\nthe entire shape ABCD be?\\nA: 62.87\\nText-to-Video Prompt:\\nInput Image:\\nReasoning Video:\\n1st frame\\nPlace piece A with its upper-\\nleft corner at (x, y) = (0, 3).\\nQ: Check out an Tangram puzzle below. The left panel is an empty Tangram puzzle,\\nwhile the right panel shows available pieces to complete the puzzle. Keep in mind\\nthat you can rotate or flip the pieces. Can the Tangram puzzle be completed with\\nthe available pieces, yes or no?\\nA: Yes.\\n‚úóBad\\n‚úóBad\\n~ Moderate\\nSuccess Rate: 33%\\nFigure 9: Showcase of 2D Geometry Reasoning by Veo-3 (Part I). While Veo-3 shows potential in\\nrecognizing simple patterns, it lacks the robust constraint awareness essential for accurate geometric\\nmanipulation.\\n14\\n\\x0cInput Image:\\nReasoning Video:\\nAnimate the dots connecting sequentially from 1 to\\n25, each straight line appearing smoothly until the\\nfull outline emerges. Keep the background with the\\nsmiling sun and plants unchanged. Static shot.\\nV. Question:\\nText-to-Video Prompt:\\nQ: Connect the black dots in the image sequentially with straight\\nlines according to the edge numbers (i.e., connect dot 1 to dot 2,\\ndot 2 to dot 3, and so on). The final result will form a simple line\\ndrawing. What does this drawing represent?\\nA: Duck.\\n1st frame\\nInput Image:\\nReasoning Video:\\nVI. Question:\\nText-to-Video Prompt:\\n1st frame\\nVII. Question:\\nText-to-Video Prompt:\\n~ Moderate\\nSuccess Rate: 83%\\nInput Image:\\nReasoning Video:\\n1st frame\\n‚úóBad\\nAnimate\\nthe\\nnumbered\\ndots\\nconnecting\\nsequentially from 1 to 118, each straight line\\nappearing\\nsmoothly\\nas\\nthe\\noutline\\ngradually\\nemerges. Keep all numbers and dots visible while\\nthe connecting lines form step by step. Static shot.\\nQ: Connect the black dots in the image sequentially with straight\\nlines according to the edge numbers (i.e., connect dot 1 to dot 2,\\ndot 2 to dot 3, and so on). The final result will form a simple line\\ndrawing. What does this drawing represent?\\nA: Lion.\\nAnimate\\nthe\\nnumbered\\ndots\\nconnecting\\nsequentially from 1 to 118, each straight line\\nappearing\\nsmoothly\\nas\\nthe\\noutline\\ngradually\\nemerges. Keep all numbers and dots visible while\\nthe connecting lines form step by step. Static shot.\\nQ: Connect the black dots in the image sequentially with straight\\nlines according to the edge numbers (i.e., connect dot 1 to dot 2,\\ndot 2 to dot 3, and so on). The final result will form a simple line\\ndrawing. What does this drawing represent?\\nA: Bird.\\n‚úóBad\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nPan vertically at a steady speed to center the top yellow line, then the bottom one, keeping \\nboth visible with identical scale and exposure. Ensure all elements stay unchanged and finish \\nwith a full view showing both lines together.\\nVIII. Question:\\nText-to-Video Prompt:\\nQ: Is the top yellow line \\nshorter than the bottom \\nyellow line?\\nA: No.\\n1st frame\\n‚úóBad\\nFigure 10: Showcase of 2D Geometry Reasoning by Veo-3 (Part II). Veo-3‚Äôs reasoning abilities are\\nfurther challenged by complex sequential instructions and the need to preserve structural integrity.\\n15\\n\\x0cInput Image:\\nReasoning Video:\\nShow the rough semicircular track with\\nheight label h and a block at P; release it,\\nadd faint friction streaks as it slides down\\nand up the right side, stopping below the\\nrim.\\nShow\\nthe\\nmove\\nquickly\\nand\\ncompletely. Static shot.\\nI. Question:\\nText-to-Video Prompt:\\nQ: The figure shows a rough semicircular track whose ends are at a\\nvertical height h. A block placed at point P at one end of the track is\\nreleased from rest and slides past the bottom of the track. Which of the\\nfollowing is true of the height to which the block rises on the other side\\nof the track?\\nA: It is between zero and h; the exact height depends on how much\\nenergy is lost to friction.\\n1st frame\\nInput Image:\\nReasoning Video:\\nII. Question:\\nText-to-Video Prompt:\\n1st frame\\nIII. Question:\\nText-to-Video Prompt:\\nIV. Question:\\nAnimate the red ball moving along the blue\\narrow\\'s direction, bouncing off\\nthe black\\nwalls according to reflection rules, keeping\\nspeed consistent. Continue its path upward\\nuntil it reaches and collides with one of the\\nnumbered top bricks. Static shot.\\nQ: The red ball moves in the direction indicated by the blue arrow\\nand bounces off the black side walls upon collision; the component\\nof its velocity perpendicular to the wall reverses in direction but\\nmaintains its magnitude, while the component parallel to the wall\\nremains unchanged. Based on this behavior, please estimate which\\nnumbered brick (from 1 to 10) at the top the red ball will hit first.\\nA: 1.\\nInput Image:\\nReasoning Video:\\n1st frame\\nDynamically\\ndepict\\nthe\\nattraction\\nbetween\\nmagnets,\\npaying\\nattention to speed and intensity. Static shot.\\nQ: Think about the magnetic force between\\nthe magnets in each pair.\\nA: The magnetic force is stronger in Pair 2.\\nText-to-Video Prompt:\\nInput Image:\\nReasoning Video:\\n1st frame\\nThe orange gear rotates counterclockwise in the given view.\\nAnimate the provided planetary gear system. The orange gear is\\nfixed on the green gear. The central orange sun gear rotates\\ncounterclockwise, driving the yellow planet gear. All components\\nmust maintain their relative axial positions and proper gear\\nmeshing. The camera is static, with no zoom or pan.\\nQ: The orange gear is fixed on the\\nstationary green gear. If the orange\\ngear rotates counterclockwise in the\\ngiven view, what is the motion of the\\nyellow gear relative to the orange gear?\\nA: Clockwise rotation.\\nCounterclockwise revolution.\\n‚úóBad\\n‚úóBad\\n‚úóBad\\n~ Moderate\\nSuccess Rate: 83%\\nFigure 11: Showcase of Physics-based Reasoning by Veo-3. The physics scenarios demonstrate lo-\\ncally plausible dynamics and reflections, alongside systematic quantitative and causal inconsistencies\\nunder frictional, force-driven, or constrained interactions.\\n16\\n\\x0c2.7\\nPhysics-based Reasoning\\nTask Description and Evaluated Aspects.\\nThe physics-based reasoning category assesses a\\nmodel‚Äôs capacity to depict and reason about motion dynamics, physical causality, and rule-based\\ninteractions between objects. Tasks in this group involve gravity, collisions, reflection, momentum, or\\nenergy conservation, requiring the model to generate physically plausible and temporally coherent\\nmotion. Evaluation focuses on two complementary aspects: (i) physical plausibility, which means\\nwhether the simulated motion obeys common physical principles; and (ii) causal correctness, which is\\nwhether object interactions are consistent with the underlying cause-and-effect relationships described\\nin the prompt.\\nDefinition of Good / Moderate / Bad.\\nWe rate the performance according to the following criteria:\\n‚úìGood: The motion sequence adheres to physical laws such as gravity, momentum, and\\nenergy conservation. Object interactions are realistic and temporally smooth, and the visual\\noutcome remains coherent and credible throughout.\\n~ Moderate: The physical relations are approximately correct but include minor inconsisten-\\ncies, such as irregular acceleration, timing mismatch, or slight violation of conservation. The\\noverall motion remains interpretable and visually plausible.\\n‚úóBad: The motion is physically implausible or visually chaotic‚Äîobjects float, stop abruptly,\\nor behave contrary to basic causal principles. Severe artifacts or temporal discontinuities\\ndisrupt the perception of a coherent physical process.\\nData Source.\\nWe draw samples from MMMU [77], ScienceQA [49], and related physical reasoning\\nsubsets of RBench-V [26] and SpatialViz-Bench [66], covering scenarios such as object collisions,\\npendulum motion, frictional sliding, and optical or magnetic interactions.\\nExample and Analysis.\\nFigure 11 presents four representative physics tasks and their outputs.\\nCase I shows that the model can produce a visually coherent slide, but the behavior violates basic\\nphysical laws. Case II is the most reliable, where reflections and general trajectory shape are rendered\\nplausibly and the task attains a high success rate, although small angular or timing offsets are common.\\nIn case III, the model conveys attraction through motion, yet the depicted dynamics do not reliably\\ntrack the intended force magnitudes or causal ordering. Finally, case IV exposes structural failures,\\nincorrect meshing, inconsistent relative rotations, and nonphysical contact behavior occur frequently,\\nso the mechanical constraints are not respected. Overall, the model can synthesize locally plausible\\ndynamics and handle simple reflection rules, but it fails to maintain quantitative physical constraints\\nand causal fidelity in frictional, force-driven, or mechanically constrained scenarios.\\nTakeaway 7\\nVeo-3 often generates visually plausible short-term dynamics, but it systematically fails to\\npreserve quantitative physical constraints (energy, momentum), causal ordering, and contact\\nmechanics in frictional, force-driven, or mechanically constrained scenarios. Thus, its outputs\\nare somewhat useful for qualitative illustration but are not reliable for quantitative physics\\ninference or causal prediction.\\n2.8\\nRotation Reasoning\\nTask Description and Evaluated Aspects.\\nThe rotation reasoning task assesses the ability to\\nreason about planar object rotation and maintain consistent spatial grounding under rotational\\ntransformations, thereby supporting subsequent reasoning processes. In each instance, the model is\\nrequired to accurately rotate target objects within a fixed 2D plane while preserving the overall scene\\nstructure and structural consistency, followed by performing reasoning tasks like grounding and OCR.\\nThe evaluation focuses on both the accuracy of the rotation in terms of angle and direction, and the\\nprecision of the resulting reasoning tasks.\\n17\\n\\x0cInput Image:\\nReasoning Video:\\nI. Question:\\nText-to-Video Prompt:\\n1st frame\\nInput Image:\\nReasoning Video:\\nRotate the scene 180 degrees clockwise. Then draw a bounding \\nbox around the leftmost vending machine.\\nII. Question:\\nText-to-Video Prompt:\\nQ: Looking up from the floor, how many rows\\nof drinks are in the leftmost vending machine?\\nA: 2\\n1st frame\\nInput Image:\\nReasoning Video:\\n‚úóBad\\nThe entire \\'Original\\' grid figure performs one smooth, continuous 360-degree \\nrotation clockwise within its own 2D plane. The camera stays static, with no pan.\\nIV. Question:\\nText-to-Video Prompt:\\nQÔºöWhich grid can be obtained \\nby rotating the grid only?\\n1st frame\\n‚úóBad\\nRotate the scene 45 degrees clockwise. Then draw bounding boxes around the\\nfrontmost skiing character.\\nQ: Is the frontmost skier\\nwearing a scarf?\\nA: No.\\nInput Image:\\nReasoning Video:\\nIII. Question:\\nText-to-Video Prompt:\\n1st frame\\n‚úóBad\\nRotate the video frame 90 degrees counterclockwise in the 2D\\nplane, then draw bounding boxes around each \\'IKEA\\' label.\\nQ: On which floors are the \\'IKEA\\' labels located?\\nA: One on the top floor, one on the middle floor, \\nand one on the bottom floor.\\n~ Moderate\\nSuccess Rate: 83%\\nA: A\\nFigure 12: Showcase of Rotation Reasoning by Veo-3. Veo-3 struggles in complex scenes. However,\\nits foundational grasp of simple rotations signals its potential to support rotation-based reasoning\\ntasks.\\n18\\n\\x0cDefinition of Good / Moderate / Bad.\\nModel outputs are categorized into three quality levels:\\n‚úìGood: The rotation is accurate, complete, and strictly confined to the 2D plane, with no\\nextraneous scene motion. The following reasoning tasks are completed correctly. Target\\nobjects remain precisely grounded after rotation.\\n~ Moderate: The rotation is largely correct but may be incomplete or slightly off-angle,\\nthough still confined to the 2D plane. The following reasoning tasks are mostly completed.\\nMinor temporal or visual inconsistencies may appear, but do not alter the core 2D structure or\\nobject grounding.\\n‚úóBad: The model fails to perform the correct rotation, extends the transformation into 3D\\nspace, or introduces substantial scene distortion. Cannot complete the following reasoning\\ntask. The original 2D structure is altered, leading to inaccurate grounding of the target objects.\\nData Source.\\nTo specifically assess the rotation reasoning task, we recruit some PhD-level experts\\nwith deep expertise in text-image reasoning to design the evaluation data manually, followed by the\\nnecessary review process, as mentioned in Section 3.2. Each question is designed following the\\nprinciple that it must involve a 2D rotation to reach the correct solution, ensuring the task genuinely\\nprobes rotational understanding rather than simple visual matching. Moreover, we sample data from\\nthe 2DRotation subset from the SpatialViz-Bench [66], and reformulate the question into instructions\\nfor the video models.\\nExample and Analysis.\\nThe results are shown in Figure 12. In case I, we find that Veo-3 handles\\nsmall-angle rotations and simple planar scenes reasonably well, demonstrating a basic grasp of\\nrotational motion. However, in more complex scenarios like cases II, III, and IV, the model often\\nignores the 2D rotation constraint and inadvertently alters the 3D structure, resulting in incorrect\\nrotations and degraded spatial grounding. Such errors frequently propagate to downstream tasks, such\\nas OCR in case III, or object localization in case II, due to inconsistencies in post-rotation alignment.\\nThese observations suggest that the reasoning behavior of Veo-3 remains more pattern-driven rather\\nthan principle-driven. However, as it demonstrates a partial understanding of planar rotation, this can\\nto some extent facilitate subsequent reasoning tasks.\\nTakeaway 8\\nVeo-3 exhibits only a superficial understanding of rotation reasoning. While it can approximate\\nsmall planar rotations, it fails to preserve geometric consistency under larger or compound\\ntransformations.\\n2.9\\nTable and Chart Reasoning\\nTask Description and Evaluated Aspects.\\nThe table and chart reasoning task requires the model\\nto identify and focus on the key elements within visualizations or tabular data. For evaluation, we\\nfurther consider how effectively the model identifies the regions relevant to the query and whether\\nit can transition smoothly and visually coherently to these areas, preserving clarity, continuity, and\\nproper scaling.\\nDefinition of Good / Moderate / Bad.\\nWe rate the performance according to the following criteria:\\n‚úìGood: Camera precisely focuses on the correct chart or table segment, smoothly high-\\nlighting or zooming into the queried data (e.g., correct year, category, or value). Motion is\\ncontinuous, the chart and table remain clear, and no distortion or overexposure occurs.\\n~ Moderate: Camera approximately focuses on the right region but partially misses boundaries,\\nintroduces slight blur, or transitions abruptly. Data can still be inferred.\\n‚úóBad: Video fails to locate the correct region or changes the chart or table geometry\\nunnaturally. Motion jitter, scaling errors, or artifacts make data unreadable or misleading.\\n19\\n\\x0cInput Image:\\nReasoning Video:\\nStart with smoothly zooming in to focus on the \\'Nova Scotia\\' row. Then,\\nsmoothly zoom out to the full view of the chart. End with smoothly zooming\\nin to focus on the \\'Manitoba\\' row. The chart itself, including all its data, lines,\\nand labels, must remain completely static and unchanged throughout the video.\\nI. Question:\\nText-to-Video Prompt:\\nQ: What is the sum of footwear\\nmanufacturing establishments in\\nNova Scotia and Mantioba as of\\nDecember 2020?\\nA: 3\\n1st frame\\nInput Image:\\nReasoning Video:\\nII. Question:\\nText-to-Video Prompt:\\n1st frame\\nIII. Question:\\nText-to-Video Prompt:\\nIV. Question:\\nStart with a static, full view of the chart. Then, smoothly zoom the camera in to focus on\\nthe vertical area corresponding to the year 2014. The chart itself, including all its data,\\nlines, and labels, must remain completely static and unchanged throughout the video.\\nQ: In the year 2014, which\\nopinion is dominant?\\nA: Unfavorable.\\nInput Image:\\nReasoning Video:\\n1st frame\\nZoom in to focus on the smallest section in the chart. The chart itself, including all its data,\\nlines, and labels, must remain completely static and unchanged throughout the video.\\nQ: What\\' the color of \\nsmallest section in the chart?\\nA: Gray.\\nText-to-Video Prompt:\\nInput Image:\\nReasoning Video:\\n1st frame\\nDraw a bounding box around the end market for the Engineered\\nSystems segment. The table itself, including all its text, lines, and labels,\\nmust remain completely static and unchanged throughout the video.\\nQ: What is the end market for the \\nEngineered Systems segment?\\nA: Printing & Identification, Industrials.\\n‚úóBad\\n~ Moderate\\nSuccess Rate: 83%\\n‚úóBad\\n‚úóBad\\nFigure 13: Showcase of Table and Chart Reasoning by Veo-3. Veo-3 demonstrates an initial ability\\nto focus on relevant data regions but lacks the precision and consistency required for reliable visual\\nanalysis.\\n20\\n\\x0cData Source.\\nWe use samples from the ChartQA [52] dataset and TableVQA-Bench [34].\\nExample and Analysis.\\nFor charts, as presented in cases I, II and III in Figure 13, Veo-3 can often\\nzoom into an approximately correct region but lacks the precision needed to accurately locate the\\nqueried data. For tables, as shown in case IV, Veo-3 fails to correctly identify the required element\\nand tends to select entries randomly. The model also frequently adds, modifies, or distorts existing\\nchart and table elements, resulting in visual inconsistencies that undermine the accuracy of chart\\ninterpretation.\\nTakeaway 9\\nVeo-3 demonstrates emerging competence and potential in structured visual understanding, but\\nstill falls short of functioning as a precise and reliable chart-table reasoner.\\n2.10\\nObject Counting Reasoning\\nTask Description and Evaluated Aspects.\\nIn this category, we focus on the ability to accurately\\nenumerate objects within a 2D or 3D scene. In each instance, the model is required to identify, ground,\\nand count target objects, typically by highlighting, drawing bounding boxes, applying numerical\\nlabels, or panning. The evaluation focuses on the accuracy of the count and the precision of the\\nspatial grounding, performed within a scene that remains static or experiences only minimal motion,\\nensuring the counting process is not influenced.\\nDefinition of Good / Moderate / Bad.\\nModel outputs are categorized into three quality levels:\\n‚úìGood: The model precisely highlights, draws bounding boxes around, or labels the objects\\nwith correct numbers, and performs smooth and controlled panning when necessary to cover\\nall targets. Motion is continuous, and the scene remains static or experiences only slight\\nchanges that do not influence the counting process.\\n~ Moderate: The model approximately highlights or draws bounding boxes around the objects,\\nor performs panning with minor instability or incomplete coverage. Objects or the scene may\\nmove or change slightly, but this does not strongly affect the counting process.\\n‚úóBad: The model fails to correctly highlight, label, or draw bounding boxes around the\\nobjects, or pans erratically such that parts of the scene are missed or revisited unnecessarily.\\nObjects or the scene move or change substantially, severely affecting the counting process.\\nData Source.\\nThe 2D object counting data are sampled from the counting subset of RBench-V [25].\\nThe 3D object counting data are from the Super-CLEVER dataset [45] and VAT [46].\\nExample and Analysis.\\nThe results are shown in Figures 14 and 15. In the 2D counting tasks from\\ncases I to III, objects frequently move or change during the process, negatively impacting counting\\nstability and accuracy. In the 3D counting tasks, Veo-3 successfully handles simple grounding and\\ncounting scenarios, as demonstrated in case V, but struggles with scenes involving complex materials\\nor geometric variations in cases VI and VII, leading to inaccurate counts. Additionally, in the panning\\nprocess of case VII, the camera fails to precisely move to the regions containing all target objects,\\nfurther hindering the counting process.\\nTakeaway 10\\nVeo-3 demonstrates basic counting capability but lacks the spatial control and robustness\\nrequired for reliable object enumeration in dynamic or complex scenes.\\n21\\n\\x0ccropped_0.jpg\\nInput Image:\\nReasoning Video:\\nA scanner dot moves along the black line from bottom-left to top-right. As soon as this\\ndot enters a new grid square, that entire square is instantly filled with yellow color and\\nstays yellow. A square only turns yellow if the scanner dot on the line has entered it.\\nStatic camera, no zoom.\\nI. Question:\\nText-to-Video Prompt:\\nQ:\\nHow\\nmany\\nunit\\nsquares\\ndoes\\nthe\\nline\\nsegment pass through in\\nthe given grid diagram?\\nA: 16\\n1st frame\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nHighlight only the rectangles in the figure with a bright yellow color. Not highlight \\nany other shapes like squares, triangles, circles, or irregular polygons. Static camera, \\nno zoom, no pan.\\nII. Question:\\nText-to-Video Prompt:\\nQ: How many rectangles \\nare there in the figure?\\nA: 8\\n1st frame\\n‚úóBad\\n‚úóBad\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nLabel all the fish with increasing numbers (1, 2, 3, ...). The fish keep static. Static \\ncamera, no zoom, no pan.\\nIII. Question:\\nText-to-Video Prompt:\\nQ: How many rectangles \\nare there in the figure?\\nA: 18\\n1st frame\\n‚úóBad\\nFigure 14: Showcase of 2D Object Counting Reasoning by Veo-3. Veo-3‚Äôs lack of spatial control\\noften introduces object motion, undermining the stability and accuracy of the counting process.\\n2.11\\nGUI Reasoning\\nTask Description and Evaluated Aspects.\\nIn the Graphical User Interface (GUI) reasoning task,\\nwe focus on the capability to understand and interact with graphical user interfaces across different\\noperating systems, including Android, Linux, and Web environments. In each instance, the model is\\nrequired to perform actions, such as clicking on specific UI elements. The evaluation focuses on the\\naccuracy of the click and the temporal coherence of the interaction, ensuring the scene and irrelevant\\nUI elements remain consistent.\\nDefinition of Good / Moderate / Bad.\\nWe define the evaluation criteria in three levels:\\n‚úìGood: The click is precise, with no extraneous actions. No superfluous icons appear, and\\nthe original data and icons remain unchanged.\\n~ Moderate: The click is precise but may be accompanied by minor extraneous actions.\\nSuperfluous icons might appear but do not obscure the click target, and original data or icons\\nshow only slight alterations.\\n22\\n\\x0ccropped_0.jpg\\nInput Image:\\nReasoning Video:\\nDraw bounding boxes around the brown metal mountain bikes to the right of \\nthe origami crane. Static shot.\\nV. Question:\\nText-to-Video Prompt:\\nQ: There is a small yellow object that is \\nto the left of the tiny metal motorbike; \\nhow many brown metal mountain bikes \\nare to the right of it?\\nA: 1\\n1st frame\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nDraw bounding boxes around any matte tandem bikes and metal cruisers present in the \\nscene. Static shot.\\nVI. Question:\\nText-to-Video Prompt:\\nQ: How many cyan things \\nare matte tandem bikes or \\nmetal cruisers?\\nA: 1\\n1st frame\\n‚úìGood\\nSuccess Rate: 100%\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nPan smoothly to include both the lid‚Äìbody interface and the spout or cap in view at a fixed \\nscale, keeping exposure steady and avoiding any visual or geometric changes.\\nVII. Question:\\nText-to-Video Prompt:\\nQ: How many burners are \\non the stove?\\nA: 4\\n1st frame\\n~ Moderate\\nSuccess Rate: 17%\\n‚úìGood\\nSuccess Rate: 33%\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nDraw bounding boxes around the tiny things that have the same material as the green \\nmotorbike. Static shot.\\nIV. Question:\\nText-to-Video Prompt:\\nQ: How many tiny things \\nhave the same material as \\nthe green motorbike?\\nA: 1\\n1st frame\\n‚úóBad\\nFigure 15: Showcase of 3D Object Counting Reasoning by Veo-3. Veo-3‚Äôs basic 3D counting\\nabilities are challenged by complex materials, geometric variations, and imprecise camera control.\\n23\\n\\x0ccropped_0.jpg\\nInput Image:\\nReasoning Video:\\n \\n \\nClick the pkgs folder to collapse it. Static shot.\\nI. Question:\\nText-to-Video Prompt:\\nQ: Collapse the pkgs folder.\\nA: \\n1st frame\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\n \\nClick the calendar icon located to the right of the flight date options, \\nnext to the price display for June 6. Static shot.\\nII. Question:\\nText-to-Video Prompt:\\nQ: A calendar icon \\nlocated to the right of \\nthe flight date options, \\nnext to the price \\ndisplay for June 6.\\n1st frame\\n‚úó Bad\\n  \\n‚úó Bad\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\n \\nClick the navigation arrow located at the right edge of the browse by category \\ncarousel. Static shot.\\nIII. Question:\\nText-to-Video Prompt:\\nQ: A navigation \\narrow located at the \\nright edge of the \\nbrowse by category \\ncarousel.\\n1st frame\\n  \\n‚úó Bad\\nA:\\nA:\\nFigure 16: Showcase of GUI Reasoning by Veo-3. Veo-3‚Äôs attempts at graphical interface interaction\\nexhibit visual inconsistencies and logical inaccuracies, indicating only a shallow grasp of underlying\\nGUI logic. Note that the answer to each question is a bounding box. For visual clarity, screenshots\\nwith the ground-truth bounding boxes are shown.\\n‚úóBad: The click is imprecise or erratic. Original data and icons are significantly altered,\\nhindering judgment and assessment.\\nData Source.\\nThe Linux data are selected from the Common Linux Screenshot subset of ScreenSpot-\\nPro [42], while the Android and Web data are drawn from the OS Android and OS Web subsections\\nof MMBench-GUI [67], respectively.\\nExample and Analysis.\\nAcross the three cases in Figure 16, Veo-3 fails to accurately capture the\\ncorrect click position and often exhibits inconsistencies between the click location and the resulting\\non-screen effect. In addition, it occasionally alters or generates new icons and text, which can\\ninterfere with judgment. In the Web system in case III, however, the model demonstrates partial GUI\\nresponsiveness and provides some degree of visual feedback.\\n24\\n\\x0ccropped_0.jpg\\nInput Image:\\nReasoning Video:\\nPan to the banana while keeping tray edges in view. Fix scale (banana ~two-thirds of \\nthe frame, axis horizontal). Sweep once along the inner concave edge from stem to tip \\nat constant speed, then stop and hold at its midpoint.\\nI. Question‚Ä†:\\nText-to-Video Prompt:\\nQ: Which point \\ncorresponds to \\nthe affordance for \\nmanipulating the \\nbanana?\\n1st frame\\n~ Moderate\\nSuccess Rate: 33%\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nKeep the cucumber‚Äôs start and the pot opening in view. Sweep once from start to pot \\nat fixed scale and speed, briefly dwelling at four evenly spaced waypoints (p1‚Üíp4) \\nalong the path, then hold on both endpoints.\\nII. Question‚Ä†:\\nText-to-Video Prompt:\\nQ: Which set of 4 \\npoints is a right \\ntrajectory when \\ndoing place a \\ncucumber into a pot?\\n1st frame\\n‚úóBad\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nPan smoothly to include both the lid‚Äìbody interface and the spout or cap in view at a fixed \\nscale, keeping exposure steady and avoiding any visual or geometric changes.\\nIII. Question‚Ä†:\\nText-to-Video Prompt:\\nQ: Is the container sealed?\\nA: No.\\n1st frame\\n~ Moderate\\nSuccess Rate: 17%\\nA:\\nA:\\nFigure 17: Showcase of Embodied Reasoning by Veo-3. It illustrates plausible static affordance\\ndetection in simple settings, common workaround/hallucination behaviors for dynamic manipulations,\\nand failures to reliably localize or preserve manipulation-relevant context. ‚Ä† Green points in the\\nanswer image denote ground-truth points or trajectories.\\nTakeaway 11\\nVeo-3 demonstrates a limited awareness of GUI click actions, imitating interaction behaviors\\nwithout fully grasping the underlying functional logic.\\n2.12\\nEmbodied Reasoning\\nTask Description and Evaluated Aspects.\\nThis category evaluates the model‚Äôs potential to perceive\\nand reason about object affordances and manipulation dynamics. It involves recognizing both static\\nand dynamic affordances, as well as identifying manipulation-relevant object and scene attributes.\\nEvaluation focuses on two aspects: (i) the generation of stable and contextually relevant visual\\nsequences, and (ii) the maintenance of reasoning fidelity without resorting to implausible planning\\nshortcuts or hallucinated interactions.\\n25\\n\\x0cDefinition of Good / Moderate / Bad.\\nWe define the evaluation criteria in three levels:\\n‚úìGood: The sweep/framing covers all candidates fairly (equal or near-equal dwell), centers\\nthe manipulation-relevant geometry (e.g., handle + frame/gap, lid-body interface, hinge side)\\nwith crisp focus and stable scale; no cropping of key context; no content alterations.\\n~ Moderate: The view roughly includes the right region(s) but with minor bias or coverage\\nissues: slight off-center, brief under-exposure of one candidate, small motion jitter, or shallow\\ncontext (still enough to infer).\\n‚úóBad: The camera misses or biases the evidence (e.g., lingers only on one point, crops\\naway the hinge/rail, over-zooms a non-relevant patch), introduces distortion/content edits, or\\nproduces footage from which a fair decision cannot be made.\\nData Source.\\nWe select samples from Robobench [51] for the analysis. In addition to a general\\nunderstanding of static attributes, we also sample data to assess whether Veo-3 can perform direct\\nreasoning on tasks involving the generation of static and dynamic affordances.\\nExample and Analysis.\\nAs shown in Figure 17, Veo-3 demonstrates the ability to comprehend\\nobjects within real-world scenes. However, its capacity for assisting visual reasoning in embodied\\nscenarios remains constrained by insufficient stability. As illustrated in case I, when provided with a\\nclearly defined object for manipulation, Veo-3 is capable of generating plausible manipulation affor-\\ndances. When it comes to dynamic affordances, Veo-3 tends to employ workarounds to compensate\\nfor its planning deficiencies, as evidenced in case II, where it generated a new cucumber instead of\\nthe intended object. With respect to static attributes, Veo-3 struggles to accurately differentiate visual\\nprompts and misidentifies the position of containers. As shown in case III, the green box, intended to\\nspecify the location of the container, inadvertently led Veo-3 to produce hallucinations.\\nTakeaway 12\\nVeo-3‚Äôs capabilities are currently limited to basic object recognition rather than true embodied\\nreasoning. It lacks the necessary planning and stability to reliably interpret and act upon\\ndynamic or spatially constrained instructions, indicating its limitations in understanding and\\nreasoning of real-world interactions.\\n2.13\\nMedical Reasoning\\nTask Description and Evaluated Aspects.\\nThis category assesses the model‚Äôs ability to localize\\nlesions or structures, identify relevant attributes (e.g., side, lobe), recognize pathological patterns\\n(e.g., ‚Äújump distribution‚Äù), and make binary decisions (e.g., presence or absence). The evaluation\\nfocuses on both the correctness of object manipulation and the visual stability of the surrounding\\nregions.\\nDefinition of Good / Moderate / Bad.\\nWe define the evaluation criteria in three levels:\\n‚úìGood: The camera cleanly settles on the correct anatomical level/lesion, with clear margins\\nand readable context; motion is reasonable; no geometric distortion or content alteration.\\n~ Moderate: The view roughly covers the right area but is slightly off (partial coverage, mild\\nblur, small framing mistakes). The general shape of the tissue or organ can still be observed.\\n‚úóBad: The video misses the target region or introduces distortions/crops that hide key cues.\\nTissues or organs begin to distort. Misleading results due to confusion of medical terminology.\\nData Source.\\nWe select samples representing different body parts from the ViTAR [9] dataset.\\nExample and Analysis.\\nWe showcase the evaluation results in Figure 18. Veo-3 retains the\\nability to manipulate images when dealing with medical images. However, due to its lack of medical\\n26\\n\\x0ccropped_0.jpg\\nInput Image:\\nReasoning Video:\\nShow the full sagittal lumbar view, then sweep smoothly from L1 to S1 at constant speed \\nwithout pausing. End on a view showing adjacent disc spaces, including narrow and normal \\nlevels. Keep image content and geometry unchanged.\\nI. Question:\\nText-to-Video Prompt:\\nQ: What is the distribution \\npattern of stenotic segments?\\nA: Jump distribution.\\n1st frame\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nShow the full PA chest view, then adjust framing to include both the heart silhouette and\\nthe widest inner thoracic diameter at a fixed scale. Keep contrast and geometry\\nunchanged, holding steady for visual CTR estimation.\\nII. Question:\\nText-to-Video Prompt:\\nQ: Is cardiomegaly presentÔºü\\nA: No.\\n1st frame\\n‚úóBad\\ncropped_0.jpg\\nInput Image:\\nReasoning Video:\\nShow the full axial CT, then pan and zoom smoothly to the right lung so the nodule and \\nnearby fissure appear together. Keep windowing standard and geometry unchanged.\\nIII. Question:\\nText-to-Video Prompt:\\nQ: Which lobe contains \\nthe pulmonary nodule?\\nA: Left lobe.\\n1st frame\\n‚úóBad\\n‚úóBad\\nFigure 18: Showcase of Medical Reasoning by Veo-3. As shown in cases I and III, Veo-3 fails to\\nmaintain the shape of the rest of medical organization. Veo-3 also can not understand and precisely\\nlocate the mentioned medical terminology in the prompt, as demonstrated in case II.\\nknowledge, Veo-3 struggles to accurately manipulate the correct objects when instructions include\\nmedical terminology. This phenomenon is evident across all cases. Furthermore, Veo-3 cannot model\\nmedical organs effectively. When performing operations such as zooming in, the medical images\\nsuffer from significant distortion, resulting in a substantial loss of detail.\\nTakeaway 13\\nVeo-3‚Äôs failure to handle the reasoning in the medical domain, causing distortion even on simple\\nzoom-ins, highlights its limited grasp of specialized, non-general knowledge.\\n27\\n\\x0cMME-COF\\nFigure 19: Category Distribution.\\nTable 1: Key Statistics of MME-COF.\\nStatistic\\nNumber\\nTotal entries\\n59\\nTotal categories\\n12\\nMax prompt length\\n124\\nAvg prompt length\\n36.7\\nMax entries per category\\n7\\nAvg entries per category\\n4.9\\n3\\nMME-COF\\n3.1\\nBenchmark Overview\\nTo standardize the empirical study and systematically evaluate the reasoning potential of state-of-the-\\nart generative video models [21, 55, 56], we introduce MME-COF, which, to our knowledge, is the\\nfirst benchmark specifically designed to reveal and quantify the reasoning potential of video models.\\n3.2\\nBenchmark Composition\\nData Curation and Distribution.\\nAligning with the task taxonomy in Section 2.1, the MME-COF\\nbenchmark is curated from the cases used in our empirical study. It comprises 59 curated entries and\\ninstruction prompts spanning 12 diverse reasoning categories. The key statistics of MME-COF and\\nits overall composition are summarized in Table 1, Figure 2b and Figure 19.\\nReview Process.\\nFollowing the prompt design protocol in Section 2.1, all prompts undergo a\\ntwo-stage review process. In the cross-validation phase, each prompt was independently reviewed by\\nanother expert to ensure semantic clarity, alignment with the intended reasoning task, and the absence\\nof linguistic bias. In the final adjudication phase, discrepancies were jointly discussed and resolved\\nthrough consensus. This multi-step procedure ensured that every prompt was conceptually precise,\\nvisually grounded, and fully aligned with the evaluation objectives of MME-COF.\\n3.3\\nEvaluation Protocol\\nModels and Generation Settings.\\nWe evaluate the leading video models in a zero-shot setting,\\nincluding Kling-v1 [38], Seedance-1.0-pro [19], Veo-3.0-preview [70], Veo-3.0-fast [70], Sora-2 [56],\\nSora-2-pro [56]. Each model generates six video samples per prompt, and final scores were computed\\nas the mean across all samples. All videos are generated at a 16:9 aspect ratio. We adopt the\\ndefault 8-second duration for the Sora and Veo series, while retaining the default 5-second length for\\nKling and Seedance. Note that, since most video models apply automated safety filters and content\\nmoderation, which may block sensitive content, we exclude videos that are suppressed by such filters\\nfrom our evaluation.\\nEvaluation Metrics.\\nWe employ Gemini-2.5-Pro [12] as an automatic verifier to evaluate each\\ngenerated video. Gemini is prompted with the following evaluation criteria and returns structured\\nscores between 0 and 4, where higher values indicate better performance:\\n1) Instruction Alignment (0-4): Measures how well the video follows the described structure\\nand sequence in the prompt. A high score indicates that the visual steps faithfully reflect\\nthe textual instructions.\\n2) Temporal Consistency (0-4): Evaluates the smoothness and continuity between frames.\\nDisjointed or abrupt transitions will lead to a lower score.\\n28\\n\\x0cTable 2: Model-level Overall and Per-dimension Performance on MME-COF. Mean scores and\\nstandard deviations are reported on a 0‚Äì4 scale, as graded by Gemini-2.5-Pro.\\nModel\\nOverall\\nInstruction\\nAlignment\\nTemporal\\nConsistency\\nVisual\\nStability\\nContent\\nFidelity\\nFocus\\nRelevance\\nKling-v1 [38]\\n0.64 ¬± 0.91\\n0.01 ¬± 0.09\\n0.15 ¬± 0.75\\n2.43 ¬± 1.86\\n0.21 ¬± 0.79\\n0.43 ¬± 1.07\\nSeedance-1.0-pro [19]\\n1.41 ¬± 1.51\\n0.30 ¬± 0.86\\n1.65 ¬± 1.57\\n2.00 ¬± 1.72\\n1.13 ¬± 1.65\\n1.98 ¬± 1.75\\nVeo-3.0-fast [21]\\n1.44 ¬± 1.51\\n0.56 ¬± 1.09\\n1.37 ¬± 1.51\\n1.88 ¬± 1.73\\n1.10 ¬± 1.52\\n2.27 ¬± 1.69\\nVeo-3.0-preview [21]\\n1.45 ¬± 1.50\\n0.54 ¬± 1.06\\n1.43 ¬± 1.53\\n1.89 ¬± 1.71\\n1.12 ¬± 1.49\\n2.26 ¬± 1.73\\nSora-2-pro [56]\\n1.66 ¬± 1.53\\n0.48 ¬± 0.96\\n1.36 ¬± 1.59\\n2.39 ¬± 1.65\\n1.64 ¬± 1.72\\n2.44 ¬± 1.73\\nSora-2 [56]\\n1.72 ¬± 1.59\\n0.59 ¬± 1.12\\n1.52 ¬± 1.69\\n2.32 ¬± 1.68\\n1.62 ¬± 1.75\\n2.52 ¬± 1.71\\nTable 3: Per-category Scores on MME-COF. Mean scores and standard deviations are reported on\\na 0‚Äì4 scale, as graded by Gemini-2.5-Pro.\\nCategory\\nKling-v1 [38]\\nSeedance-1.0\\nPro [19]\\nVeo-3.0\\nFast [21]\\nVeo-3.0\\nPreview [21]\\nSora-2 [56]\\nSora-2\\nPro [56]\\nVisual Detail\\n0.72 ¬± 0.69\\n1.37 ¬± 1.39\\n1.10 ¬± 1.24\\n1.59 ¬± 1.68\\n1.14 ¬± 1.32\\n1.08 ¬± 1.89\\nVisual Trace\\n0.49 ¬± 0.65\\n1.23 ¬± 1.13\\n1.43 ¬± 1.26\\n1.48 ¬± 1.24\\n1.51 ¬± 1.37\\n1.75 ¬± 1.31\\nReal-world Spatial\\n0.77 ¬± 0.76\\n1.79 ¬± 1.53\\n2.07 ¬± 1.54\\n2.10 ¬± 1.46\\n1.84 ¬± 1.43\\n1.77 ¬± 1.35\\n3D Geometry\\n0.61 ¬± 0.58\\n1.95 ¬± 1.64\\n1.71 ¬± 1.54\\n1.54 ¬± 1.43\\n1.37 ¬± 1.49\\n1.42 ¬± 1.45\\n2D Geometry\\n0.49 ¬± 0.67\\n0.96 ¬± 1.11\\n1.18 ¬± 1.15\\n1.27 ¬± 1.20\\n1.77 ¬± 1.45\\n1.77 ¬± 1.21\\nPhysics-based\\n0.60 ¬± 0.62\\n1.27 ¬± 1.25\\n1.44 ¬± 1.39\\n1.44 ¬± 1.35\\n2.13 ¬± 1.32\\n2.10 ¬± 1.33\\nRotation\\n0.22 ¬± 0.34\\n2.30 ¬± 1.46\\n1.83 ¬± 1.44\\n1.60 ¬± 1.29\\n1.62 ¬± 1.37\\n1.44 ¬± 1.28\\nTable & Chart\\n0.87 ¬± 0.72\\n0.71 ¬± 1.18\\n0.82 ¬± 1.30\\n0.96 ¬± 1.44\\n1.84 ¬± 1.61\\n1.48 ¬± 1.59\\nGUI\\n1.09 ¬± 0.51\\n0.70 ¬± 0.76\\n1.11 ¬± 1.09\\n1.18 ¬± 0.89\\n1.88 ¬± 1.64\\n1.52 ¬± 1.48\\nObject Counting\\n0.64 ¬± 0.58\\n1.15 ¬± 0.97\\n2.03 ¬± 1.42\\n1.84 ¬± 1.42\\n2.06 ¬± 1.48\\n1.86 ¬± 1.41\\nEmbodied\\n0.80 ¬± 0.00\\n1.82 ¬± 1.67\\n1.33 ¬± 1.57\\n1.18 ¬± 1.46\\n1.30 ¬± 1.51\\n1.40 ¬± 1.42\\nMedical\\n1.15 ¬± 1.17\\n1.56 ¬± 1.41\\n0.27 ¬± 0.39\\n0.30 ¬± 0.58\\n2.08 ¬± 1.56\\n1.81 ¬± 1.42\\n3) Visual Stability (0-4): Assesses the stability of the video in terms of camera motion, object\\nappearance, and scene composition. Shaky or glitchy outputs are penalized.\\n4) Content Fidelity (0-4): Determines how accurately the key elements described in the\\nprompt are preserved. Hallucinated or missing objects/events will reduce the score.\\n5) Focus Relevance (0-4): Examines whether the video‚Äôs visual attention remains focused on\\nthe correct objects or regions throughout. Irrelevant distractions or poorly framed targets\\nare penalized.\\nWe adopt a direct prompting strategy, instructing Gemini with the prompt, videos, and evaluation\\ncriteria to produce numerical scores in JSON format directly.\\n3.4\\nQuantitative Results and Analysis\\nWe report the quantitative scores of the five evaluated models across the five reasoning dimensions in\\nTable 2, and provide detailed per-category results in Table 3 and Figure 2a.\\nOverall, most models exhibit limited reasoning capability across all tasks in MME-COF, reflected\\nby generally low scores. Among the five dimensions, Visual Stability achieves the highest average,\\nindicating that current video models can generate smooth and coherent sequences. Yet, their behavior\\nremains largely at the level of pattern replay rather than genuine reasoning.\\nThe Sora-2 series [56] shows relative advantages in physics-based, embodied, and medical reason-\\ning, while the Veo-3.0 series [21] performs comparatively better in real-world spatial reasoning.\\nSeedance-1.0-pro [19] demonstrates relative strength in rotation and 3D geometry reasoning. These\\ntrends suggest that different models specialize in distinct reasoning aspects. However, their mean\\nscores remain below 2.0 out of 4, highlighting substantial room for improvement and pointing to\\nopportunities for more targeted enhancement in future development.\\n29\\n\\x0c4\\nRelated Work\\nVideo Models.\\nVideo models have been progressively evolving both in the fields of video under-\\nstanding and generation. For video understanding methods, earlier approaches, such as MViT [14],\\nVideo Swin Transformer [48], and VideoMAE [62], aim to learn a robust representation that fosters\\ndownstream tasks. With the rise of LLMs, recent approaches encode videos as tokens and exploit the\\nlanguage backbone for captioning [61], event localization [59], and high-level reasoning [28, 83].\\nVideo generation models have also attracted much attention. Closed system, including OpenAI‚Äôs\\nSora [55, 56], Runway‚Äôs Gen-3 [58], Pika Labs [57], Luma AI [50], and Google DeepMind‚Äôs\\nVeo series [20, 21], have exhibited impressive results. However, they remain inaccessible due to\\ntheir closed-source nature. Open-source alternatives have recently become available: Stable Video\\nDiffusion [6] introduces efficient training strategies, Hunyan-Video [37] proposes systematic scaling,\\nand Wan-2.1 [64] presents an efficient 3D VAE with expanded pipelines.\\nReasoning with Video.\\nThe advent of large reasoning models [24, 60, 27, 69], such as OpenAI\\no1 [54] and DeepSeek-R1 [23], has spurred the development of video reasoning benchmarks. Most\\ncurrent methods [15, 44, 53] employ MLLMs specialized in video reasoning understanding. For\\nexample, Video-R1 [15] specifically targets temporal reasoning capabilities by introducing a temporal\\ngroup relative policy optimization (GRPO) loss. VideoChat-R1 [44] focuses on spatio-temporal\\nreasoning abilities by training with GRPO and rule-based rewards. A two-stage training strategy,\\ncombining SFT and RL, is used by VideoRFT [65]. When trained on vast collections of images\\nand videos, this strategy boosts the model‚Äôs ability to handle QA tasks, whether in general con-\\ntexts or reasoning-focused ones. These methods primarily focus on enhancing specific types of\\nquestion-answering or captioning tasks. Concurrently, [70] demonstrates the large potential of video\\ngenerative models in video reasoning. These models have implicitly acquired world knowledge\\nthroughdemonstrates impressive performance on various tasks, includinging and reasoning capability.\\nYet, this direction has rarely been explored and only experimented with in zero-shot settings.\\nEvaluation of Video Models as Zero Shot Learner.\\nRecently, several works have been exploring\\nthe zero-shot capability of video generation models in various domains, including general-purpose\\nvision understanding [70, 17], medical imaging [39], and world models [68]. [70] conducts experi-\\nments on Veo 3 with a variety of vision tasks that have not been explicitly included during training.\\nThe video model showcases surprising performance on multiple tasks like object segmentation, image\\nediting, and even maze solving. [39] later adopts a similar paradigm to medical images understanding\\ntasks and finds video generation models also show powerful capabilities, e.g., delineation of anatomi-\\ncal structures in CT scans, medical image segmentation, and even forecasting of future 3D CT phases.\\nBesides, [68] shows that video generation models could also understand complex temporal causality\\nand world knowledge in the real world, thereby serving as a world model [2, 33].\\n5\\nConclusions and Insights\\nVideo models demonstrate an intuitive understanding of the simple visual world.\\nRecent\\nvideo models can generate high-fidelity videos with realistic motion dynamics, suggesting that they\\nhave internalized substantial visual and structural knowledge about the world. Through qualitative\\nresults from our empirical study and quantitative results from the MME-COF benchmark, our work\\nconfirms that these models do exhibit intuitive yet local reasoning potential. This emergent behavior,\\nwhich aligns with the ‚ÄúChain-of-Frame‚Äù (CoF) mechanism, is revealed across several common\\nsuccess patterns. (i) Fine-grained Grounding. Models demonstrate a capability for fine-grained\\nattribute and spatial grounding, especially when targets are visually distinct, as presented in visual\\ndetail reasoning tasks. (ii) Short-horizon Trace Consistency. In Visual Trace Reasoning tasks,\\nmodels can maintain short-term consistency in visual traces. (iii) Emergent Tool-Use Simulation.\\nAn emergent ability to follow CoF instructions that mimic tool-use is presented, such as drawing\\nlines in 2D geometry, highlighting targets in object counting, or controlling the camera in table\\nand chart reasoning. (iv) Foundational Spatial and Geometric Grasp. This includes single-step 3D\\ngeometry transformations, understanding basic real-world spatial layouts, finding coherent sequential\\npaths, and handling small-angle Rotations. (v) Preliminary Real-world Interaction. Models display\\na preliminary comprehension of real-world interaction, generating coherent manipulation paths in\\nembodied reasoning.\\n30\\n\\x0cComplex visual reasoning reveals fundamental limitations.\\nHowever, visual reasoning demands\\nmore than these foundational skills. It tests a model‚Äôs ability to maintain long-horizon logical\\nconsistency, adhere to abstract constraints, and understand functional principles. In these complex\\nareas, our study reveals fundamental limitations and several common failure patterns. (i) Causal and\\nPhysical Logic. This is evident in physics-based reasoning, where the model generates implausible\\nmotion that violates basic causal principles, and in visual trace reasoning, where the generated\\nsequences break causal order with illogical steps. (ii) Long-horizon and Rule-grounded Reasoning.\\nIn visual trace reasoning, models fail to maintain state and adhere to task-specific rules over extended\\nsequences. (iii) Geometric and Spatial Logic. Models fail at multi-step or complex transformations\\nin 3D/2D geometry and real-world spatial tasks, often breaking constraints or prioritizing visual\\nplausibility over correctness. (iv) Functional and Interaction Logic. They merely imitate GUI actions\\nwithout grasping their purpose and lack the necessary planning and stability for reliable Embodied\\ntasks, often resorting to workarounds. (v) Perceptual Precision and Specialized Knowledge. This\\nweakness appears when models fail to identify small or indistinct targets in visual detail reasoning,\\ndistort data in table and chart tasks, and fail to process specialized medical imagery due to a lack of\\ndomain understanding.\\nCurrent video models are not yet ready as standalone zero-shot reasoners.\\nOverall, our findings\\nshow that current video models are not yet reliable as standalone zero-shot reasoners. Strong\\ngenerative performance does not automatically imply robust reasoning during inference. The model‚Äôs\\nbehavior appears to be driven more by learning surface-level patterns and correlations rather than by\\ninternalizing general principles. It excels at short-term coherence rather than long-horizon causality.\\nThis is evident when the model prioritizes visual plausibility over precise spatial reasoning, or favors\\nvisually symmetric patterns over strictly adhering to geometric instructions. This tendency to produce\\nplausible but instructionally flawed outputs reveals a reasoning process that is pattern-driven, not\\nprinciple-driven, thereby undermining its ability to function as a standalone zero-shot reasoner.\\nThe potential in advancing next-generation collaborative visual reasoning.\\nDespite these\\nlimitations, the emergent behaviors observed in video models signal strong potential. The CoF\\nconcept suggests a novel modality for reasoning through visual problems step by step. While these\\nmodels are not yet robust standalone reasoners, their foundational capabilities demonstrate that they\\ncan be guided through carefully designed prompts. This suggests a path where video models exhibit\\nencouraging signs as complementary visual engines alongside dedicated reasoning models.\\nReferences\\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4\\ntechnical report. arXiv preprint arXiv:2303.08774, 2023.\\n[2] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit\\nChattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model\\nplatform for physical ai. arXiv preprint arXiv:2501.03575, 2025.\\n[3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge,\\nYu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.\\n[4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang\\nZhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding,\\nlocalization, text reading, and beyond, 2023.\\n[5] Zechen Bai, Hai Ci, and Mike Zheng Shou. Impossible videos. arXiv preprint arXiv:2503.14378,\\n2025.\\n[6] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Do-\\nminik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion:\\nScaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023.\\n[7] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja\\nFidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent\\n31\\n\\x0cdiffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition, pages 22563‚Äì22575, 2023.\\n[8] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\\nand Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\\n[9] Kaitao Chen, Shaohao Rui, Yankai Jiang, Jiamin Wu, Qihao Zheng, Chunfeng Song, Xiaosong\\nWang, Mu Zhou, and Mianxin Liu. Think twice to see more: Iterative visual reasoning in\\nmedical vlms. arXiv preprint arXiv:2510.10052, 2025.\\n[10] Xinyan Chen, Renrui Zhang, Dongzhi Jiang, Aojun Zhou, Shilin Yan, Weifeng Lin, and\\nHongsheng Li. Mint-cot: Enabling interleaved visual tokens in mathematical chain-of-thought\\nreasoning. arXiv preprint arXiv:2506.05331, 2025.\\n[11] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu,\\nWenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling\\nand audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024.\\n[12] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit\\nDhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the\\nfrontier with advanced reasoning, multimodality, long context, and next generation agentic\\ncapabilities. arXiv preprint arXiv:2507.06261, 2025.\\n[13] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,\\nAiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd\\nof models. arXiv e-prints, pages arXiv‚Äì2407, 2024.\\n[14] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and\\nChristoph Feichtenhofer. Multiscale vision transformers. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision (ICCV), pages 6824‚Äì6835, 2021.\\n[15] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei\\nWu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning\\nin mllms. arXiv preprint arXiv:2503.21776, 2025.\\n[16] Zhanzhou Feng, Qingpei Guo, Xinyu Xiao, Ruihan Xu, Ming Yang, and Shiliang Zhang. Unified\\nvideo generation via next-set prediction in continuous domain. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision, pages 19427‚Äì19438, 2025.\\n[17] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang,\\nChenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive\\nevaluation benchmark of multi-modal llms in video analysis. CVPR 2025 Highlight, 2024.\\n[18] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong,\\nJianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal\\nlarge language model. arXiv preprint arXiv:2312.11370, 2023.\\n[19] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li,\\nJiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation\\nmodels. arXiv preprint arXiv:2506.09113, 2025.\\n[20] Google DeepMind. Veo 2, 12 2024. Accessed: 2024.\\n[21] Google DeepMind. Veo-3 technical report. Technical report, Google DeepMind, May 2025.\\n[22] Kaisi Guan, Zhengfeng Lai, Yuchong Sun, Peng Zhang, Wei Liu, Kieran Liu, Meng Cao, and\\nRuihua Song. Etva: Evaluation of text-to-video alignment via fine-grained question generation\\nand answering. arXiv preprint arXiv:2503.16867, 2025.\\n[23] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\\nShirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in\\nllms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.\\n32\\n\\x0c[24] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan,\\nJian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint\\narXiv:2505.07062, 2025.\\n[25] Meng-Hao Guo, Xuanyu Chu, Qianrui Yang, Zhe-Han Mo, Yiqing Shen, Pei-lin Li, Xinjie Lin,\\nJinnian Zhang, Xin-Sheng Chen, Yi Zhang, et al. Rbench-v: A primary assessment for visual\\nreasoning models with multi-modal outputs. arXiv preprint arXiv:2505.16770, 2025.\\n[26] Meng-Hao Guo, Xuanyu Chu, Qianrui Yang, Zhe-Han Mo, Yiqing Shen, Pei-Lin Li, Xinjie Lin,\\nJinnian Zhang, Xin-Sheng Chen, Yi Zhang, Kiyohiro Nakayama, Zhengyang Geng, Houwen\\nPeng, Han Hu, and Shi-Min Hu. Rbench-v: A primary assessment for visual reasoning models\\nwith multi-modal outputs. 2025.\\n[27] Ziyu Guo*, Renrui Zhang*, Chengzhuo Tong*, Zhizheng Zhao*, Peng Gao, Hongsheng Li, and\\nPheng-Ann Heng. Can we generate images with cot? let‚Äôs verify and reinforce image generation\\nstep by step. CVPR 2025, 2025.\\n[28] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei\\nLiu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos.\\narXiv preprint arXiv:2501.13826, 2025.\\n[29] Mengdi Jia, Zekun Qi, Shaochen Zhang, Wenyao Zhang, Xinqiang Yu, Jiawei He, He Wang, and\\nLi Yi. Omnispatial: Towards comprehensive spatial reasoning benchmark for vision language\\nmodels. arXiv preprint arXiv:2506.03135, 2025.\\n[30] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\\nSaulnier, L√©lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\\nLavril, Thomas Wang, Timoth√©e Lacroix, and William El Sayed. Mistral 7b, 2023.\\n[31] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan\\nJin, Claire Guo, Shen Yan, et al. Mme-cot: Benchmarking chain-of-thought in large multimodal\\nmodels for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621,\\n2025.\\n[32] Emily Jin, Jiaheng Hu, Zhuoyi Huang, Ruohan Zhang, Jiajun Wu, Li Fei-Fei, and Roberto\\nMart√≠n-Mart√≠n. Mini-behavior: A procedurally generated benchmark for long-horizon decision-\\nmaking in embodied ai. arXiv preprint arXiv:2310.01824, 2023.\\n[33] Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Huang Gao, and Jiashi\\nFeng. How far is video generation from world model? ‚Äì a physical law perspective. arXiv\\npreprint arXiv:2406.16860, 2024.\\n[34] Yoonsik Kim, Moonbin Yim, and Ka Yeon Song. Tablevqa-bench: A visual question answering\\nbenchmark on multiple table domains. arXiv preprint arXiv:2404.19205, 2024.\\n[35] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\\nlanguage models are zero-shot reasoners. Advances in neural information processing systems,\\n35:22199‚Äì22213, 2022.\\n[36] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jos√© Lezama, Jonathan Huang, Grant Schindler, Rachel\\nHornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: A large language\\nmodel for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023.\\n[37] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin\\nLi, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: A systematic framework for large video\\ngenerative models. arXiv preprint arXiv:2412.03603, 2024.\\n[38] Kuaishou Technology. Kling ai: Next-generation ai creative studio. https://klingai.com/,\\nJune 2024.\\n[39] Yuxiang Lai, Jike Zhong, Ming Li, Yuheng Li, and Xiaofeng Yang. Are video models emerging\\nas zero-shot learners and reasoners in medical imaging? arXiv preprint arXiv:2510.10254,\\n2025.\\n33\\n\\x0c[40] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan\\nZhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint\\narXiv:2408.03326, 2024.\\n[41] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vuli¬¥c,\\nand Furu Wei. Imagine while reasoning in space: Multimodal visualization-of-thought. arXiv\\npreprint arXiv:2501.07542, 2025.\\n[42] Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang,\\nand Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer\\nuse. arXiv preprint arXiv:2504.07981, 2025.\\n[43] Linjie Li, Mahtab Bigverdi, Jiawei Gu, Zixian Ma, Yinuo Yang, Ziang Li, Yejin Choi, and Ranjay\\nKrishna. Unfolding spatial cognition: Evaluating multimodal models on visual simulations.\\narXiv preprint arXiv:2506.04633, 2025.\\n[44] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao,\\nYi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforce-\\nment fine-tuning. arXiv preprint arXiv:2504.06958, 2025.\\n[45] Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin\\nVan Durme, and Alan L Yuille. Super-clevr: A virtual benchmark to diagnose domain robustness\\nin visual reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition, pages 14963‚Äì14973, 2023.\\n[46] Dairu Liu, Ziyue Wang, Minyuan Ruan, Fuwen Luo, Chi Chen, Peng Li, and Yang Liu. Visual\\nabstract thinking empowers multimodal reasoning. arXiv preprint arXiv:2505.20164, 2025.\\n[47] Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, and\\nLu Hou. Fetv: A benchmark for fine-grained evaluation of open-domain text-to-video generation.\\nAdvances in Neural Information Processing Systems, 36:62352‚Äì62387, 2023.\\n[48] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin\\ntransformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 3202‚Äì3211, 2022.\\n[49] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind\\nTafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought\\nchains for science question answering. Advances in Neural Information Processing Systems,\\n35:2507‚Äì2521, 2022.\\n[50] LumaLabs. Dream machine, 06 2024. Accessed: 2024.\\n[51] Yulin Luo, Chun-Kai Fan, Menghang Dong, Jiayu Shi, Mengdi Zhao, Bo-Wen Zhang, Cheng\\nChi, Jiaming Liu, Gaole Dai, Rongyu Zhang, Ruichuan An, Kun Wu, Zhengping Che, Shaoxuan\\nXie, Guocai Yao, Zhongxia Zhao, Pengwei Wang, Guang Liu, Zhongyuan Wang, Tiejun Huang,\\nand Shanghang Zhang. Robobench: A comprehensive evaluation benchmark for multimodal\\nlarge language models as embodied brain, 2025.\\n[52] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: A benchmark\\nfor question answering about charts with visual and logical reasoning. In Findings of the\\nAssociation for Computational Linguistics: ACL 2022, pages 2263‚Äì2279, Dublin, Ireland, May\\n2022. Association for Computational Linguistics.\\n[53] Jiahao Meng, Xiangtai Li, Haochen Wang, Yue Tan, Tao Zhang, Lingdong Kong, Yunhai Tong,\\nAnran Wang, Zhiyang Teng, Yujing Wang, and Zhuochen Wang. Open-o3 video: Grounded\\nvideo reasoning with explicit spatio-temporal evidence. arXiv preprint arXiv:2510.20579, 2025.\\n[54] OpenAI.\\nOpenai\\no1\\nsystem\\ncard.\\nhttps://openai.com/index/\\nopenai-o1-system-card/, December 2024. Accessed: 2024-12-05.\\n[55] OpenAI. Video generation models as world simulators. Technical report, OpenAI, 2024.\\n[56] OpenAI. Sora 2 system card. Technical report, OpenAI, September 2025.\\n34\\n\\x0c[57] PikaLabs. Pika 1.5, 10 2024. Accessed: 2024.\\n[58] Runway. Introducing gen-3 alpha: A new frontier for video generation. https://runwayml.\\ncom/research/introducing-gen-3-alpha/, June 2024.\\n[59] Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu. Audio-visual event\\nlocalization in unconstrained videos. In Proceedings of the European conference on computer\\nvision (ECCV), pages 247‚Äì263, 2018.\\n[60] Chengzhuo Tong*, Ziyu Guo*, Renrui Zhang*, Wenyu Shan*, Xinyu Wei, Zhenghao Xing,\\nHongsheng Li, and Pheng-Ann Heng. Delving into rl for image generation with cot: A study on\\ndpo vs. grpo. arXiv preprint arXiv:2505.17017, 2025.\\n[61] Tony Cheng Tong, Sirui He, Zhiwen Shao, and Dit-Yan Yeung. G-veval: A versatile metric for\\nevaluating image and video captions using gpt-4o. In Proceedings of the AAAI Conference on\\nArtificial Intelligence, volume 39, pages 7419‚Äì7427, 2025.\\n[62] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are\\ndata-efficient learners for self-supervised video pre-training. In Advances in Neural Information\\nProcessing Systems (NeurIPS), 2022.\\n[63] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu,\\nHaiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative\\nmodels. arXiv preprint arXiv:2503.20314, 2025.\\n[64] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao,\\nJianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative\\nmodels. arXiv preprint arXiv:2503.20314, 2025.\\n[65] Qi Wang, Yanrui Yu, Ye Yuan, Rui Mao, and Tianfei Zhou. Videorft: Incentivizing video\\nreasoning capability in mllms via reinforced fine-tuning. arXiv preprint arXiv:2505.12434,\\n2025.\\n[66] Siting Wang, Minnan Pei, Luoyang Sun, Cheng Deng, Kun Shao, Zheng Tian, Haifeng Zhang,\\nand Jun Wang. Spatialviz-bench: An mllm benchmark for spatial visualization. arXiv preprint\\narXiv:2507.07610, 2025.\\n[67] Xuehui Wang, Zhenyu Wu, JingJing Xie, Zichen Ding, Bowen Yang, Zehao Li, Zhaoyang\\nLiu, Qingyun Li, Xuan Dong, Zhe Chen, et al. Mmbench-gui: Hierarchical multi-platform\\nevaluation framework for gui agents. arXiv preprint arXiv:2507.19478, 2025.\\n[68] Zeqing Wang, Xinyu Wei, Bairui Li, Zhen Guo, Jinrui Zhang, Hongyang Wei, Keze Wang, and\\nLei Zhang. Videoverse: How far is your t2v generator from a world model? arXiv preprint\\narXiv:2510.08398, 2025.\\n[69] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\\nAdvances in neural information processing systems, 35:24824‚Äì24837, 2022.\\n[70] Thadd√§us Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky,\\nBeen Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners.\\narXiv preprint arXiv:2509.20328, 2025.\\n[71] Penghao Wu and Saining Xie. V?: Guided visual search as a core mechanism in multimodal\\nllms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\\npages 13084‚Äì13094, 2024.\\n[72] Qiucheng Wu, Handong Zhao, Michael Saxon, Trung Bui, William Yang Wang, Yang Zhang,\\nand Shiyu Chang. Vsp: Assessing the dual challenges of perception and reasoning in spatial\\nplanning tasks for vlms. arXiv preprint arXiv:2407.01863, 2024.\\n[73] Weiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu,\\nHouqiang Li, Xiaohua Wang, Xizhou Zhu, Wenhai Wang, Jifeng Dai, and Jinguo Zhu. Visulogic:\\nA benchmark for evaluating visual reasoning in multi-modal large language models. arXiv\\npreprint arXiv:2504.15279, 2025.\\n35\\n\\x0c[74] Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen\\nChen, Haodong Duan, Xiangyu Yue, et al. Mmsi-bench: A benchmark for multi-image spatial\\nintelligence. arXiv preprint arXiv:2505.23764, 2025.\\n[75] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming\\nYang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion\\nmodels with an expert transformer. arXiv preprint arXiv:2408.06072, 2024.\\n[76] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos√© Lezama, Han Zhang, Huiwen Chang, Alexander G\\nHauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video\\ntransformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 10459‚Äì10469, 2023.\\n[77] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens,\\nDongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal\\nunderstanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition, pages 9556‚Äì9567, 2024.\\n[78] Chenyu Zhang, Daniil Cherniavskii, Andrii Zadaianchuk, Antonios Tragoudaras, Antonios\\nVozikis, Thijmen Nijdam, Derck WE Prinzhorn, Mark Bodracska, Nicu Sebe, and Efstratios\\nGavves. Morpheus: Benchmarking physical reasoning of video generative models with real\\nphysical experiments. arXiv preprint arXiv:2504.02918, 2025.\\n[79] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun\\nZhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly\\nsee the diagrams in visual math problems? ECCV 2024, 2024.\\n[80] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming\\nLiu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual instruction\\ntuning. arXiv e-prints, pages arXiv‚Äì2407, 2024.\\n[81] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu,\\nand Chunyuan Li. Llava-next: A strong zero-shot video understanding model, April 2024.\\n[82] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting\\nin large language models. arXiv preprint arXiv:2210.03493, 2022.\\n[83] Yilun Zhao, Haowei Zhang, Lujing Xie, Tongyan Hu, Guo Gan, Yitao Long, Zhiyuan Hu,\\nWeiyuan Chen, Chuhan Li, Zhijian Xu, et al. Mmvu: Measuring expert-level multi-discipline\\nvideo understanding. In Proceedings of the Computer Vision and Pattern Recognition Confer-\\nence, pages 8475‚Äì8489, 2025.\\n[84] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun\\nZhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all.\\narXiv preprint arXiv:2412.20404, 2024.\\n[85] Wenwen Zhuang, Xin Huang, Xiantao Zhang, and Jin Zeng. Math-puma: Progressive up-\\nward multimodal alignment to enhance mathematical reasoning. In Proceedings of the AAAI\\nConference on Artificial Intelligence, volume 39, pages 26183‚Äì26191, 2025.\\n36',\n",
       " 'type': 'Document'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_json = json.loads(doc[0].json())\n",
    "doc_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "716bea76-011a-4d4e-ad47-651211e89694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark',\n",
       " 'Summary': 'Veo-3 demonstrates emerging visual reasoning capabilities, particularly in short-horizon spatial coherence and fine-grained grounding, but struggles with complex reasoning conditions like long-horizon causal consistency, strict geometric constraints, and abstract logic. While it shows encouraging signs as a complementary visual engine alongside specialized reasoning models, it is not yet reliable as a standalone zero-shot reasoner.',\n",
       " 'Overview': \"The study comprehensively investigates Veo-3's reasoning potential across 12 dimensions, revealing strengths in short-horizon spatial coherence and local dynamics, but significant limitations in handling complex geometric constraints, causal reasoning, and abstract logic. The research highlights the model's potential as a visual engine alongside specialized reasoning models, emphasizing the need for further advancements in its reasoning capabilities.\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_answer = {\n",
    "    'Title' : doc_json['metadata']['title'],\n",
    "    'Summary' : llm_response_json['summary'],\n",
    "    'Overview' : llm_response_json['overview']\n",
    "}\n",
    "final_answer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9498a1ee-0ba0-4485-bf4c-dc9d3572d83f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f55724-3a9a-4b73-970f-fcde1593c5f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94ed93f1-8c17-4e2b-8db8-01b637d537f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = f\"{BASE_URL}/fetch_all_sources\"\n",
    "response = requests.get(url)\n",
    "contexts = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cb29d46-7c07-4ef6-92f6-ce37d140460e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for\\n  Understanding Anything',\n",
       " 'summary': 'Multimodal large language models (MLLMs) have shown strong capabilities but\\nremain limited to fixed modality pairs and require costly fine-tuning with\\nlarge aligned datasets. Building fully omni-capable models that can integrate\\ntext, images, audio, and video remains impractical and lacks robust reasoning\\nsupport. In this paper, we propose an Agent-Omni framework that coordinates\\nexisting foundation models through a master-agent system, enabling flexible\\nmultimodal reasoning without retraining. The master agent interprets user\\nintent, delegates subtasks to modality-specific agents, and integrates their\\noutputs into coherent responses. Extensive experiments across text, image,\\naudio, video, and omni benchmarks show that Agent-Omni consistently achieves\\nstate-of-the-art performance, particularly on tasks requiring complex\\ncross-modal reasoning. Its agent-based design enables seamless integration of\\nspecialized foundation models, ensuring adaptability to diverse inputs while\\nmaintaining transparency and interpretability. In addition, the framework is\\nmodular and easily extensible, allowing future improvements as stronger models\\nbecome available. %We release an open-source implementation to support\\ncontinued research on scalable and reliable omni-modal reasoning.',\n",
       " 'link': 'http://arxiv.org/abs/2511.02834v1',\n",
       " 'published': '2025-11-04T18:59:09+00:00'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts['arxiv'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b39bf2e-137e-41d7-8dca-0a1a05604801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27aaadb8-645e-456d-8309-e7a300c2fee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving pdf to: cache/Agent-OmniTest-Time Multimodal Reasoning via Model Coordination for Understanding Anything.pdf.\n",
      "{'Title': 'Agent-OmniTest-Time Multimodal Reasoning via Model Coordination for Understanding Anything', 'Summary': 'The Agent-Omni framework proposes a novel approach to achieving truly omni-capable multimodal reasoning at test time. It coordinates existing foundation models through a master-agent system, enabling flexible integration of text, images, audio, and video inputs without retraining. The system interprets user intent, delegates subtasks to modality-specific agents, and integrates their outputs into coherent responses, consistently achieving state-of-the-art performance across diverse benchmarks.', 'Overview': 'Agent-Omni utilizes a hierarchical agent architecture. A master agent interprets user queries, identifies relevant modalities, and delegates subtasks to specialized foundation models (text, image, audio, video). These models generate structured outputs, which are then integrated by the master agent into a final, coherent answer. This approach avoids the costly and impractical requirement of large-scale fine-tuning across all modalities, offering a flexible and adaptable solution for complex multimodal understanding and reasoning.'}\n",
      "Saving pdf to: cache/TWIST2ScalablePortableand Holistic Humanoid Data Collection System.pdf.\n",
      "{'Title': 'TWIST2ScalablePortableand Holistic Humanoid Data Collection System', 'Summary': 'TWIST2 is a portable, scalable, and holistic humanoid data collection system that enables long-horizon, dexterous, and mobile humanoid skills. It utilizes a PICO4U VR device for whole-body motion streaming and a custom neck attachment for egocentric vision. The system achieves 100 successful pick & place demonstrations in 15 minutes and facilitates the training of a hierarchical visuomotor policy for autonomous humanoid control. Key innovations include a general motion tracker for retargeting human motions and a diffusion policy for learning visuomotor policies. The entire system is open-sourced to promote reproducibility and accelerate research in humanoid robotics.', 'Overview': \"TWIST2 represents a significant advancement in humanoid robotics data collection. By combining a readily accessible VR system with a tailored neck attachment and a robust hierarchical policy learning framework, it overcomes limitations of previous approaches. The system's portability, scalability, and ability to capture complex human-like movements make it a valuable tool for advancing research in areas such as human-robot interaction, manipulation, and locomotion. The open-source nature of the system further fosters collaboration and accelerates progress in the field.\"}\n",
      "Saving pdf to: cache/DensemarksLearning Canonical Embeddings for Human Heads Images via Point Tracks.pdf.\n",
      "{'Title': 'DensemarksLearning Canonical Embeddings for Human Heads Images via Point Tracks', 'Summary': 'DenseMarks is a novel learned representation for human heads that generates dense, 3D embeddings for each pixel in a head image. Trained using a contrastive loss guided by point tracker matches, it enables high-quality dense correspondences, robust tracking, and a smooth, interpretable canonical space for applications like head tracking and stereo reconstruction. The method leverages a Vision Transformer (ViT) network and is designed to handle complex features like hair and accessories.', 'Overview': \"The DenseMarks method addresses limitations in existing head tracking approaches by creating a dense, pixel-level representation of human heads. This approach improves robustness to occlusions and variations in pose, enabling applications requiring accurate and complete head modeling. The method's canonical space provides a structured and interpretable representation for further analysis and interaction.\"}\n",
      "Saving pdf to: cache/MemSearcherTraining LLMs to ReasonSearch and Manage Memory via End-to-End Reinforcement Learning.pdf.\n",
      "{'Title': 'MemSearcherTraining LLMs to ReasonSearch and Manage Memory via End-to-End Reinforcement Learning', 'Summary': 'MemSearcher is an agentic workflow that maintains a compact memory throughout interactions, iteratively updating it with essential information. It utilizes a multi-context GRPO algorithm to optimize reasoning, search strategies, and memory management, achieving significant improvements over baseline methods in question answering tasks.', 'Overview': 'MemSearcher leverages a multi-context GRPO algorithm to train agents that can effectively reason, utilize search engines, and manage memory simultaneously. The system maintains a compact memory by iteratively updating it with essential information, leading to improved performance and efficiency compared to traditional ReAct-based approaches.'}\n",
      "Saving pdf to: cache/Can LLMs subtract numbers .pdf.\n",
      "{'Title': 'Can LLMs subtract numbers ', 'Summary': \"This study systematically evaluates subtraction in large language models (LLMs), revealing a significant performance gap compared to addition. LLMs consistently struggle with negative results, often omitting the negative sign even when correctly computing the magnitude. Probing experiments demonstrate that while LLMs internally represent the sign difference, this information isn't reliably transferred to the generated output. Few-shot prompting offers modest improvements, but instruction-tuned models achieve near-perfect accuracy in generating the negative sign, highlighting the disconnect between internal representation and output generation.\", 'Overview': 'The research demonstrates that LLMs exhibit a substantial weakness in subtraction, particularly when the result is negative. The models frequently fail to include the negative sign, indicating a fundamental challenge in translating internal numerical representations into accurate textual outputs. Instruction-tuning significantly improves performance, suggesting that targeted training can bridge this gap, but the underlying issue of representation-generation mismatch remains a key area for future research.'}\n"
     ]
    }
   ],
   "source": [
    "for item in contexts['arxiv']:\n",
    "    pdf_doc = PaperUtils().process_paper(item)\n",
    "    params = {\"query\": query, \"query_context\": pdf_doc['content']}\n",
    "    url = f\"{BASE_URL}/summarize_with_llm\"\n",
    "    response = requests.post(url, params=params)\n",
    "    llm_response = response.json()['llm_response']\n",
    "    llm_response_json = json_parser.parse(llm_response)\n",
    "    final_answer = {\n",
    "    'Title' : pdf_doc['title'],\n",
    "    'Summary' : llm_response_json['summary'],\n",
    "    'Overview' : llm_response_json['overview']\n",
    "    }\n",
    "    print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5407a59f-42b2-4b1c-8e74-f9ae07fec554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac657e85-fb7a-49e6-8e75-cb2e56cf936e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5318d094-d618-4ea9-8865-db4f7ee0802d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
